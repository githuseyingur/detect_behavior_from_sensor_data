{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, joblib, numpy as np, pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tensorflow.keras.utils import Sequence, to_categorical, pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, Activation, add, MaxPooling1D, Dropout,\n",
    "    Bidirectional, LSTM, GlobalAveragePooling1D, Dense, Multiply, Reshape,\n",
    "    Lambda, Concatenate, GRU, GaussianNoise\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import torch\n",
    "import kagglehub\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.amp import autocast\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from torch.optim import Adam as AdamTorch # as AdamTF\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from scipy.signal import firwin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4928003",
   "metadata": {
    "_cell_guid": "a2972d12-bbe6-48da-b0d1-3c024a3ce776",
    "_uuid": "b634cefa-f1a0-46b2-9917-7520309b966f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.0075,
     "end_time": "2025-09-02T17:00:54.733604",
     "exception": false,
     "start_time": "2025-09-02T17:00:54.726104",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba234da",
   "metadata": {
    "_cell_guid": "87809ac8-406b-4d0c-b8c5-504b071288c3",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "1a6cdff9-4cf8-4c88-b56e-ec5e1d38874d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-02T17:00:54.750608Z",
     "iopub.status.busy": "2025-09-02T17:00:54.750368Z",
     "iopub.status.idle": "2025-09-02T17:01:31.422315Z",
     "shell.execute_reply": "2025-09-02T17:01:31.421612Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 36.690436,
     "end_time": "2025-09-02T17:01:31.432423",
     "exception": false,
     "start_time": "2025-09-02T17:00:54.741987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 17:01:01.962434: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756832462.342689      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756832462.437397      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ imports ready · tensorflow 2.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756832479.185724      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1756832479.186448      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ INFERENCE MODE – loading artefacts from /kaggle/input/old-kg87-10folds-8359\n",
      "  Loading models for ensemble inference...\n",
      "--------------------------------------------------\n",
      "[INFO]NumUseModels:20\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "seed_everything(seed=42)\n",
    "TRAIN = False                 \n",
    "RAW_DIR = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\n",
    "PRETRAINED_DIR = Path(\"/kaggle/input/old-kg87-10folds-8359\")\n",
    "EXPORT_DIR = Path(\"./\")\n",
    "BATCH_SIZE = 64\n",
    "PAD_PERCENTILE = 95\n",
    "LR_INIT = 5e-4\n",
    "WD = 3e-3\n",
    "MIXUP_ALPHA = 0.4\n",
    "EPOCHS = 160\n",
    "PATIENCE = 40\n",
    "\n",
    "print(\"▶ imports ready · tensorflow\", tf.__version__)\n",
    "\n",
    "# Tensor Manipulations\n",
    "def time_sum(x): return K.sum(x, axis=1) \n",
    "def squeeze_last_axis(x): return tf.squeeze(x, axis=-1)\n",
    "def expand_last_axis(x): return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "def se_block(x, reduction=8):\n",
    "    ch = x.shape[-1]\n",
    "    se = GlobalAveragePooling1D()(x)\n",
    "    se = Dense(ch // reduction, activation='relu')(se)\n",
    "    se = Dense(ch, activation='sigmoid')(se)\n",
    "    se = Reshape((1, ch))(se)\n",
    "    return Multiply()([x, se])\n",
    "\n",
    "def residual_se_cnn_block(x, filters, kernel_size, pool_size=2, drop=0.3, wd=1e-4):\n",
    "    shortcut = x\n",
    "    for _ in range(2):\n",
    "        x = Conv1D(filters, kernel_size, padding='same', use_bias=False,\n",
    "                   kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "    x = se_block(x)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding='same', use_bias=False,\n",
    "                          kernel_regularizer=l2(wd))(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size)(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    return x\n",
    "\n",
    "def attention_layer(inputs):\n",
    "    score = Dense(1, activation='tanh')(inputs)\n",
    "    score = Lambda(squeeze_last_axis)(score)\n",
    "    weights = Activation('softmax')(score)\n",
    "    weights = Lambda(expand_last_axis)(weights)\n",
    "    context = Multiply()([inputs, weights])\n",
    "    context = Lambda(time_sum)(context)\n",
    "    return context\n",
    "\n",
    "\n",
    "# Normalizes and cleans the time series sequence. \n",
    "def preprocess_sequence(df_seq: pd.DataFrame, feature_cols: list[str], scaler: StandardScaler):\n",
    "    mat = df_seq[feature_cols].ffill().bfill().fillna(0).values\n",
    "    return scaler.transform(mat).astype('float32')\n",
    "\n",
    "\n",
    "# MixUp the data argumentation in order to regularize the neural network. \n",
    "class MixupGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size, alpha=0.2):\n",
    "        self.X, self.y = X, y\n",
    "        self.batch = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.indices = np.arange(len(X))\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch))\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i*self.batch:(i+1)*self.batch]\n",
    "        Xb, yb = self.X[idx], self.y[idx]\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        perm = np.random.permutation(len(Xb))\n",
    "        X_mix = lam * Xb + (1-lam) * Xb[perm]\n",
    "        y_mix = lam * yb + (1-lam) * yb[perm]\n",
    "        return X_mix, y_mix\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    \n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :] \n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "             \n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200):  # Assuming 200Hz sampling rate\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
    "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "\n",
    "            # Calculate the relative rotation\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            \n",
    "            # Convert delta rotation to angular velocity vector\n",
    "            # The rotation vector (Euler axis * angle) scaled by 1/dt\n",
    "            # is a good approximation for small delta_rot\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            # If quaternion is invalid, angular velocity remains zero\n",
    "            pass\n",
    "            \n",
    "    return angular_vel\n",
    "\n",
    "# Computes per-step angular displacement between consecutive IMU orientation quaternions\n",
    "def calculate_angular_distance(rot_data):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
    "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0 \n",
    "            continue\n",
    "        try:\n",
    "          \n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            \n",
    "           \n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0 \n",
    "            pass\n",
    "            \n",
    "    return angular_dist\n",
    "\n",
    "\n",
    "# Model\n",
    "def build_two_branch_model(pad_len, imu_dim, tof_dim, n_classes, wd=1e-4):\n",
    "    inp = Input(shape=(pad_len, imu_dim+tof_dim))\n",
    "    imu = Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "\n",
    "    # IMU deep branch\n",
    "    x1 = residual_se_cnn_block(imu, 64, 3, drop=0.1, wd=wd)\n",
    "    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.1, wd=wd)\n",
    "\n",
    "    # TOF/Thermal lighter branch\n",
    "    x2 = Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof)\n",
    "    x2 = BatchNormalization()(x2); x2 = Activation('relu')(x2)\n",
    "    x2 = MaxPooling1D(2)(x2); x2 = Dropout(0.2)(x2)\n",
    "    x2 = Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x2)\n",
    "    x2 = BatchNormalization()(x2); x2 = Activation('relu')(x2)\n",
    "    x2 = MaxPooling1D(2)(x2); x2 = Dropout(0.2)(x2)\n",
    "\n",
    "    merged = Concatenate()([x1, x2])\n",
    "\n",
    "    xa = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    xb = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    xc = GaussianNoise(0.09)(merged)\n",
    "    xc = Dense(16, activation='elu')(xc)\n",
    "    \n",
    "    x = Concatenate()([xa, xb, xc])\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = attention_layer(x)\n",
    "\n",
    "    for units, drop in [(256, 0.5), (128, 0.3)]:\n",
    "        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x); x = Activation('relu')(x)\n",
    "        x = Dropout(drop)(x)\n",
    "\n",
    "    out = Dense(n_classes, activation='softmax', kernel_regularizer=l2(wd))(x)\n",
    "    return Model(inp, out)\n",
    "\n",
    "tmp_model = build_two_branch_model(127,7,325,18)\n",
    "print(\"▶ INFERENCE MODE – loading artefacts from\", PRETRAINED_DIR)\n",
    "final_feature_cols = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "pad_len        = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n",
    "scaler         = joblib.load(PRETRAINED_DIR / \"scaler.pkl\")\n",
    "gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "\n",
    "\n",
    "custom_objs = {\n",
    "    'time_sum': time_sum, 'squeeze_last_axis': squeeze_last_axis, 'expand_last_axis': expand_last_axis,\n",
    "    'se_block': se_block, 'residual_se_cnn_block': residual_se_cnn_block, 'attention_layer': attention_layer,\n",
    "}\n",
    "\n",
    "\n",
    "models1 = []\n",
    "print(f\"  Loading models for ensemble inference...\")\n",
    "PRETRAINED_DIR = Path(\"/kaggle/input/newmodel-localseed1-10folds-8323\")\n",
    "for fold in range(10):\n",
    "    model_path = PRETRAINED_DIR / f\"gesture_model_fold_{fold}.h5\"\n",
    "    model = tf.keras.models.load_model(model_path, compile=False, custom_objects=custom_objs) # tf.keras.models !!\n",
    "        \n",
    "    models1.append(model)\n",
    "\n",
    "PRETRAINED_DIR2 = Path(\"/kaggle/input/old-kg87-10folds-8359\")\n",
    "for fold in range(10):\n",
    "    model_path = PRETRAINED_DIR2 / f\"gesture_model_fold_{fold}.h5\"\n",
    "    model = tf.keras.models.load_model(model_path, compile=False, custom_objects=custom_objs) # tf.keras.models !!\n",
    "        \n",
    "    models1.append(model)\n",
    "print(\"-\"*50)\n",
    "print(f\"[INFO]NumUseModels:{len(models1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c5aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "\n",
    "# z ekseni ivme için spektral enerji\n",
    "def spectral_energy(signal, fs=1):\n",
    "    f, Pxx = welch(signal, fs=fs, nperseg=len(signal))\n",
    "    return np.sum(Pxx)  # toplam enerji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe586627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import sobel\n",
    "\n",
    "# Spatial gradient (Sobel)-based features for ToF data\n",
    "def calculate_spatial_tof_features(seq_df, sensor_id):\n",
    "    \n",
    "    # Reshape 1D 64-pixel input into 8x8 and compute Sobel gradients\n",
    "    pixel_cols = [f\"tof_{sensor_id}_v{p}\" for p in range(64)]\n",
    "    tof_data = seq_df[pixel_cols].replace(-1, np.nan).ffill().bfill().fillna(0).values\n",
    "    \n",
    "    # Frame x 64 → (N x 8 x 8)\n",
    "    N = len(seq_df)\n",
    "    reshaped = tof_data.reshape(N, 8, 8)\n",
    "    \n",
    "    # Compute spatial gradients (Sobel x and y)\n",
    "    sobel_x = sobel(reshaped, axis=1)\n",
    "    sobel_y = sobel(reshaped, axis=2)\n",
    "    grad_mag = np.sqrt(sobel_x ** 2 + sobel_y ** 2)\n",
    "\n",
    "    # Compute summary statistics\n",
    "    grad_mean = grad_mag.mean(axis=(1, 2))\n",
    "    grad_std  = grad_mag.std(axis=(1, 2))\n",
    "    grad_max  = grad_mag.max(axis=(1, 2))\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        f'tof_{sensor_id}_grad_mean': grad_mean,\n",
    "        f'tof_{sensor_id}_grad_std': grad_std,\n",
    "        f'tof_{sensor_id}_grad_max': grad_max\n",
    "    }, index=seq_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d51747",
   "metadata": {
    "_cell_guid": "50488121-c655-4e6d-bb0d-d73414c98ed0",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "ede06ff2-e2dc-4a14-a7b6-421a54b8bb9a",
    "execution": {
     "iopub.execute_input": "2025-09-02T17:01:31.449637Z",
     "iopub.status.busy": "2025-09-02T17:01:31.448764Z",
     "iopub.status.idle": "2025-09-02T17:01:31.458389Z",
     "shell.execute_reply": "2025-09-02T17:01:31.457860Z"
    },
    "papermill": {
     "duration": 0.018965,
     "end_time": "2025-09-02T17:01:31.459408",
     "exception": false,
     "start_time": "2025-09-02T17:01:31.440443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict1(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    df_seq = sequence.to_pandas()\n",
    "    #df_seq = extract_tof_features_fast(df_seq)\n",
    "    seq_df_copy = df_seq.copy()\n",
    "    linear_accel = remove_gravity_from_acc(df_seq, df_seq)\n",
    "    df_seq['linear_acc_x'], df_seq['linear_acc_y'], df_seq['linear_acc_z'] = linear_accel[:, 0], linear_accel[:, 1], linear_accel[:, 2]\n",
    "    df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)\n",
    "    df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n",
    "    #df_seq['jerk_mad_25'] = df_seq['linear_acc_mag_jerk'].rolling(5).apply(lambda x: np.median(np.abs(x - np.median(x))), raw=True)\n",
    "    \n",
    "    # sequence\n",
    "    df_seq['acc_x_spectral_energy'] = df_seq.groupby('sequence_id')['acc_x'].transform(spectral_energy)\n",
    "    df_seq['acc_y_spectral_energy'] = df_seq.groupby('sequence_id')['acc_y'].transform(spectral_energy)\n",
    "    df_seq['acc_z_spectral_energy'] = df_seq.groupby('sequence_id')['acc_z'].transform(spectral_energy)\n",
    "    df_seq['linear_acc_mag_spectral_energy'] = df_seq.groupby('sequence_id')['linear_acc_mag'].transform(spectral_energy)\n",
    "\n",
    "    \n",
    "    angular_vel = calculate_angular_velocity_from_quat(df_seq)\n",
    "    df_seq['angular_vel_x'], df_seq['angular_vel_y'], df_seq['angular_vel_z'] = angular_vel[:, 0], angular_vel[:, 1], angular_vel[:, 2]\n",
    "    df_seq['angular_distance'] = calculate_angular_distance(df_seq)\n",
    "\n",
    "    for col in ['acc_x', 'acc_y', 'acc_z', 'linear_acc_x', 'linear_acc_y', 'linear_acc_z', 'angular_vel_x', 'angular_vel_y', 'angular_vel_z']:\n",
    "        if col in df_seq.columns:\n",
    "            df_seq[f'{col}_diff'] = df_seq.groupby('sequence_id')[col].diff().fillna(0)\n",
    "            df_seq[f'{col}_abs_diff'] = np.abs(df_seq.groupby('sequence_id')[col].diff()).fillna(0) # Mutlak fark\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]; tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "        df_seq[f'tof_{i}_mean'], df_seq[f'tof_{i}_std'], df_seq[f'tof_{i}_min'], df_seq[f'tof_{i}_max'] = tof_data.mean(axis=1), tof_data.std(axis=1), tof_data.min(axis=1), tof_data.max(axis=1)\n",
    "        spatial_feats = calculate_spatial_tof_features(seq_df_copy, i)\n",
    "        df_seq = pd.concat([df_seq, spatial_feats], axis=1)\n",
    "        \n",
    "        \n",
    "    mat_unscaled = df_seq[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32')\n",
    "    mat_scaled = scaler.transform(mat_unscaled)\n",
    "    pad_input = pad_sequences([mat_scaled], maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n",
    "\n",
    "    \n",
    "    all_preds = [model.predict(pad_input, verbose=0)[0] for model in models1]\n",
    "    \n",
    "    avg_pred =  np.mean(all_preds, axis=0)\n",
    "    return avg_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4010e6e",
   "metadata": {
    "_cell_guid": "7c4b4b2f-7a87-4bb7-a7d4-987209906dfe",
    "_uuid": "8ed75223-3db6-4998-b5d9-7d17bbea6fb3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.007205,
     "end_time": "2025-09-02T17:01:31.474168",
     "exception": false,
     "start_time": "2025-09-02T17:01:31.466963",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cbe4e8",
   "metadata": {
    "_cell_guid": "0be9a02d-5c4b-4987-bb1f-e5ae13b4630e",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "6468f507-f6fe-440a-a086-5d805e16ce84",
    "execution": {
     "iopub.execute_input": "2025-09-02T17:01:31.490677Z",
     "iopub.status.busy": "2025-09-02T17:01:31.490410Z",
     "iopub.status.idle": "2025-09-02T17:04:11.585752Z",
     "shell.execute_reply": "2025-09-02T17:04:11.585121Z"
    },
    "papermill": {
     "duration": 160.105495,
     "end_time": "2025-09-02T17:04:11.587101",
     "exception": false,
     "start_time": "2025-09-02T17:01:31.481606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n",
      "Have precomputed, skip compute.\n",
      "\n",
      "交叉验证折叠统计:\n",
      "\n",
      "Fold 1:\n",
      "类别                                                 训练集        验证集       \n",
      "Above ear - pull hair                              511        127       \n",
      "Cheek - pinch skin                                 509        128       \n",
      "Drink from bottle/cup                              129        32        \n",
      "Eyebrow - pull hair                                510        128       \n",
      "Eyelash - pull hair                                512        128       \n",
      "Feel around in tray and pull out an object         129        32        \n",
      "Forehead - pull hairline                           512        128       \n",
      "Forehead - scratch                                 512        128       \n",
      "Glasses on/off                                     128        33        \n",
      "Neck - pinch skin                                  512        128       \n",
      "Neck - scratch                                     512        128       \n",
      "Pinch knee/leg skin                                129        32        \n",
      "Pull air toward your face                          381        96        \n",
      "Scratch knee/leg skin                              129        32        \n",
      "Text on phone                                      512        128       \n",
      "Wave hello                                         382        96        \n",
      "Write name in air                                  382        95        \n",
      "Write name on leg                                  129        32        \n",
      "\n",
      "Fold 2:\n",
      "类别                                                 训练集        验证集       \n",
      "Above ear - pull hair                              511        127       \n",
      "Cheek - pinch skin                                 509        128       \n",
      "Drink from bottle/cup                              129        32        \n",
      "Eyebrow - pull hair                                510        128       \n",
      "Eyelash - pull hair                                512        128       \n",
      "Feel around in tray and pull out an object         129        32        \n",
      "Forehead - pull hairline                           512        128       \n",
      "Forehead - scratch                                 512        128       \n",
      "Glasses on/off                                     129        32        \n",
      "Neck - pinch skin                                  512        128       \n",
      "Neck - scratch                                     512        128       \n",
      "Pinch knee/leg skin                                129        32        \n",
      "Pull air toward your face                          381        96        \n",
      "Scratch knee/leg skin                              129        32        \n",
      "Text on phone                                      512        128       \n",
      "Wave hello                                         382        96        \n",
      "Write name in air                                  382        95        \n",
      "Write name on leg                                  129        32        \n",
      "\n",
      "Fold 3:\n",
      "类别                                                 训练集        验证集       \n",
      "Above ear - pull hair                              510        128       \n",
      "Cheek - pinch skin                                 510        127       \n",
      "Drink from bottle/cup                              129        32        \n",
      "Eyebrow - pull hair                                511        127       \n",
      "Eyelash - pull hair                                512        128       \n",
      "Feel around in tray and pull out an object         129        32        \n",
      "Forehead - pull hairline                           512        128       \n",
      "Forehead - scratch                                 512        128       \n",
      "Glasses on/off                                     129        32        \n",
      "Neck - pinch skin                                  512        128       \n",
      "Neck - scratch                                     512        128       \n",
      "Pinch knee/leg skin                                128        33        \n",
      "Pull air toward your face                          382        95        \n",
      "Scratch knee/leg skin                              129        32        \n",
      "Text on phone                                      512        128       \n",
      "Wave hello                                         382        96        \n",
      "Write name in air                                  382        95        \n",
      "Write name on leg                                  128        33        \n",
      "\n",
      "Fold 4:\n",
      "类别                                                 训练集        验证集       \n",
      "Above ear - pull hair                              510        128       \n",
      "Cheek - pinch skin                                 510        127       \n",
      "Drink from bottle/cup                              129        32        \n",
      "Eyebrow - pull hair                                511        127       \n",
      "Eyelash - pull hair                                512        128       \n",
      "Feel around in tray and pull out an object         128        33        \n",
      "Forehead - pull hairline                           512        128       \n",
      "Forehead - scratch                                 512        128       \n",
      "Glasses on/off                                     129        32        \n",
      "Neck - pinch skin                                  512        128       \n",
      "Neck - scratch                                     512        128       \n",
      "Pinch knee/leg skin                                129        32        \n",
      "Pull air toward your face                          382        95        \n",
      "Scratch knee/leg skin                              128        33        \n",
      "Text on phone                                      512        128       \n",
      "Wave hello                                         383        95        \n",
      "Write name in air                                  381        96        \n",
      "Write name on leg                                  129        32        \n",
      "\n",
      "Fold 5:\n",
      "类别                                                 训练集        验证集       \n",
      "Above ear - pull hair                              510        128       \n",
      "Cheek - pinch skin                                 510        127       \n",
      "Drink from bottle/cup                              128        33        \n",
      "Eyebrow - pull hair                                510        128       \n",
      "Eyelash - pull hair                                512        128       \n",
      "Feel around in tray and pull out an object         129        32        \n",
      "Forehead - pull hairline                           512        128       \n",
      "Forehead - scratch                                 512        128       \n",
      "Glasses on/off                                     129        32        \n",
      "Neck - pinch skin                                  512        128       \n",
      "Neck - scratch                                     512        128       \n",
      "Pinch knee/leg skin                                129        32        \n",
      "Pull air toward your face                          382        95        \n",
      "Scratch knee/leg skin                              129        32        \n",
      "Text on phone                                      512        128       \n",
      "Wave hello                                         383        95        \n",
      "Write name in air                                  381        96        \n",
      "Write name on leg                                  129        32        \n"
     ]
    }
   ],
   "source": [
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :] \n",
    "            continue\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i+1]\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
    "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
    "            continue\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return angular_vel\n",
    "\n",
    "def calculate_angular_distance(rot_data):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i+1]\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
    "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0\n",
    "            continue\n",
    "        try:\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0 \n",
    "            pass\n",
    "    return angular_dist\n",
    "\n",
    "\n",
    "class CMIFeDataset(Dataset):\n",
    "    def __init__(self, data_path, config):\n",
    "        self.config = config\n",
    "        self.init_feature_names(data_path)\n",
    "        df = self.generate_features(pd.read_csv(data_path, usecols=set(self.base_cols+self.feature_cols)))\n",
    "        self.generate_dataset(df)\n",
    "\n",
    "    def init_feature_names(self, data_path):\n",
    "        self.imu_engineered_features = [\n",
    "            'acc_mag', 'rot_angle',\n",
    "            'acc_mag_jerk', 'rot_angle_vel',\n",
    "            'linear_acc_mag', 'linear_acc_mag_jerk',\n",
    "            'angular_vel_x', 'angular_vel_y', 'angular_vel_z',\n",
    "            'angular_distance'\n",
    "        ]\n",
    "\n",
    "        self.tof_mode = self.config.get(\"tof_mode\", \"stats\")\n",
    "        self.tof_region_stats = ['mean', 'std', 'min', 'max']\n",
    "        self.tof_cols = self.generate_tof_feature_names()\n",
    "\n",
    "        columns = pd.read_csv(data_path, nrows=0).columns.tolist()\n",
    "        imu_cols_base = ['linear_acc_x', 'linear_acc_y', 'linear_acc_z']\n",
    "        imu_cols_base.extend([c for c in columns if c.startswith('rot_') and c not in ['rot_angle', 'rot_angle_vel']])\n",
    "        self.imu_cols = list(dict.fromkeys(imu_cols_base + self.imu_engineered_features))\n",
    "        self.thm_cols = [c for c in columns if c.startswith('thm_')]\n",
    "        self.feature_cols = self.imu_cols + self.thm_cols + self.tof_cols\n",
    "        self.imu_dim = len(self.imu_cols)\n",
    "        self.thm_dim = len(self.thm_cols)\n",
    "        self.tof_dim = len(self.tof_cols)\n",
    "        self.base_cols = ['acc_x', 'acc_y', 'acc_z',\n",
    "                          'rot_x', 'rot_y', 'rot_z', 'rot_w',\n",
    "                          'sequence_id', 'subject', \n",
    "                          'sequence_type', 'gesture', 'orientation'] + [c for c in columns if c.startswith('thm_')] + [f\"tof_{i}_v{p}\" for i in range(1, 6) for p in range(64)]\n",
    "        self.fold_cols = ['subject', 'sequence_type', 'gesture', 'orientation']\n",
    "\n",
    "    def generate_tof_feature_names(self):\n",
    "        features = []\n",
    "        if self.config.get(\"tof_raw\", False):\n",
    "            for i in range(1, 6):\n",
    "                features.extend([f\"tof_{i}_v{p}\" for p in range(64)])\n",
    "        for i in range(1, 6):\n",
    "            if self.tof_mode != 0:\n",
    "                for stat in self.tof_region_stats:\n",
    "                    features.append(f'tof_{i}_{stat}')\n",
    "                if self.tof_mode > 1:\n",
    "                    for r in range(self.tof_mode):\n",
    "                        for stat in self.tof_region_stats:\n",
    "                            features.append(f'tof{self.tof_mode}_{i}_region_{r}_{stat}')\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        for r in range(mode):\n",
    "                            for stat in self.tof_region_stats:\n",
    "                                features.append(f'tof{mode}_{i}_region_{r}_{stat}')\n",
    "        return features\n",
    "\n",
    "    def compute_features(self, df):\n",
    "        df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n",
    "        df['rot_angle'] = 2 * np.arccos(df['rot_w'].clip(-1, 1))\n",
    "        df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n",
    "        df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n",
    "            \n",
    "        linear_accel_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            acc_data_group = group[['acc_x', 'acc_y', 'acc_z']]\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            linear_accel_group = remove_gravity_from_acc(acc_data_group, rot_data_group)\n",
    "            linear_accel_list.append(pd.DataFrame(linear_accel_group, columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index))\n",
    "        df_linear_accel = pd.concat(linear_accel_list)\n",
    "        df = pd.concat([df, df_linear_accel], axis=1)\n",
    "        df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n",
    "        df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n",
    "    \n",
    "        angular_vel_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            angular_vel_group = calculate_angular_velocity_from_quat(rot_data_group)\n",
    "            angular_vel_list.append(pd.DataFrame(angular_vel_group, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index))\n",
    "        df_angular_vel = pd.concat(angular_vel_list)\n",
    "        df = pd.concat([df, df_angular_vel], axis=1)\n",
    "    \n",
    "        angular_distance_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            angular_dist_group = calculate_angular_distance(rot_data_group)\n",
    "            angular_distance_list.append(pd.DataFrame(angular_dist_group, columns=['angular_distance'], index=group.index))\n",
    "        df_angular_distance = pd.concat(angular_distance_list)\n",
    "        df = pd.concat([df, df_angular_distance], axis=1)\n",
    "\n",
    "        if self.tof_mode != 0:\n",
    "            new_columns = {}\n",
    "            for i in range(1, 6):\n",
    "                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "                tof_data = df[pixel_cols].replace(-1, np.nan)\n",
    "                new_columns.update({\n",
    "                    f'tof_{i}_mean': tof_data.mean(axis=1),\n",
    "                    f'tof_{i}_std': tof_data.std(axis=1),\n",
    "                    f'tof_{i}_min': tof_data.min(axis=1),\n",
    "                    f'tof_{i}_max': tof_data.max(axis=1)\n",
    "                })\n",
    "                if self.tof_mode > 1:\n",
    "                    region_size = 64 // self.tof_mode\n",
    "                    for r in range(self.tof_mode):\n",
    "                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                        new_columns.update({\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                        })\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        region_size = 64 // mode\n",
    "                        for r in range(mode):\n",
    "                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                            new_columns.update({\n",
    "                                f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                            })\n",
    "            df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
    "        return df\n",
    "        \n",
    "    def generate_features(self, df):\n",
    "        self.le = LabelEncoder()\n",
    "        df['gesture_int'] = self.le.fit_transform(df['gesture'])\n",
    "        self.class_num = len(self.le.classes_)\n",
    "        \n",
    "        if all(c in df.columns for c in self.imu_engineered_features) and all(c in df.columns for c in self.tof_cols):\n",
    "            print(\"Have precomputed, skip compute.\")\n",
    "        else:\n",
    "            print(\"Not precomputed, do compute.\")\n",
    "            df = self.compute_features(df)\n",
    "\n",
    "        if self.config.get(\"save_precompute\", False):\n",
    "            df.to_csv(self.config.get(\"save_filename\", \"train.csv\"))\n",
    "        return df\n",
    "\n",
    "    def scale(self, data_unscaled):\n",
    "        scaler_function = self.config.get(\"scaler_function\", StandardScaler())\n",
    "        scaler = scaler_function.fit(np.concatenate(data_unscaled, axis=0))\n",
    "        return [scaler.transform(x) for x in data_unscaled], scaler\n",
    "\n",
    "    def pad(self, data_scaled, cols):\n",
    "        pad_data = np.zeros((len(data_scaled), self.pad_len, len(cols)), dtype='float32')\n",
    "        for i, seq in enumerate(data_scaled):\n",
    "            seq_len = min(len(seq), self.pad_len)\n",
    "            pad_data[i, :seq_len] = seq[:seq_len]\n",
    "        return pad_data\n",
    "\n",
    "    def get_nan_value(self, data, ratio):\n",
    "        max_value = data.max().max()\n",
    "        nan_value = -max_value * ratio\n",
    "        return nan_value\n",
    "\n",
    "    def generate_dataset(self, df):\n",
    "        seq_gp = df.groupby('sequence_id') \n",
    "        imu_unscaled, thm_unscaled, tof_unscaled = [], [], []\n",
    "        classes, lens = [], []\n",
    "        self.imu_nan_value = self.get_nan_value(df[self.imu_cols], self.config[\"nan_ratio\"][\"imu\"])\n",
    "        self.thm_nan_value = self.get_nan_value(df[self.thm_cols], self.config[\"nan_ratio\"][\"thm\"])\n",
    "        self.tof_nan_value = self.get_nan_value(df[self.tof_cols], self.config[\"nan_ratio\"][\"tof\"])\n",
    "\n",
    "        self.fold_feats = defaultdict(list)\n",
    "        for seq_id, seq_df in seq_gp:\n",
    "            imu_data = seq_df[self.imu_cols]\n",
    "            if self.config[\"fbfill\"][\"imu\"]:\n",
    "                imu_data = imu_data.ffill().bfill()\n",
    "            imu_unscaled.append(imu_data.fillna(self.imu_nan_value).values.astype('float32'))\n",
    "\n",
    "            thm_data = seq_df[self.thm_cols]\n",
    "            if self.config[\"fbfill\"][\"thm\"]:\n",
    "                thm_data = thm_data.ffill().bfill()\n",
    "            thm_unscaled.append(thm_data.fillna(self.thm_nan_value).values.astype('float32'))\n",
    "\n",
    "            tof_data = seq_df[self.tof_cols]\n",
    "            if self.config[\"fbfill\"][\"tof\"]:\n",
    "                tof_data = tof_data.ffill().bfill()\n",
    "            tof_unscaled.append(tof_data.fillna(self.tof_nan_value).values.astype('float32'))\n",
    "            \n",
    "            classes.append(seq_df['gesture_int'].iloc[0])\n",
    "            lens.append(len(imu_data))\n",
    "\n",
    "            for col in self.fold_cols:\n",
    "                self.fold_feats[col].append(seq_df[col].iloc[0])\n",
    "            \n",
    "        self.dataset_indices = classes\n",
    "        self.pad_len = int(np.percentile(lens, self.config.get(\"percent\", 95)))\n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            x_unscaled = [np.concatenate([imu, thm, tof], axis=1) for imu, thm, tof in zip(imu_unscaled, thm_unscaled, tof_unscaled)]\n",
    "            x_scaled, self.x_scaler = self.scale(x_unscaled)\n",
    "            x = self.pad(x_scaled, self.imu_cols+self.thm_cols+self.tof_cols)\n",
    "            self.imu = x[..., :self.imu_dim]\n",
    "            self.thm = x[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "            self.tof = x[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "        else:\n",
    "            imu_scaled, self.imu_scaler = self.scale(imu_unscaled)\n",
    "            thm_scaled, self.thm_scaler = self.scale(thm_unscaled)\n",
    "            tof_scaled, self.tof_scaler = self.scale(tof_unscaled)\n",
    "            self.imu = self.pad(imu_scaled, self.imu_cols)\n",
    "            self.thm = self.pad(thm_scaled, self.thm_cols)\n",
    "            self.tof = self.pad(tof_scaled, self.tof_cols)\n",
    "        self.precompute_scaled_nan_values()\n",
    "        self.class_ = F.one_hot(torch.from_numpy(np.array(classes)).long(), num_classes=len(self.le.classes_)).float().numpy()\n",
    "        self.class_weight = torch.FloatTensor(compute_class_weight('balanced', classes=np.arange(len(self.le.classes_)), y=classes))\n",
    "\n",
    "    def precompute_scaled_nan_values(self):\n",
    "        dummy_df = pd.DataFrame(\n",
    "            np.array([[self.imu_nan_value]*len(self.imu_cols) + \n",
    "                     [self.thm_nan_value]*len(self.thm_cols) +\n",
    "                     [self.tof_nan_value]*len(self.tof_cols)]),\n",
    "            columns=self.imu_cols + self.thm_cols + self.tof_cols\n",
    "        )\n",
    "        \n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            scaled = self.x_scaler.transform(dummy_df)\n",
    "            self.imu_scaled_nan = scaled[0, :self.imu_dim].mean()\n",
    "            self.thm_scaled_nan = scaled[0, self.imu_dim:self.imu_dim+self.thm_dim].mean()\n",
    "            self.tof_scaled_nan = scaled[0, self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim].mean()\n",
    "        else:\n",
    "            self.imu_scaled_nan = self.imu_scaler.transform(dummy_df[self.imu_cols])[0].mean()\n",
    "            self.thm_scaled_nan = self.thm_scaler.transform(dummy_df[self.thm_cols])[0].mean()\n",
    "            self.tof_scaled_nan = self.tof_scaler.transform(dummy_df[self.tof_cols])[0].mean()\n",
    "\n",
    "    def get_scaled_nan_tensors(self, imu, thm, tof):\n",
    "        return torch.full(imu.shape, self.imu_scaled_nan, device=imu.device), \\\n",
    "            torch.full(thm.shape, self.thm_scaled_nan, device=thm.device), \\\n",
    "            torch.full(tof.shape, self.tof_scaled_nan, device=tof.device)\n",
    "\n",
    "    def inference_process(self, sequence):\n",
    "        df_seq = sequence.to_pandas().copy()\n",
    "        if not all(c in df_seq.columns for c in self.imu_engineered_features):\n",
    "            df_seq['acc_mag'] = np.sqrt(df_seq['acc_x']**2 + df_seq['acc_y']**2 + df_seq['acc_z']**2)\n",
    "            df_seq['rot_angle'] = 2 * np.arccos(df_seq['rot_w'].clip(-1, 1))\n",
    "            df_seq['acc_mag_jerk'] = df_seq['acc_mag'].diff().fillna(0)\n",
    "            df_seq['rot_angle_vel'] = df_seq['rot_angle'].diff().fillna(0)\n",
    "            if all(col in df_seq.columns for col in ['acc_x', 'acc_y', 'acc_z', 'rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                linear_accel = remove_gravity_from_acc(\n",
    "                    df_seq[['acc_x', 'acc_y', 'acc_z']], \n",
    "                    df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "                )\n",
    "                df_seq[['linear_acc_x', 'linear_acc_y', 'linear_acc_z']] = linear_accel\n",
    "            else:\n",
    "                df_seq['linear_acc_x'] = df_seq.get('acc_x', 0)\n",
    "                df_seq['linear_acc_y'] = df_seq.get('acc_y', 0)\n",
    "                df_seq['linear_acc_z'] = df_seq.get('acc_z', 0)\n",
    "            df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)\n",
    "            df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n",
    "            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                angular_vel = calculate_angular_velocity_from_quat(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
    "                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = angular_vel\n",
    "            else:\n",
    "                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = 0\n",
    "            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                df_seq['angular_distance'] = calculate_angular_distance(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
    "            else:\n",
    "                df_seq['angular_distance'] = 0\n",
    "\n",
    "        if self.tof_mode != 0:\n",
    "            new_columns = {} \n",
    "            for i in range(1, 6):\n",
    "                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "                tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "                new_columns.update({\n",
    "                    f'tof_{i}_mean': tof_data.mean(axis=1),\n",
    "                    f'tof_{i}_std': tof_data.std(axis=1),\n",
    "                    f'tof_{i}_min': tof_data.min(axis=1),\n",
    "                    f'tof_{i}_max': tof_data.max(axis=1)\n",
    "                })\n",
    "                if self.tof_mode > 1:\n",
    "                    region_size = 64 // self.tof_mode\n",
    "                    for r in range(self.tof_mode):\n",
    "                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                        new_columns.update({\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                        })\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        region_size = 64 // mode\n",
    "                        for r in range(mode):\n",
    "                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                            new_columns.update({\n",
    "                                f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                            })\n",
    "            df_seq = pd.concat([df_seq, pd.DataFrame(new_columns)], axis=1)\n",
    "        \n",
    "        imu_unscaled = df_seq[self.imu_cols]\n",
    "        if self.config[\"fbfill\"][\"imu\"]:\n",
    "            imu_unscaled = imu_unscaled.ffill().bfill()\n",
    "        imu_unscaled = imu_unscaled.fillna(self.imu_nan_value).values.astype('float32')\n",
    "\n",
    "        thm_unscaled = df_seq[self.thm_cols]\n",
    "        if self.config[\"fbfill\"][\"thm\"]:\n",
    "            thm_unscaled = thm_unscaled.ffill().bfill()\n",
    "        thm_unscaled = thm_unscaled.fillna(self.thm_nan_value).values.astype('float32')\n",
    "\n",
    "        tof_unscaled = df_seq[self.tof_cols]\n",
    "        if self.config[\"fbfill\"][\"tof\"]:\n",
    "            tof_unscaled = tof_unscaled.ffill().bfill()\n",
    "        tof_unscaled = tof_unscaled.fillna(self.tof_nan_value).values.astype('float32')\n",
    "        \n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            x_unscaled = np.concatenate([imu_unscaled, thm_unscaled, tof_unscaled], axis=1)\n",
    "            x_scaled = self.x_scaler.transform(x_unscaled)\n",
    "            imu_scaled = x_scaled[..., :self.imu_dim]\n",
    "            thm_scaled = x_scaled[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "            tof_scaled = x_scaled[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "        else:\n",
    "            imu_scaled = self.imu_scaler.transform(imu_unscaled)\n",
    "            thm_scaled = self.thm_scaler.transform(thm_unscaled)\n",
    "            tof_scaled = self.tof_scaler.transform(tof_unscaled)\n",
    "\n",
    "        combined = np.concatenate([imu_scaled, thm_scaled, tof_scaled], axis=1)\n",
    "        padded = np.zeros((self.pad_len, combined.shape[1]), dtype='float32')\n",
    "        seq_len = min(combined.shape[0], self.pad_len)\n",
    "        padded[:seq_len] = combined[:seq_len]\n",
    "        imu = padded[..., :self.imu_dim]\n",
    "        thm = padded[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "        tof = padded[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "        \n",
    "        return torch.from_numpy(imu).float().unsqueeze(0), torch.from_numpy(thm).float().unsqueeze(0), torch.from_numpy(tof).float().unsqueeze(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imu[idx], self.thm[idx], self.tof[idx], self.class_[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_)\n",
    "\n",
    "class CMIFoldDataset:\n",
    "    def __init__(self, data_path, config, full_dataset_function, n_folds=5, random_seed=0):\n",
    "        self.full_dataset = full_dataset_function(data_path=data_path, config=config)\n",
    "        self.imu_dim = self.full_dataset.imu_dim\n",
    "        self.thm_dim = self.full_dataset.thm_dim\n",
    "        self.tof_dim = self.full_dataset.tof_dim\n",
    "        self.le = self.full_dataset.le\n",
    "        self.class_names = self.full_dataset.le.classes_\n",
    "        self.class_weight = self.full_dataset.class_weight\n",
    "        self.n_folds = n_folds\n",
    "        self.skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
    "        self.folds = list(self.skf.split(np.arange(len(self.full_dataset)), np.array(self.full_dataset.dataset_indices)))\n",
    "    \n",
    "    def get_fold_datasets(self, fold_idx):\n",
    "        if self.folds is None or fold_idx >= self.n_folds:\n",
    "            return None, None\n",
    "        fold_train_idx, fold_valid_idx = self.folds[fold_idx]\n",
    "        return Subset(self.full_dataset, fold_train_idx), Subset(self.full_dataset, fold_valid_idx)\n",
    "\n",
    "    def print_fold_stats(self):\n",
    "        def get_label_counts(subset):\n",
    "            counts = {name: 0 for name in self.class_names}\n",
    "            if subset is None:\n",
    "                return counts\n",
    "            for idx in subset.indices:\n",
    "                label_idx = self.full_dataset.dataset_indices[idx]\n",
    "                counts[self.class_names[label_idx]] += 1\n",
    "            return counts\n",
    "        \n",
    "      \n",
    "        for fold_idx in range(self.n_folds):\n",
    "            train_fold, valid_fold = self.get_fold_datasets(fold_idx)\n",
    "            train_counts = get_label_counts(train_fold)\n",
    "            valid_counts = get_label_counts(valid_fold)\n",
    "                \n",
    "            print(f\"\\nFold {fold_idx + 1}:\")\n",
    "            print(f\"{'类别':<50} {'训练集':<10} {'验证集':<10}\")\n",
    "            for name in self.class_names:\n",
    "                print(f\"{name:<50} {train_counts[name]:<10} {valid_counts[name]:<10}\")\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction = 8):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction, bias=True)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, L)\n",
    "        se = F.adaptive_avg_pool1d(x, 1).squeeze(-1)      # -> (B, C)\n",
    "        se = F.relu(self.fc1(se), inplace=True)          # -> (B, C//r)\n",
    "        se = self.sigmoid(self.fc2(se)).unsqueeze(-1)    # -> (B, C, 1)\n",
    "        return x * se                \n",
    "\n",
    "class ResNetSEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, wd = 1e-4):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels,\n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels,\n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        # SE\n",
    "        self.se = SEBlock(out_channels)\n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
    "                          padding=0, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        identity = self.shortcut(x)              # (B, out, L)\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)                       # (B, out, L)\n",
    "        out = out + identity\n",
    "        return self.relu(out)\n",
    "\n",
    "class CMIModel(nn.Module):\n",
    "    def __init__(self, imu_dim, thm_dim, tof_dim, n_classes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.imu_branch = nn.Sequential(\n",
    "            self.residual_se_cnn_block(imu_dim, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"],\n",
    "                                       drop=kwargs[\"imu1_dropout\"]),\n",
    "            self.residual_se_cnn_block(kwargs[\"imu1_channels\"], kwargs[\"feat_dim\"], kwargs[\"imu2_layers\"],\n",
    "                                       drop=kwargs[\"imu2_dropout\"])\n",
    "        )\n",
    "\n",
    "        self.thm_branch = nn.Sequential(\n",
    "            nn.Conv1d(thm_dim, kwargs[\"thm1_channels\"], kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(kwargs[\"thm1_channels\"]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2, ceil_mode=True),\n",
    "            nn.Dropout(kwargs[\"thm1_dropout\"]),\n",
    "            \n",
    "            nn.Conv1d(kwargs[\"thm1_channels\"], kwargs[\"feat_dim\"], kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(kwargs[\"feat_dim\"]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2, ceil_mode=True),\n",
    "            nn.Dropout(kwargs[\"thm2_dropout\"])\n",
    "        )\n",
    "        \n",
    "        self.tof_branch = nn.Sequential(\n",
    "            nn.Conv1d(tof_dim, kwargs[\"tof1_channels\"], kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(kwargs[\"tof1_channels\"]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2, ceil_mode=True),\n",
    "            nn.Dropout(kwargs[\"tof1_dropout\"]),\n",
    "            \n",
    "            nn.Conv1d(kwargs[\"tof1_channels\"], kwargs[\"feat_dim\"], kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(kwargs[\"feat_dim\"]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2, ceil_mode=True),\n",
    "            nn.Dropout(kwargs[\"tof2_dropout\"])\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, kwargs[\"feat_dim\"]))\n",
    "        self.bert = BertModel(BertConfig(\n",
    "            hidden_size=kwargs[\"feat_dim\"],\n",
    "            num_hidden_layers=kwargs[\"bert_layers\"],\n",
    "            num_attention_heads=kwargs[\"bert_heads\"],\n",
    "            intermediate_size=kwargs[\"feat_dim\"]*4\n",
    "        ))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(kwargs[\"feat_dim\"], kwargs[\"cls1_channels\"], bias=False),\n",
    "            nn.BatchNorm1d(kwargs[\"cls1_channels\"]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(kwargs[\"cls1_dropout\"]),\n",
    "            nn.Linear(kwargs[\"cls1_channels\"], kwargs[\"cls2_channels\"], bias=False),\n",
    "            nn.BatchNorm1d(kwargs[\"cls2_channels\"]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(kwargs[\"cls2_dropout\"]),\n",
    "            nn.Linear(kwargs[\"cls2_channels\"], n_classes)\n",
    "        )\n",
    "    \n",
    "    def residual_se_cnn_block(self, in_channels, out_channels, num_layers, pool_size=2, drop=0.3, wd=1e-4):\n",
    "        return nn.Sequential(\n",
    "            *[ResNetSEBlock(in_channels=in_channels, out_channels=in_channels) for i in range(num_layers)],\n",
    "            ResNetSEBlock(in_channels, out_channels, wd=wd),\n",
    "            nn.MaxPool1d(pool_size),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "    \n",
    "    def forward(self, imu, thm, tof):\n",
    "        imu_feat = self.imu_branch(imu.permute(0, 2, 1))\n",
    "        thm_feat = self.thm_branch(thm.permute(0, 2, 1))\n",
    "        tof_feat = self.tof_branch(tof.permute(0, 2, 1))\n",
    "        \n",
    "        bert_input = torch.cat([imu_feat, thm_feat, tof_feat], dim=-1).permute(0, 2, 1)\n",
    "        cls_token = self.cls_token.expand(bert_input.size(0), -1, -1)  \n",
    "        bert_input = torch.cat([cls_token, bert_input], dim=1) \n",
    "        outputs = self.bert(inputs_embeds=bert_input)\n",
    "        pred_cls = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        return self.classifier(pred_cls)\n",
    "\n",
    "\n",
    "CUDA0 = \"cuda:0\"\n",
    "seed = 0\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "n_folds = 5\n",
    "\n",
    "universe_csv_path = Path(\"/kaggle/input/cmi-precompute/pytorch/all/1/tof-1_raw.csv\")\n",
    "\n",
    "deterministic = kagglehub.package_import('wasupandceacar/deterministic').deterministic\n",
    "deterministic.init_all(seed)\n",
    "def init_dataset():\n",
    "    dataset_config = {\n",
    "        \"percent\": 95,\n",
    "        \"scaler_function\": StandardScaler(),\n",
    "        \"nan_ratio\": {\n",
    "            \"imu\": 0,\n",
    "            \"thm\": 0,\n",
    "            \"tof\": 0,\n",
    "        },\n",
    "        \"fbfill\": {\n",
    "            \"imu\": True,\n",
    "            \"thm\": True,\n",
    "            \"tof\": True,\n",
    "        },\n",
    "        \"one_scale\": True,\n",
    "        \"tof_raw\": True,\n",
    "        \"tof_mode\": 16,\n",
    "        \"save_precompute\": False,\n",
    "    }\n",
    "    dataset = CMIFoldDataset(universe_csv_path, dataset_config,\n",
    "                             n_folds=n_folds, random_seed=seed, full_dataset_function=CMIFeDataset)\n",
    "    dataset.print_fold_stats()\n",
    "    return dataset\n",
    "\n",
    "def get_fold_dataset(dataset, fold):\n",
    "    _, valid_dataset = dataset.get_fold_datasets(fold)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    return valid_loader\n",
    "\n",
    "dataset = init_dataset()\n",
    "\n",
    "model_function = CMIModel\n",
    "model_args = {\"feat_dim\": 500,\n",
    "              \"imu1_channels\": 219, \"imu1_dropout\": 0.2946731587132302, \"imu2_dropout\": 0.2697745571929592,\n",
    "              \"imu1_weight_decay\": 0.0014824054650601245, \"imu2_weight_decay\": 0.002742543773142381,\n",
    "              \"imu1_layers\": 0, \"imu2_layers\": 0,\n",
    "              \"thm1_channels\": 82, \"thm1_dropout\": 0.2641274454844602, \"thm2_dropout\": 0.302896343020985, \n",
    "              \"tof1_channels\": 82, \"tof1_dropout\": 0.2641274454844602, \"tof2_dropout\": 0.3028963430209852, \n",
    "              \"bert_layers\": 8, \"bert_heads\": 10,\n",
    "              \"cls1_channels\": 937, \"cls2_channels\": 303, \"cls1_dropout\": 0.2281834512100508, \"cls2_dropout\": 0.22502521933558461}\n",
    "model_args.update({\n",
    "    \"imu_dim\": dataset.full_dataset.imu_dim, \n",
    "    \"thm_dim\": dataset.full_dataset.thm_dim,\n",
    "    \"tof_dim\": dataset.full_dataset.tof_dim,\n",
    "    \"n_classes\": dataset.full_dataset.class_num})\n",
    "model_dir = Path(\"/kaggle/input/cmi-models-public/pytorch/train_fold_model05_tof16_raw/1\")\n",
    "\n",
    "model_dicts = [\n",
    "    {\n",
    "        \"model_function\": model_function,\n",
    "        \"model_args\": model_args,\n",
    "        \"model_path\": model_dir / f\"fold{fold}/best_ema.pt\",\n",
    "    } for fold in range(n_folds)\n",
    "]\n",
    "\n",
    "models2 = list()\n",
    "for model_dict in model_dicts:\n",
    "    model_function = model_dict[\"model_function\"]\n",
    "    model_args = model_dict[\"model_args\"]\n",
    "    model_path = model_dict[\"model_path\"]\n",
    "    model = model_function(**model_args).to(CUDA0)\n",
    "    state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in torch.load(model_path).items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.eval()\n",
    "    models2.append(model)\n",
    "\n",
    "\n",
    "metric_package = kagglehub.package_import('wasupandceacar/cmi-metric')\n",
    "\n",
    "metric = metric_package.Metric()\n",
    "imu_only_metric = metric_package.Metric()\n",
    "\n",
    "def to_cuda(*tensors):\n",
    "    return [tensor.to(CUDA0) for tensor in tensors]\n",
    "\n",
    "def predict_valid(model, imu, thm, tof):\n",
    "    pred = model(imu, thm, tof)\n",
    "    return pred\n",
    "\n",
    "def valid(model, valid_bar):\n",
    "    with torch.no_grad():\n",
    "        for imu, thm, tof, y in valid_bar:\n",
    "            imu, thm, tof, y = to_cuda(imu, thm, tof, y)\n",
    "            with autocast(device_type='cuda', dtype=torch.bfloat16): \n",
    "                logits = predict_valid(model, imu, thm, tof)\n",
    "            metric.add(dataset.le.classes_[y.argmax(dim=1).cpu()], dataset.le.classes_[logits.argmax(dim=1).cpu()])\n",
    "            _, thm, tof = dataset.full_dataset.get_scaled_nan_tensors(imu, thm, tof)\n",
    "            with autocast(device_type='cuda', dtype=torch.bfloat16): \n",
    "                logits = model(imu, thm, tof)\n",
    "            imu_only_metric.add(dataset.le.classes_[y.argmax(dim=1).cpu()], dataset.le.classes_[logits.argmax(dim=1).cpu()])\n",
    "\n",
    "# for fold, model in enumerate(models2):\n",
    "#     valid_loader = get_fold_dataset(dataset, fold)\n",
    "#     valid_bar = tqdm(valid_loader, desc=f\"Valid\", position=0, leave=False)\n",
    "#     valid(model, valid_bar)\n",
    "\n",
    "# print(f\"\"\"\n",
    "# Normal score: {metric.score()}\n",
    "# IMU only score: {imu_only_metric.score()}\n",
    "# \"\"\")\n",
    "\n",
    "def avg_predict(models, imu, thm, tof):\n",
    "    outputs = []\n",
    "    with autocast(device_type='cuda'):\n",
    "        for model in models:\n",
    "            logits = model(imu, thm, tof)\n",
    "        outputs.append(logits)\n",
    "    return torch.mean(torch.stack(outputs), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f65e649",
   "metadata": {
    "_cell_guid": "aa3e6f52-ea3b-463d-8ba3-1b18a90a15fe",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "0e0d9d33-199d-4180-bd41-766e739ae8cb",
    "execution": {
     "iopub.execute_input": "2025-09-02T17:04:11.604695Z",
     "iopub.status.busy": "2025-09-02T17:04:11.604428Z",
     "iopub.status.idle": "2025-09-02T17:04:11.608782Z",
     "shell.execute_reply": "2025-09-02T17:04:11.608055Z"
    },
    "papermill": {
     "duration": 0.014496,
     "end_time": "2025-09-02T17:04:11.610020",
     "exception": false,
     "start_time": "2025-09-02T17:04:11.595524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_2\n",
    "\n",
    "def predict2(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    imu, thm, tof = dataset.full_dataset.inference_process(sequence)\n",
    "    with torch.no_grad():\n",
    "        imu, thm, tof = to_cuda(imu, thm, tof)\n",
    "        logits = avg_predict(models2, imu, thm, tof)\n",
    "        probabilities = F.softmax(logits, dim=1).cpu().numpy()\n",
    "    return probabilities # logits.cpu().numpy()\n",
    "    # return dataset.le.classes_[logits.argmax(dim=1).cpu()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1535f85f",
   "metadata": {
    "_cell_guid": "df7d9f50-50d9-424e-8253-c34c896807ec",
    "_uuid": "8a8aa3e9-bf4a-42d4-b8a6-d4e5ca173b20",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.007801,
     "end_time": "2025-09-02T17:04:11.626106",
     "exception": false,
     "start_time": "2025-09-02T17:04:11.618305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c6605fe",
   "metadata": {
    "_cell_guid": "a740302f-1c27-4df5-bd55-20a1349eedbb",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "4467066a-05f5-4c2a-bd78-3cfce5d5c291",
    "execution": {
     "iopub.execute_input": "2025-09-02T17:04:11.644489Z",
     "iopub.status.busy": "2025-09-02T17:04:11.644250Z",
     "iopub.status.idle": "2025-09-02T17:04:18.829590Z",
     "shell.execute_reply": "2025-09-02T17:04:18.828544Z"
    },
    "papermill": {
     "duration": 7.196495,
     "end_time": "2025-09-02T17:04:18.830876",
     "exception": false,
     "start_time": "2025-09-02T17:04:11.634381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompetitionMetric could not be imported. OOF/CV score will not be calculated.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m     os.environ[\u001b[33m'\u001b[39m\u001b[33mTF_CUDNN_DETERMINISTIC\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     20\u001b[39m     os.environ[\u001b[33m'\u001b[39m\u001b[33mTF_DETERMINISTIC_OPS\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mseed_everything\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m warnings.filterwarnings(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m TRAIN = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mseed_everything\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mseed_everything\u001b[39m(seed=\u001b[32m42\u001b[39m):\n\u001b[32m      8\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    実行環境の乱数シードを統一的に設定する関数。\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mos\u001b[49m.environ[\u001b[33m'\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(seed)\n\u001b[32m     12\u001b[39m     random.seed(seed)\n\u001b[32m     13\u001b[39m     np.random.seed(\u001b[32m2025\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    from cmi_2025_metric_copy_for_import import CompetitionMetric\n",
    "except ImportError:\n",
    "    CompetitionMetric = None\n",
    "    print(\"CompetitionMetric could not be imported. OOF/CV score will not be calculated.\")\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"\n",
    "    実行環境の乱数シードを統一的に設定する関数。\n",
    "    \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(2025)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "seed_everything(seed=42)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "TRAIN = False\n",
    "\n",
    "RAW_DIR = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\n",
    "YOUR_MODELS_DIR = Path(\"/kaggle/input/cmi-data-gated-gru\")\n",
    "PUBLIC_TF_MODEL_DIR = Path(\"/kaggle/input/lb-0-78-quaternions-tf-bilstm-gru-attention\")\n",
    "PUBLIC_PT_MODEL_DIR = Path(\"/kaggle/input/cmi3-models-p\")\n",
    "EXPORT_DIR = Path(\"./\")\n",
    "\n",
    "BATCH_SIZE = 64          \n",
    "PAD_PERCENTILE = 95      \n",
    "LR_INIT = 4e-4          \n",
    "WD = 3e-3               \n",
    "MIXUP_ALPHA = 0.4       \n",
    "EPOCHS = 360            \n",
    "PATIENCE = 50          \n",
    "N_SPLITS = 10         \n",
    "MASKING_PROB = 0.25     \n",
    "GATE_LOSS_WEIGHT = 0.2  \n",
    "\n",
    "print(f\"  - TensorFlow: {tf.__version__}\")\n",
    "print(f\"  - PyTorch: {torch.__version__}\")\n",
    "print(f\"▶ TRAIN: {TRAIN}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mean_pt = torch.tensor([\n",
    "    0, 0, 0, 0, 0, 0, 9.0319e-03, 1.0849e+00, -2.6186e-03, 3.7651e-03,\n",
    "    -5.3660e-03, -2.8177e-03, 1.3318e-03, -1.5876e-04, 6.3495e-01,\n",
    "    6.2877e-01, 6.0607e-01, 6.2142e-01, 6.3808e-01, 6.5420e-01,\n",
    "    7.4102e-03, -3.4159e-03, -7.5237e-03, -2.6034e-02, 2.9704e-02,\n",
    "    -3.1546e-02, -2.0610e-03, -4.6986e-03, -4.7216e-03, -2.6281e-02,\n",
    "    1.5799e-02, 1.0016e-02\n",
    "], dtype=torch.float32).view(1, -1, 1).to(device)\n",
    "\n",
    "std_pt = torch.tensor([\n",
    "    1, 1, 1, 1, 1, 1, 0.2067, 0.8583, 0.3162,\n",
    "    0.2668, 0.2917, 0.2341, 0.3023, 0.3281, 1.0264, 0.8838, 0.8686, 1.0973,\n",
    "    1.0267, 0.9018, 0.4658, 0.2009, 0.2057, 1.2240, 0.9535, 0.6655, 0.2941,\n",
    "    0.3421, 0.8156, 0.6565, 1.1034, 1.5577\n",
    "], dtype=torch.float32).view(1, -1, 1).to(device) + 1e-8\n",
    "\n",
    "class ImuFeatureExtractor(nn.Module):\n",
    "\n",
    "    def __init__(self, fs=100., add_quaternion=False):\n",
    "        super().__init__()\n",
    "        self.fs = fs\n",
    "        self.add_quaternion = add_quaternion\n",
    "\n",
    "        k = 15\n",
    "\n",
    "        self.lpf = nn.Conv1d(6, 6, kernel_size=k, padding=k//2,\n",
    "                                 groups=6, bias=False)\n",
    "        nn.init.kaiming_uniform_(self.lpf.weight, a=math.sqrt(5))\n",
    "\n",
    "        self.lpf_acc  = nn.Conv1d(3, 3, k, padding=k//2, groups=3, bias=False)\n",
    "        self.lpf_gyro = nn.Conv1d(3, 3, k, padding=k//2, groups=3, bias=False)\n",
    "\n",
    "    def forward(self, imu):\n",
    "        acc  = imu[:, 0:3, :]\n",
    "        gyro = imu[:, 3:6, :]\n",
    "\n",
    "        # 1) magnitude\n",
    "        acc_mag  = torch.norm(acc,  dim=1, keepdim=True)\n",
    "        gyro_mag = torch.norm(gyro, dim=1, keepdim=True)\n",
    "\n",
    "        # 2) jerk\n",
    "        jerk = F.pad(acc[:, :, 1:] - acc[:, :, :-1], (1,0))\n",
    "        gyro_delta = F.pad(gyro[:, :, 1:] - gyro[:, :, :-1], (1,0))\n",
    "\n",
    "        # 3) energy\n",
    "        acc_pow  = acc ** 2\n",
    "        gyro_pow = gyro ** 2\n",
    "\n",
    "        # 4) LPF / HPF\n",
    "        acc_lpf  = self.lpf_acc(acc)\n",
    "        acc_hpf  = acc - acc_lpf\n",
    "        gyro_lpf = self.lpf_gyro(gyro)\n",
    "        gyro_hpf = gyro - gyro_lpf\n",
    "\n",
    "        features = [\n",
    "            acc, gyro,\n",
    "            acc_mag, gyro_mag,\n",
    "            jerk, gyro_delta,\n",
    "            acc_pow, gyro_pow,\n",
    "            acc_lpf, acc_hpf,\n",
    "            gyro_lpf, gyro_hpf,\n",
    "        ]\n",
    "        return torch.cat(features, dim=1)\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool1d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False), nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        y = self.excitation(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class ResidualSECNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, pool_size=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.se = SEBlock(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(nn.Conv1d(in_channels, out_channels, 1, bias=False), nn.BatchNorm1d(out_channels))\n",
    "        self.pool = nn.MaxPool1d(pool_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)\n",
    "        out += self.shortcut(x)\n",
    "        return self.dropout(self.pool(F.relu(out)))\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, x):\n",
    "        scores = torch.tanh(self.attention(x))\n",
    "        weights = F.softmax(scores.squeeze(-1), dim=1)\n",
    "        return torch.sum(x * weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "class TwoBranchModel(nn.Module):\n",
    "    def __init__(self, pad_len, imu_dim_raw, tof_dim, n_classes, dropouts=[0.3, 0.3, 0.3, 0.3, 0.4, 0.5, 0.3], feature_engineering=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.feature_engineering = feature_engineering\n",
    "        imu_dim = 32 if feature_engineering else imu_dim_raw\n",
    "        self.imu_fe = ImuFeatureExtractor(**kwargs) if feature_engineering else nn.Identity()\n",
    "        self.fir_nchan = 7\n",
    "        numtaps = 33\n",
    "        fir_kernel = torch.tensor(firwin(numtaps, cutoff=1.0, fs=10.0, pass_zero=False), dtype=torch.float32).view(1, 1, -1).repeat(self.fir_nchan, 1, 1)\n",
    "        self.register_buffer(\"fir_kernel\", fir_kernel)\n",
    "        self.imu_block1 = ResidualSECNNBlock(imu_dim, 64, 3, dropout=dropouts[0])\n",
    "        self.imu_block2 = ResidualSECNNBlock(64, 128, 5, dropout=dropouts[1])\n",
    "        self.tof_conv1 = nn.Conv1d(tof_dim, 64, 3, padding=1, bias=False)\n",
    "        self.tof_bn1, self.tof_pool1, self.tof_drop1 = nn.BatchNorm1d(64), nn.MaxPool1d(2), nn.Dropout(dropouts[2])\n",
    "        self.tof_conv2 = nn.Conv1d(64, 128, 3, padding=1, bias=False)\n",
    "        self.tof_bn2, self.tof_pool2, self.tof_drop2 = nn.BatchNorm1d(128), nn.MaxPool1d(2), nn.Dropout(dropouts[3])\n",
    "        self.bilstm = nn.LSTM(256, 128, bidirectional=True, batch_first=True)\n",
    "        self.lstm_dropout = nn.Dropout(dropouts[4])\n",
    "        self.attention = AttentionLayer(256)\n",
    "        self.dense1, self.bn_dense1, self.drop1 = nn.Linear(256, 256, bias=False), nn.BatchNorm1d(256), nn.Dropout(dropouts[5])\n",
    "        self.dense2, self.bn_dense2, self.drop2 = nn.Linear(256, 128, bias=False), nn.BatchNorm1d(128), nn.Dropout(dropouts[6])\n",
    "        self.classifier = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        imu_raw = x[:, :, :self.fir_nchan].transpose(1, 2)\n",
    "        tof = x[:, :, self.fir_nchan:].transpose(1, 2)\n",
    "        imu_fe = self.imu_fe(imu_raw)\n",
    "        filtered = F.conv1d(imu_fe[:, :self.fir_nchan, :], self.fir_kernel, padding=self.fir_kernel.shape[-1] // 2, groups=self.fir_nchan)\n",
    "        imu = (torch.cat([filtered, imu_fe[:, self.fir_nchan:, :]], dim=1) - mean_pt) / std_pt\n",
    "        x1 = self.imu_block1(imu); x1 = self.imu_block2(x1)\n",
    "        x2 = self.tof_drop1(self.tof_pool1(F.relu(self.tof_bn1(self.tof_conv1(tof)))))\n",
    "        x2 = self.tof_drop2(self.tof_pool2(F.relu(self.tof_bn2(self.tof_conv2(x2)))))\n",
    "        merged = torch.cat([x1, x2], dim=1).transpose(1, 2)\n",
    "        lstm_out, _ = self.bilstm(merged); lstm_out = self.lstm_dropout(lstm_out)\n",
    "        attended = self.attention(lstm_out)\n",
    "        x = self.drop1(F.relu(self.bn_dense1(self.dense1(attended))))\n",
    "        x = self.drop2(F.relu(self.bn_dense2(self.dense2(x))))\n",
    "        return self.classifier(x)\n",
    "\n",
    "class PublicTwoBranchModel(nn.Module):\n",
    " \n",
    "    def __init__(self, pad_len, imu_dim_raw, tof_dim, n_classes, dropouts=[0.3, 0.3, 0.3, 0.3, 0.4, 0.5, 0.3], feature_engineering=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.feature_engineering = feature_engineering\n",
    "        imu_dim = 32 if feature_engineering else imu_dim_raw\n",
    "        self.imu_fe = ImuFeatureExtractor(**kwargs) if feature_engineering else nn.Identity()\n",
    "        self.fir_nchan = 7\n",
    "        numtaps = 33\n",
    "        fir_kernel = torch.tensor(firwin(numtaps, cutoff=1.0, fs=10.0, pass_zero=False), dtype=torch.float32).view(1, 1, -1).repeat(self.fir_nchan, 1, 1)\n",
    "        self.register_buffer(\"fir_kernel\", fir_kernel)\n",
    "        self.imu_block1 = ResidualSECNNBlock(imu_dim, 64, 3, dropout=dropouts[0])\n",
    "        self.imu_block2 = ResidualSECNNBlock(64, 128, 5, dropout=dropouts[1])\n",
    "        self.tof_conv1 = nn.Conv1d(tof_dim, 64, 3, padding=1, bias=False)\n",
    "        self.tof_bn1, self.tof_pool1, self.tof_drop1 = nn.BatchNorm1d(64), nn.MaxPool1d(2), nn.Dropout(dropouts[2])\n",
    "        self.tof_conv2 = nn.Conv1d(64, 128, 3, padding=1, bias=False)\n",
    "        self.tof_bn2, self.tof_pool2, self.tof_drop2 = nn.BatchNorm1d(128), nn.MaxPool1d(2), nn.Dropout(dropouts[3])\n",
    "        self.bilstm = nn.LSTM(256, 128, bidirectional=True, batch_first=True) \n",
    "        self.lstm_dropout = nn.Dropout(dropouts[4])\n",
    "        self.attention = AttentionLayer(256) \n",
    "        self.dense1, self.bn_dense1, self.drop1 = nn.Linear(256, 256, bias=False), nn.BatchNorm1d(256), nn.Dropout(dropouts[5])\n",
    "        self.dense2, self.bn_dense2, self.drop2 = nn.Linear(256, 128, bias=False), nn.BatchNorm1d(128), nn.Dropout(dropouts[6])\n",
    "        self.classifier = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        imu_raw = x[:, :, :self.fir_nchan].transpose(1, 2)\n",
    "        tof = x[:, :, self.fir_nchan:].transpose(1, 2)\n",
    "        imu_fe = self.imu_fe(imu_raw)\n",
    "        filtered = F.conv1d(imu_fe[:, :self.fir_nchan, :], self.fir_kernel, padding=self.fir_kernel.shape[-1] // 2, groups=self.fir_nchan)\n",
    "   \n",
    "        imu = (torch.cat([filtered, imu_fe[:, self.fir_nchan:, :]], dim=1) - mean_pt) / std_pt\n",
    "        x1 = self.imu_block1(imu); x1 = self.imu_block2(x1)\n",
    "        x2 = self.tof_drop1(self.tof_pool1(F.relu(self.tof_bn1(self.tof_conv1(tof)))))\n",
    "        x2 = self.tof_drop2(self.tof_pool2(F.relu(self.tof_bn2(self.tof_conv2(x2)))))\n",
    "        merged = torch.cat([x1, x2], dim=1).transpose(1, 2)\n",
    "        lstm_out, _ = self.bilstm(merged); lstm_out = self.lstm_dropout(lstm_out)\n",
    "        attended = self.attention(lstm_out)\n",
    "        x = self.drop1(F.relu(self.bn_dense1(self.dense1(attended))))\n",
    "        x = self.drop2(F.relu(self.bn_dense2(self.dense2(x))))\n",
    "        return self.classifier(x)\n",
    "\n",
    "def pad_sequences_torch3(sequences, maxlen, padding='post', truncating='post', value=0.0):\n",
    "    result = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) >= maxlen: seq = seq[:maxlen] if truncating == 'post' else seq[-maxlen:]\n",
    "        else:\n",
    "            pad_len = maxlen - len(seq)\n",
    "            pad_array = np.full((pad_len, seq.shape[1]), value)\n",
    "            seq = np.concatenate([seq, pad_array]) if padding == 'post' else np.concatenate([pad_array, seq])\n",
    "        result.append(seq)\n",
    "    return np.array(result, dtype=np.float32)\n",
    "\n",
    "\n",
    "def remove_gravity_from_acc3(acc_data, rot_data):\n",
    "    acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "    for i in range(len(acc_values)):\n",
    "        if np.all(np.isnan(quat_values[i])):\n",
    "            linear_accel[i, :] = acc_values[i, :]\n",
    "            continue\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except (ValueError, IndexError):\n",
    "            linear_accel[i, :] = acc_values[i, :]\n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat3(rot_data, time_delta=1/200):\n",
    "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    angular_vel = np.zeros((len(quat_values), 3))\n",
    "    for i in range(len(quat_values) - 1):\n",
    "        q_t, q_t_plus_dt = quat_values[i], quat_values[i+1]\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isnan(q_t_plus_dt)): continue\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except (ValueError, IndexError): pass\n",
    "    return angular_vel\n",
    "\n",
    "def calculate_angular_distance3(rot_data):\n",
    "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    angular_dist = np.zeros(len(quat_values))\n",
    "    for i in range(len(quat_values) - 1):\n",
    "        q1, q2 = quat_values[i], quat_values[i+1]\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isnan(q2)): continue\n",
    "        try:\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            angular_dist[i] = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "        except (ValueError, IndexError): pass\n",
    "    return angular_dist\n",
    "\n",
    "def time_sum(x): return K.sum(x, axis=1)\n",
    "def squeeze_last_axis(x): return tf.squeeze(x, axis=-1)\n",
    "def expand_last_axis(x): return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "def se_block(x, reduction=8):\n",
    "    ch = x.shape[-1]\n",
    "    se = GlobalAveragePooling1D()(x)\n",
    "    se = Dense(ch // reduction, activation='relu')(se)\n",
    "    se = Dense(ch, activation='sigmoid')(se)\n",
    "    se = Reshape((1, ch))(se)\n",
    "    return Multiply()([x, se])\n",
    "\n",
    "def residual_se_cnn_block(x, filters, kernel_size, pool_size=2, drop=0.3, wd=1e-4):\n",
    "    shortcut = x\n",
    "    # 2層のConv1D\n",
    "    for _ in range(2):\n",
    "        x = Conv1D(filters, kernel_size, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "    # SEブロック\n",
    "    x = se_block(x)\n",
    "    # ショートカット接続\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding='same', use_bias=False, kernel_regularizer=l2(wd))(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size)(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    return x\n",
    "\n",
    "def attention_layer(inputs):\n",
    "    score = Dense(1, activation='tanh')(inputs)\n",
    "    score = Lambda(squeeze_last_axis)(score)\n",
    "    weights = Activation('softmax')(score)\n",
    "    weights = Lambda(expand_last_axis)(weights)\n",
    "    context = Multiply()([inputs, weights])\n",
    "    context = Lambda(time_sum)(context)\n",
    "    return context\n",
    "\n",
    "class GatedMixupGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size, imu_dim, class_weight=None, alpha=0.2, masking_prob=0.0):\n",
    "        self.X, self.y, self.batch, self.imu_dim = X, y, batch_size, imu_dim\n",
    "        self.class_weight, self.alpha, self.masking_prob = class_weight, alpha, masking_prob\n",
    "        self.indices = np.arange(len(X))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i*self.batch:(i+1)*self.batch]\n",
    "        Xb, yb = self.X[idx].copy(), self.y[idx].copy()\n",
    "\n",
    "        sample_weights = np.ones(len(Xb), dtype='float32')\n",
    "        if self.class_weight:\n",
    "            sample_weights = np.array([self.class_weight.get(i, 1.0) for i in yb.argmax(axis=1)])\n",
    "\n",
    "        gate_target = np.ones(len(Xb), dtype='float32')\n",
    "        if self.masking_prob > 0:\n",
    "            for j in range(len(Xb)):\n",
    "                if np.random.rand() < self.masking_prob:\n",
    "                    Xb[j, :, self.imu_dim:] = 0\n",
    "                    gate_target[j] = 0.0\n",
    "\n",
    "        if self.alpha > 0:\n",
    "            lam = np.random.beta(self.alpha, self.alpha)\n",
    "            perm = np.random.permutation(len(Xb))\n",
    "            X_mix = lam * Xb + (1 - lam) * Xb[perm]\n",
    "            y_mix = lam * yb + (1 - lam) * yb[perm]\n",
    "            gate_target_mix = lam * gate_target + (1 - lam) * gate_target[perm]\n",
    "            sample_weights_mix = lam * sample_weights + (1 - lam) * sample_weights[perm]\n",
    "            return X_mix, {'main_output': y_mix, 'tof_gate': gate_target_mix}, sample_weights_mix\n",
    "\n",
    "        return Xb, {'main_output': yb, 'tof_gate': gate_target}, sample_weights\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "def build_gated_two_branch_model(pad_len, imu_dim, tof_dim, n_classes, wd=1e-4):\n",
    "   \n",
    "    inp = Input(shape=(pad_len, imu_dim + tof_dim))\n",
    "    imu = Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "\n",
    "    x1 = residual_se_cnn_block(imu, 64, 3, drop=0.1, wd=wd)\n",
    "    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.1, wd=wd)\n",
    "\n",
    "    x2_base = Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof)\n",
    "    x2_base = BatchNormalization()(x2_base); x2_base = Activation('relu')(x2_base)\n",
    "    x2_base = MaxPooling1D(2)(x2_base); x2_base = Dropout(0.2)(x2_base)\n",
    "    x2_base = Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x2_base)\n",
    "    x2_base = BatchNormalization()(x2_base); x2_base = Activation('relu')(x2_base)\n",
    "    x2_base = MaxPooling1D(2)(x2_base); x2_base = Dropout(0.2)(x2_base)\n",
    "\n",
    "    gate_input = GlobalAveragePooling1D()(tof)\n",
    "    gate_input = Dense(16, activation='relu')(gate_input)\n",
    "    gate = Dense(1, activation='sigmoid', name='tof_gate')(gate_input)\n",
    "    x2 = Multiply()([x2_base, gate])\n",
    "\n",
    "    merged = Concatenate()([x1, x2])\n",
    "    x = Bidirectional(GRU(256, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    x = Dropout(0.45)(x)\n",
    "    x = attention_layer(x)\n",
    "\n",
    "    for units, drop in [(512, 0.5), (256, 0.4), (128, 0.3)]:\n",
    "        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(drop)(x)\n",
    "\n",
    "    out = Dense(n_classes, activation='softmax', name='main_output', kernel_regularizer=l2(wd))(x)\n",
    "\n",
    "    return Model(inputs=inp, outputs=[out, gate])\n",
    "\n",
    "final_feature_cols_A = np.load(YOUR_MODELS_DIR / \"final_feature_cols.npy\", allow_pickle=True).tolist()\n",
    "pad_len_A = int(np.load(YOUR_MODELS_DIR / \"sequence_maxlen.npy\"))\n",
    "scaler_A = joblib.load(YOUR_MODELS_DIR / \"scaler.pkl\")\n",
    "gesture_classes = np.load(YOUR_MODELS_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "custom_objs_A = {'time_sum': time_sum, 'squeeze_last_axis': squeeze_last_axis, 'expand_last_axis': expand_last_axis,\n",
    "                 'se_block': se_block, 'residual_se_cnn_block': residual_se_cnn_block, 'attention_layer': attention_layer}\n",
    "models_A = [load_model(YOUR_MODELS_DIR / f\"final_model_fold_{f}.h5\", compile=False, custom_objects=custom_objs_A) for f in range(N_SPLITS)]\n",
    "\n",
    "final_feature_cols_B = np.load(PUBLIC_TF_MODEL_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "pad_len_B = int(np.load(PUBLIC_TF_MODEL_DIR / \"sequence_maxlen.npy\"))\n",
    "scaler_B = joblib.load(PUBLIC_TF_MODEL_DIR / \"scaler.pkl\")\n",
    "custom_objs_B = custom_objs_A \n",
    "model_B = load_model(PUBLIC_TF_MODEL_DIR / \"gesture_two_branch_mixup.h5\", compile=False, custom_objects=custom_objs_B)\n",
    "\n",
    "final_feature_cols_C = np.load(PUBLIC_PT_MODEL_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "pad_len_C = int(np.load(PUBLIC_PT_MODEL_DIR / \"sequence_maxlen.npy\"))\n",
    "scaler_C = joblib.load(PUBLIC_PT_MODEL_DIR / \"scaler.pkl\")\n",
    "\n",
    "pt_models = []\n",
    "for f in range(5):\n",
    "    checkpoint = torch.load(PUBLIC_PT_MODEL_DIR / f\"gesture_two_branch_fold{f}.pth\", map_location=device)\n",
    "    cfg = {'pad_len': checkpoint['pad_len'], 'imu_dim_raw': checkpoint['imu_dim'],\n",
    "           'tof_dim': checkpoint['tof_dim'], 'n_classes': checkpoint['n_classes']}\n",
    "    m = PublicTwoBranchModel(**cfg).to(device)\n",
    "    m.load_state_dict(checkpoint['model_state_dict'])\n",
    "    m.eval()\n",
    "    pt_models.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6f8b4f",
   "metadata": {
    "_cell_guid": "75f45de1-f56c-4f58-ae44-8edaf5b94855",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "3d540ab3-d32e-4f32-9cee-d37bc032b7de",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-02T17:04:18.848802Z",
     "iopub.status.busy": "2025-09-02T17:04:18.848255Z",
     "iopub.status.idle": "2025-09-02T17:04:18.865443Z",
     "shell.execute_reply": "2025-09-02T17:04:18.864752Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.027217,
     "end_time": "2025-09-02T17:04:18.866599",
     "exception": false,
     "start_time": "2025-09-02T17:04:18.839382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict3(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    df_seq_orig = sequence.to_pandas()\n",
    "    df_seq_A = df_seq_orig.copy()\n",
    "    \n",
    "    linear_accel_A = remove_gravity_from_acc3(df_seq_A[['acc_x','acc_y','acc_z']], df_seq_A[['rot_x','rot_y','rot_z','rot_w']])\n",
    "    df_seq_A['linear_acc_x'], df_seq_A['linear_acc_y'], df_seq_A['linear_acc_z'] = linear_accel_A[:,0], linear_accel_A[:,1], linear_accel_A[:,2]\n",
    "    df_seq_A['linear_acc_mag'] = np.linalg.norm(linear_accel_A, axis=1)\n",
    "    df_seq_A['linear_acc_mag_jerk'] = df_seq_A['linear_acc_mag'].diff().fillna(0)\n",
    "    angular_vel_A = calculate_angular_velocity_from_quat3(df_seq_A[['rot_x','rot_y','rot_z','rot_w']])\n",
    "    df_seq_A['angular_vel_x'], df_seq_A['angular_vel_y'], df_seq_A['angular_vel_z'] = angular_vel_A[:,0], angular_vel_A[:,1], angular_vel_A[:,2]\n",
    "    df_seq_A['angular_distance'] = calculate_angular_distance3(df_seq_A[['rot_x','rot_y','rot_z','rot_w']])\n",
    "    for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']:\n",
    "        df_seq_A[f'{col}_diff'] = df_seq_A[col].diff().fillna(0)\n",
    "    cols_for_stats=['linear_acc_mag','linear_acc_mag_jerk','angular_distance']\n",
    "    for col in cols_for_stats:\n",
    "        df_seq_A[f'{col}_skew'], df_seq_A[f'{col}_kurt'] = df_seq_A[col].skew(), df_seq_A[col].kurtosis()\n",
    "    for i in range(1,6):\n",
    "        if f'tof_{i}_v0' in df_seq_A.columns:\n",
    "            pixel_cols=[f\"tof_{i}_v{p}\" for p in range(64)]; tof_data=df_seq_A[pixel_cols].replace(-1,np.nan)\n",
    "            df_seq_A[f'tof_{i}_mean'], df_seq_A[f'tof_{i}_std'], df_seq_A[f'tof_{i}_min'], df_seq_A[f'tof_{i}_max'] = tof_data.mean(axis=1),tof_data.std(axis=1),tof_data.min(axis=1),tof_data.max(axis=1)\n",
    "    tof_mean_cols=[f'tof_{i}_mean' for i in range(1,6) if f'tof_{i}_mean' in df_seq_A.columns]\n",
    "    if tof_mean_cols:\n",
    "        df_seq_A['tof_std_across_sensors']=df_seq_A[tof_mean_cols].std(axis=1)\n",
    "        df_seq_A['tof_range_across_sensors']=df_seq_A[tof_mean_cols].max(axis=1)-df_seq_A[tof_mean_cols].min(axis=1)\n",
    "    thm_cols=[f'thm_{i}' for i in range(1,6) if f'thm_{i}' in df_seq_A.columns]\n",
    "    if thm_cols:\n",
    "        df_seq_A['thm_std_across_sensors']=df_seq_A[thm_cols].std(axis=1)\n",
    "        df_seq_A['thm_range_across_sensors']=df_seq_A[thm_cols].max(axis=1)-df_seq_A[thm_cols].min(axis=1)\n",
    "    mat_A = df_seq_A[final_feature_cols_A].ffill().bfill().fillna(0).values.astype('float32')\n",
    "    mat_A = scaler_A.transform(mat_A)\n",
    "    pad_input_A = pad_sequences([mat_A], maxlen=pad_len_A, padding='post', dtype='float32')\n",
    "    preds_A_folds = [model.predict(pad_input_A, verbose=0)[0] for model in models_A]\n",
    "    avg_pred_A = np.mean(preds_A_folds, axis=0)\n",
    "\n",
    "    df_seq_B = df_seq_orig.copy()\n",
    "    df_seq_B['acc_mag']=np.sqrt(df_seq_B['acc_x']**2+df_seq_B['acc_y']**2+df_seq_B['acc_z']**2)\n",
    "    df_seq_B['rot_angle']=2*np.arccos(df_seq_B['rot_w'].clip(-1,1))\n",
    "    df_seq_B['acc_mag_jerk']=df_seq_B['acc_mag'].diff().fillna(0)\n",
    "    df_seq_B['rot_angle_vel']=df_seq_B['rot_angle'].diff().fillna(0)\n",
    "    linear_accel_B=remove_gravity_from_acc3(df_seq_B,df_seq_B)\n",
    "    df_seq_B['linear_acc_x'],df_seq_B['linear_acc_y'],df_seq_B['linear_acc_z']=linear_accel_B[:,0],linear_accel_B[:,1],linear_accel_B[:,2]\n",
    "    df_seq_B['linear_acc_mag']=np.sqrt(df_seq_B['linear_acc_x']**2+df_seq_B['linear_acc_y']**2+df_seq_B['linear_acc_z']**2)\n",
    "    df_seq_B['linear_acc_mag_jerk']=df_seq_B['linear_acc_mag'].diff().fillna(0)\n",
    "    angular_vel_B=calculate_angular_velocity_from_quat3(df_seq_B)\n",
    "    df_seq_B['angular_vel_x'],df_seq_B['angular_vel_y'],df_seq_B['angular_vel_z']=angular_vel_B[:,0],angular_vel_B[:,1],angular_vel_B[:,2]\n",
    "    df_seq_B['angular_distance']=calculate_angular_distance3(df_seq_B)\n",
    "    for i in range(1,6):\n",
    "        if f'tof_{i}_v0' in df_seq_B.columns:\n",
    "            pixel_cols=[f\"tof_{i}_v{p}\" for p in range(64)]; tof_data=df_seq_B[pixel_cols].replace(-1,np.nan)\n",
    "            df_seq_B[f\"tof_{i}_mean\"],df_seq_B[f\"tof_{i}_std\"],df_seq_B[f\"tof_{i}_min\"],df_seq_B[f\"tof_{i}_max\"]=tof_data.mean(axis=1),tof_data.std(axis=1),tof_data.min(axis=1),tof_data.max(axis=1)\n",
    "    mat_B = df_seq_B[final_feature_cols_B].ffill().bfill().fillna(0).values.astype('float32')\n",
    "    mat_B = scaler_B.transform(mat_B)\n",
    "    pad_input_B = pad_sequences([mat_B], maxlen=pad_len_B, padding='post', dtype='float32')\n",
    "    pred_B = model_B.predict(pad_input_B, verbose=0)\n",
    "    if isinstance(pred_B, list): pred_B = pred_B[0]\n",
    "\n",
    "    df_seq_C = df_seq_orig.copy()\n",
    "    mat_C = df_seq_C[final_feature_cols_C].ffill().bfill().fillna(0).values.astype('float32')\n",
    "    mat_C = scaler_C.transform(mat_C)\n",
    "    pad_input_C = pad_sequences_torch3([mat_C], maxlen=pad_len_C, padding='pre', truncating='pre')\n",
    "    with torch.no_grad():\n",
    "        pt_input = torch.from_numpy(pad_input_C).to(device)\n",
    "        preds_C_folds = [model(pt_input) for model in pt_models]\n",
    "        avg_pred_C_logits = torch.mean(torch.stack(preds_C_folds), dim=0)\n",
    "        avg_pred_C = torch.softmax(avg_pred_C_logits, dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "    \n",
    "    weights = {'A': 0.50, 'B': 0.20, 'C': 0.30}\n",
    "\n",
    "    final_pred_proba = (weights['A'] * avg_pred_A + weights['B'] * pred_B + weights['C'] * avg_pred_C)\n",
    "\n",
    "    return final_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87798d64",
   "metadata": {
    "_cell_guid": "bc59ea4f-069b-4465-a0dc-82c911bce8f1",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "5a1e7f85-d0a6-4dee-b37a-73486b8d9166",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-02T17:04:18.883546Z",
     "iopub.status.busy": "2025-09-02T17:04:18.883296Z",
     "iopub.status.idle": "2025-09-02T17:04:18.891877Z",
     "shell.execute_reply": "2025-09-02T17:04:18.891010Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.018374,
     "end_time": "2025-09-02T17:04:18.893156",
     "exception": false,
     "start_time": "2025-09-02T17:04:18.874782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'w': 0.274, 'p': 1, 'n': 'p0'}, {'w': 0.342, 'p': 2, 'n': 'p1'}, {'w': 0.382, 'p': 3, 'n': 'p2'}] \n",
      "\n",
      " [{'w': 0.382, 'p': 3, 'n': 'p2'}, {'w': 0.342, 'p': 2, 'n': 'p1'}, {'w': 0.274, 'p': 1, 'n': 'p0'}] \n",
      "-----------\n",
      "[{'w': 0.28500000000000003, 'p': 1, 'n': 'p0'}, {'w': 0.338, 'p': 2, 'n': 'p1'}, {'w': 0.375, 'p': 3, 'n': 'p2'}] \n",
      "\n",
      " [{'w': 0.393, 'p': 3, 'n': 'p2'}, {'w': 0.338, 'p': 2, 'n': 'p1'}, {'w': 0.267, 'p': 1, 'n': 'p0'}]\n",
      "-----------\n",
      "[{'w': 0.28500000000000003, 'p': 1, 'n': 'p0'}, {'w': 0.338, 'p': 2, 'n': 'p1'}, {'w': 0.375, 'p': 3, 'n': 'p2'}] \n",
      "\n",
      " [{'w': 0.267, 'p': 1, 'n': 'p0'}, {'w': 0.338, 'p': 2, 'n': 'p1'}, {'w': 0.393, 'p': 3, 'n': 'p2'}]\n",
      "-----------\n",
      "[{'w': 0.28482, 'p': 1, 'n': 'p0'}, {'w': 0.338, 'p': 2, 'n': 'p1'}, {'w': 0.37517999999999996, 'p': 3, 'n': 'p2'}]\n",
      "-----------\n",
      "[0.28482, 0.676, 1.12554]\n"
     ]
    }
   ],
   "source": [
    "# help func, example\n",
    "\n",
    "pred0,pred1,pred2, ws, cws, aws = 1,2,3, [0.274,0.342,0.382], [+0.011, -0.004, -0.007], [0.99, 0.01]\n",
    "\n",
    "lp = [{ 'w':ws[0], 'p':pred0, 'n':'p0' },\n",
    "      { 'w':ws[1], 'p':pred1, 'n':'p1' },\n",
    "      { 'w':ws[2], 'p':pred2, 'n':'p2' }] \n",
    "\n",
    "lps_asc  = [{'w':p['w'], 'p':p['p'], 'n':p['n']} for p in lp]\n",
    "lps_desc = [{'w':p['w'], 'p':p['p'], 'n':p['n']} for p in lp]\n",
    "\n",
    "lps_asc  = sorted(lps_asc,  key=lambda k:k['p'],reverse=False)\n",
    "lps_desc = sorted(lps_desc, key=lambda k:k['p'],reverse=True)\n",
    "\n",
    "print(lps_asc, \"\\n\\n\", lps_desc, \"\") \n",
    "\n",
    "for p,cw in zip(lps_asc,  cws): p['w'] += cw\n",
    "for p,cw in zip(lps_desc, cws): p['w'] += cw\n",
    "    \n",
    "print(\"-\"*11)\n",
    "print(lps_asc, \"\\n\\n\", lps_desc)     \n",
    "\n",
    "lps_asc  = sorted(lps_asc,  key=lambda k:k['n'],reverse=False)\n",
    "lps_desc = sorted(lps_desc, key=lambda k:k['n'],reverse=False)\n",
    "\n",
    "print(\"-\"*11)\n",
    "print(lps_asc, \"\\n\\n\", lps_desc)     \n",
    "\n",
    "lps = []\n",
    "\n",
    "for a,d in zip(lps_asc, lps_desc):\n",
    "    one_dict = {\n",
    "        'w':a['w']* aws[0]+aws[1] *d['w'],\n",
    "        'p':a['p'],\n",
    "        'n':a['n']\n",
    "    }\n",
    "    lps.append(one_dict)\n",
    "\n",
    "print(\"-\"*11)                       \n",
    "print(lps)\n",
    "\n",
    "wps = [ps[\"w\"]*ps[\"p\"] for ps in lps]\n",
    "\n",
    "print(\"-\"*11)                       \n",
    "print(wps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b82cc19",
   "metadata": {
    "papermill": {
     "duration": 0.008257,
     "end_time": "2025-09-02T17:04:18.909657",
     "exception": false,
     "start_time": "2025-09-02T17:04:18.901400",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 4: Split-Sensor Architecture (LB 0.841)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a214228",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T17:04:18.926816Z",
     "iopub.status.busy": "2025-09-02T17:04:18.926564Z",
     "iopub.status.idle": "2025-09-02T17:04:18.938870Z",
     "shell.execute_reply": "2025-09-02T17:04:18.938268Z"
    },
    "papermill": {
     "duration": 0.022219,
     "end_time": "2025-09-02T17:04:18.939875",
     "exception": false,
     "start_time": "2025-09-02T17:04:18.917656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CMIFoldDataset4:\n",
    "    def __init__(self, data_path, config, full_dataset_function, n_folds=5, random_seed=0):\n",
    "        self.full_dataset = full_dataset_function(data_path=data_path, config=config)\n",
    "        self.imu_dim = self.full_dataset.imu_dim\n",
    "        self.thm_dim = self.full_dataset.thm_dim\n",
    "        self.tof_dim = self.full_dataset.tof_dim\n",
    "        self.le = self.full_dataset.le\n",
    "        self.class_names = self.full_dataset.le.classes_\n",
    "        self.class_weight = self.full_dataset.class_weight\n",
    "        self.n_folds = n_folds\n",
    "        self.sgkf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
    "        self.fold_y = np.array(self.full_dataset.fold_feats[config.get(\"fold_y\", \"sequence_type\")])\n",
    "        self.fold_groups = np.array(self.full_dataset.fold_feats[config.get(\"fold_groups\", \"subject\")])\n",
    "        self.folds = list(self.sgkf.split(X=np.arange(len(self.full_dataset)), y=self.fold_y, groups=self.fold_groups))\n",
    "        self.exclude_subjects = set(config.get(\"exclude_subjects\", []))\n",
    "    \n",
    "    def get_fold_datasets(self, fold_idx):\n",
    "        if self.folds is None or fold_idx >= self.n_folds: return None, None\n",
    "        fold_train_idx, fold_valid_idx = self.folds[fold_idx]\n",
    "        subjects = np.array(self.full_dataset.fold_feats[\"subject\"])\n",
    "        train_subjects, valid_subjects = subjects[fold_train_idx], subjects[fold_valid_idx]\n",
    "        train_mask, valid_mask = ~np.isin(train_subjects, list(self.exclude_subjects)), ~np.isin(valid_subjects, list(self.exclude_subjects))\n",
    "        return Subset(self.full_dataset, np.array(fold_train_idx)[train_mask].tolist()), Subset(self.full_dataset, np.array(fold_valid_idx)[valid_mask].tolist())\n",
    "\n",
    "    def print_fold_stats(self):\n",
    "        def get_label_counts(subset):\n",
    "            counts = {name: 0 for name in self.class_names}\n",
    "            if subset is None: return counts\n",
    "            for idx in subset.indices:\n",
    "                label_idx = self.full_dataset.dataset_indices[idx]\n",
    "                counts[self.class_names[label_idx]] += 1\n",
    "            return counts\n",
    "        \n",
    "        \n",
    "        for fold_idx in range(self.n_folds):\n",
    "            train_fold, valid_fold = self.get_fold_datasets(fold_idx)\n",
    "            train_counts = get_label_counts(train_fold)\n",
    "            valid_counts = get_label_counts(valid_fold)\n",
    "            for name in self.class_names:\n",
    "                print(f\"{name:<50} {train_counts[name]:<10} {valid_counts[name]:<10}\")\n",
    "\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(self.folds):\n",
    "            train_subjects = set(self.fold_groups[train_idx])\n",
    "            val_subjects = set(self.fold_groups[val_idx])\n",
    "            print(f\"\\nFold {fold_idx + 1}:\")\n",
    "\n",
    "        self.print_filtered_stats()\n",
    "\n",
    "    def print_filtered_stats(self):\n",
    "        original_counts = defaultdict(int)\n",
    "        filtered_counts = defaultdict(int)\n",
    "        \n",
    "        for fold_idx in range(self.n_folds):\n",
    "            train_idx, val_idx = self.folds[fold_idx]\n",
    "            for idx in train_idx:\n",
    "                original_counts['train'] += 1\n",
    "            for idx in val_idx:\n",
    "                original_counts['valid'] += 1\n",
    "            train_set, val_set = self.get_fold_datasets(fold_idx)\n",
    "            filtered_counts['train'] += len(train_set)\n",
    "            filtered_counts['valid'] += len(val_set)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c084c054",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T17:04:18.956536Z",
     "iopub.status.busy": "2025-09-02T17:04:18.956292Z",
     "iopub.status.idle": "2025-09-02T17:04:18.966041Z",
     "shell.execute_reply": "2025-09-02T17:04:18.965498Z"
    },
    "papermill": {
     "duration": 0.019258,
     "end_time": "2025-09-02T17:04:18.967058",
     "exception": false,
     "start_time": "2025-09-02T17:04:18.947800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model 4 imports and classes\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast\n",
    "from collections import defaultdict\n",
    "\n",
    "# Model 4 SE Block and ResNet SE Block\n",
    "class SEBlock4(nn.Module):\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction, bias=True)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, L)\n",
    "        se = F.adaptive_avg_pool1d(x, 1).squeeze(-1)      # -> (B, C)\n",
    "        se = F.relu(self.fc1(se), inplace=True)          # -> (B, C//r)\n",
    "        se = self.sigmoid(self.fc2(se)).unsqueeze(-1)    # -> (B, C, 1)\n",
    "        return x * se                \n",
    "\n",
    "class ResNetSEBlock4(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, wd=1e-4):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels,\n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels,\n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        # SE\n",
    "        self.se = SEBlock4(out_channels)\n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
    "                          padding=0, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)\n",
    "        out += residual\n",
    "        return F.relu(out, inplace=True)\n",
    "\n",
    "class GaussianNoise4(nn.Module):\n",
    "    def __init__(self, stddev):\n",
    "        super().__init__()\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            return x + torch.randn_like(x) * self.stddev\n",
    "        return x\n",
    "\n",
    "class AttentionLayer4(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.score_fn = nn.Linear(feature_dim, 1, bias=True)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, F)\n",
    "        score = torch.tanh(self.score_fn(x))     # (B, L, 1)\n",
    "        weights = self.softmax(score.squeeze(-1))# (B, L)\n",
    "        weights = weights.unsqueeze(-1)          # (B, L, 1)\n",
    "        context = x * weights                    # (B, L, F)\n",
    "        return context.sum(dim=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9adba27b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T17:04:18.983988Z",
     "iopub.status.busy": "2025-09-02T17:04:18.983781Z",
     "iopub.status.idle": "2025-09-02T17:04:19.000713Z",
     "shell.execute_reply": "2025-09-02T17:04:19.000195Z"
    },
    "papermill": {
     "duration": 0.026762,
     "end_time": "2025-09-02T17:04:19.001750",
     "exception": false,
     "start_time": "2025-09-02T17:04:18.974988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CMIBackbone4(nn.Module):\n",
    "    def __init__(self, imu_dim, thm_dim, tof_dim, **kwargs):\n",
    "        super().__init__()\n",
    "        self.imu_acc_branch = nn.Sequential(\n",
    "            self.residual_feature_block(3, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"], drop=kwargs[\"imu1_dropout\"]),\n",
    "            self.residual_feature_block(kwargs[\"imu1_channels\"], kwargs[\"imu2_channels\"], kwargs[\"imu2_layers\"], drop=kwargs[\"imu2_dropout\"])\n",
    "        )\n",
    "        self.imu_rot_branch = nn.Sequential(\n",
    "            self.residual_feature_block(4, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"], drop=kwargs[\"imu1_dropout\"]),\n",
    "            self.residual_feature_block(kwargs[\"imu1_channels\"], kwargs[\"imu2_channels\"], kwargs[\"imu2_layers\"], drop=kwargs[\"imu2_dropout\"])\n",
    "        )\n",
    "        self.imu_other_branch = nn.Sequential(\n",
    "            self.residual_feature_block(imu_dim-7, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"], drop=kwargs[\"imu1_dropout\"]),\n",
    "            self.residual_feature_block(kwargs[\"imu1_channels\"], kwargs[\"imu2_channels\"], kwargs[\"imu2_layers\"], drop=kwargs[\"imu2_dropout\"])\n",
    "        )\n",
    "\n",
    "        self.thm_branch1, self.tof_branch1 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "        self.thm_branch2, self.tof_branch2 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "        self.thm_branch3, self.tof_branch3 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "        self.thm_branch4, self.tof_branch4 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "        self.thm_branch5, self.tof_branch5 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n",
    "\n",
    "        self.imu_proj = ResNetSEBlock4(in_channels=3*kwargs[\"imu2_channels\"], out_channels=kwargs[\"imu2_channels\"])\n",
    "        self.thm_proj = ResNetSEBlock4(in_channels=5*kwargs[\"thm2_channels\"], out_channels=kwargs[\"thm2_channels\"])\n",
    "        self.tof_proj = ResNetSEBlock4(in_channels=5*kwargs[\"tof2_channels\"], out_channels=kwargs[\"tof2_channels\"])\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=kwargs['imu2_channels']+kwargs['thm2_channels']+kwargs['tof2_channels'],\n",
    "            hidden_size=kwargs['lstm_hidden_size'],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=kwargs['imu2_channels']+kwargs['thm2_channels']+kwargs['tof2_channels'],\n",
    "            hidden_size=kwargs['gru_hidden_size'],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.noise = GaussianNoise4(kwargs['gaussian_noise_rate'])\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(kwargs['imu2_channels']+kwargs['thm2_channels']+kwargs['tof2_channels'], kwargs['dense_channels']),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        self.attn = AttentionLayer4(feature_dim=(kwargs['lstm_hidden_size']+kwargs['gru_hidden_size'])*2+kwargs['dense_channels'])  # lstm + gru + dense\n",
    "\n",
    "    def feature_block(self, in_channels, out_channels, num_layers, pool_size=2, drop=0.3):\n",
    "        return nn.Sequential(\n",
    "            *[ResNetSEBlock4(in_channels=in_channels, out_channels=in_channels) for i in range(num_layers)],\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(pool_size, ceil_mode=True),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "    def residual_feature_block(self, in_channels, out_channels, num_layers, pool_size=2, drop=0.3):\n",
    "        return nn.Sequential(\n",
    "            *[ResNetSEBlock4(in_channels=in_channels, out_channels=in_channels) for i in range(num_layers)],\n",
    "            ResNetSEBlock4(in_channels, out_channels, wd=1e-4),\n",
    "            nn.MaxPool1d(pool_size, ceil_mode=True),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "    def init_thm_tof_branch(self, thm_dim, tof_dim, **kwargs):\n",
    "        thm_branch = nn.Sequential(\n",
    "            self.feature_block(thm_dim, kwargs[\"thm1_channels\"], kwargs[\"thm1_layers\"], drop=kwargs[\"thm1_dropout\"]),\n",
    "            self.feature_block(kwargs[\"thm1_channels\"], kwargs[\"thm2_channels\"], kwargs[\"thm2_layers\"], drop=kwargs[\"thm2_dropout\"]),\n",
    "        )\n",
    "        tof_branch = nn.Sequential(\n",
    "            self.feature_block(tof_dim, kwargs[\"tof1_channels\"], kwargs[\"tof1_layers\"], drop=kwargs[\"tof1_dropout\"]),\n",
    "            self.feature_block(kwargs[\"tof1_channels\"], kwargs[\"tof2_channels\"], kwargs[\"tof2_layers\"], drop=kwargs[\"tof2_dropout\"]),\n",
    "        )\n",
    "        return thm_branch, tof_branch\n",
    "    \n",
    "    def forward(self, imus, thms, tofs):\n",
    "        imu_acc, imu_rot, imu_other = imus\n",
    "        imu_acc_feat = self.imu_acc_branch(imu_acc.permute(0, 2, 1))\n",
    "        imu_rot_feat = self.imu_rot_branch(imu_rot.permute(0, 2, 1))\n",
    "        imu_other_feat = self.imu_other_branch(imu_other.permute(0, 2, 1))\n",
    "        imu_feat = self.imu_proj(torch.cat([imu_acc_feat, imu_rot_feat, imu_other_feat], dim=1))\n",
    "        \n",
    "        thm1, thm2, thm3, thm4, thm5 = thms\n",
    "        tof1, tof2, tof3, tof4, tof5 = tofs\n",
    "        \n",
    "        thm1_feat = self.thm_branch1(thm1.permute(0, 2, 1))\n",
    "        thm2_feat = self.thm_branch2(thm2.permute(0, 2, 1))\n",
    "        thm3_feat = self.thm_branch3(thm3.permute(0, 2, 1))\n",
    "        thm4_feat = self.thm_branch4(thm4.permute(0, 2, 1))\n",
    "        thm5_feat = self.thm_branch5(thm5.permute(0, 2, 1))\n",
    "        thm_feat = self.thm_proj(torch.cat([thm1_feat, thm2_feat, thm3_feat, thm4_feat, thm5_feat], dim=1))\n",
    "        \n",
    "        tof1_feat = self.tof_branch1(tof1.permute(0, 2, 1))\n",
    "        tof2_feat = self.tof_branch2(tof2.permute(0, 2, 1))\n",
    "        tof3_feat = self.tof_branch3(tof3.permute(0, 2, 1))\n",
    "        tof4_feat = self.tof_branch4(tof4.permute(0, 2, 1))\n",
    "        tof5_feat = self.tof_branch5(tof5.permute(0, 2, 1))\n",
    "        tof_feat = self.tof_proj(torch.cat([tof1_feat, tof2_feat, tof3_feat, tof4_feat, tof5_feat], dim=1))\n",
    "        \n",
    "        feat = torch.cat([imu_feat, thm_feat, tof_feat], dim=1).permute(0, 2, 1)\n",
    "        lstm_out, _ = self.lstm(feat)\n",
    "        gru_out, _ = self.gru(feat)\n",
    "        dense_out = self.dense(self.noise(feat))\n",
    "        \n",
    "        return self.attn(torch.cat([lstm_out, gru_out, dense_out], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0364f22a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T17:04:19.018965Z",
     "iopub.status.busy": "2025-09-02T17:04:19.018775Z",
     "iopub.status.idle": "2025-09-02T17:04:19.090870Z",
     "shell.execute_reply": "2025-09-02T17:04:19.090239Z"
    },
    "papermill": {
     "duration": 0.082477,
     "end_time": "2025-09-02T17:04:19.092060",
     "exception": false,
     "start_time": "2025-09-02T17:04:19.009583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CMIFeDataset4(Dataset):\n",
    "    def __init__(self, data_path, config):\n",
    "        self.config = config\n",
    "        self.init_feature_names(data_path)\n",
    "        df = self.generate_features(pd.read_csv(data_path, usecols=set(self.use_cols) & set(self.raw_columns)))\n",
    "        self.generate_dataset(df)\n",
    "\n",
    "    def init_feature_names(self, data_path):\n",
    "        self.target_gestures = [\n",
    "            'Above ear - pull hair',\n",
    "            'Cheek - pinch skin',\n",
    "            'Eyebrow - pull hair',\n",
    "            'Eyelash - pull hair',\n",
    "            'Forehead - pull hairline',\n",
    "            'Forehead - scratch',\n",
    "            'Neck - pinch skin',\n",
    "            'Neck - scratch',\n",
    "        ]\n",
    "        self.non_target_gestures = [\n",
    "            'Write name on leg',\n",
    "            'Wave hello',\n",
    "            'Glasses on/off',\n",
    "            'Text on phone',\n",
    "            'Write name in air',\n",
    "            'Feel around in tray and pull out an object',\n",
    "            'Scratch knee/leg skin',\n",
    "            'Pull air toward your face',\n",
    "            'Drink from bottle/cup',\n",
    "            'Pinch knee/leg skin'\n",
    "        ]\n",
    "\n",
    "        self.acc_features = ['acc_mag', 'acc_mag_jerk', 'linear_acc_mag', 'linear_acc_mag_jerk']\n",
    "        self.rot_features = ['rot_angle', 'rot_angle_vel', 'angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_distance']\n",
    "        self.old_imu_features = [\n",
    "            'acc_mag', 'rot_angle','acc_mag_jerk', 'rot_angle_vel',\n",
    "            'linear_acc_mag', 'linear_acc_mag_jerk',\n",
    "            'angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_distance'\n",
    "        ]\n",
    "\n",
    "        self.extra_imu_features = self.config.get(\"imu_feats\", [])\n",
    "        self.imu_features = self.extra_imu_features.copy()\n",
    "        if self.config.get(\"add_imu_feat_default\", True):\n",
    "            if self.config.get(\"old_imu_feat\", True):\n",
    "                self.imu_features.extend(self.old_imu_features)\n",
    "            else:\n",
    "                self.imu_features.extend(self.acc_features)\n",
    "                self.imu_features.extend(self.rot_features)\n",
    "        self.er1_fearues = [\"er_x\", \"er_y\", \"er_z\"]\n",
    "        self.er2_fearues = ['er_r_xy', 'er_r_xz', 'er_r_yz', 'er_c_xy', 'er_c_xz', 'er_c_yz']\n",
    "        self.er_fearues = self.er1_fearues + self.er2_fearues\n",
    "        self.tof_mode = self.config.get(\"tof_mode\", \"stats\")\n",
    "        self.tof_region_stats = ['mean', 'std', 'min', 'max']\n",
    "        self.tof_cols = self.generate_tof_feature_names()\n",
    "\n",
    "        self.raw_columns = pd.read_csv(data_path, nrows=0).columns.tolist()\n",
    "        self.imu_acc_cols_base = ['acc_x', 'acc_y', 'acc_z', 'linear_acc_x', 'linear_acc_y', 'linear_acc_z'] if self.config.get(\"add_raw_acc\", False) else ['linear_acc_x', 'linear_acc_y', 'linear_acc_z']\n",
    "        self.imu_rot_cols_base = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n",
    "        self.imu_cols_base = self.imu_acc_cols_base + self.imu_rot_cols_base\n",
    "        self.imu_cols = list()\n",
    "        self.imu_channel_keys = defaultdict(list)\n",
    "        if self.config.get(\"add_imu_base\", True): \n",
    "            self.imu_cols.extend(self.imu_cols_base)\n",
    "            self.imu_channel_keys[\"acc\"] = self.imu_acc_cols_base\n",
    "            self.imu_channel_keys[\"rot\"] = self.imu_rot_cols_base\n",
    "        if self.config.get(\"add_imu_feats\", True): \n",
    "            self.imu_cols.extend(self.imu_features)\n",
    "            if self.config.get(\"split_imu_feat\", False):\n",
    "                if self.config.get(\"old_imu_feat\", True):\n",
    "                    assert False, \"split_imu_feat=True and old_imu_feat=True not supported\"\n",
    "                self.imu_channel_keys[\"acc_feat\"] = self.acc_features\n",
    "                self.imu_channel_keys[\"rot_feat\"] = self.rot_features\n",
    "            else:\n",
    "                if self.config.get(\"old_imu_feat\", True):\n",
    "                    self.imu_channel_keys[\"other\"].extend(self.old_imu_features)\n",
    "                else:\n",
    "                    self.imu_channel_keys[\"other\"].extend(self.acc_features)\n",
    "                    self.imu_channel_keys[\"other\"].extend(self.rot_features)\n",
    "        if self.config.get(\"add_imu_er_feats\", False): \n",
    "            self.imu_cols.extend(self.er_fearues)\n",
    "            if self.config.get(\"split_imu_feat\", False):\n",
    "                self.imu_channel_keys[\"er1_feat\"] = self.er1_fearues\n",
    "                self.imu_channel_keys[\"er2_feat\"] = self.er2_fearues\n",
    "            else:\n",
    "                self.imu_channel_keys[\"other\"].extend(self.er1_fearues)\n",
    "                self.imu_channel_keys[\"other\"].extend(self.er2_fearues)\n",
    "        self.flip_imu_cols = [f\"{col}_flip\" for col in self.imu_cols]\n",
    "        self.imu_channel_keys = {k: sorted(v) for k, v in self.imu_channel_keys.items()}\n",
    "        self.thm_cols = [c for c in self.raw_columns if c.startswith('thm_')]\n",
    "        self.thm_channel_keys = {k: [f\"thm_{k}\"] for k in range(1, 6)}\n",
    "        self.feature_cols = self.imu_cols + self.thm_cols + self.tof_cols\n",
    "        self.imu_dim = len(self.imu_cols)\n",
    "        self.thm_dim = len(self.thm_cols)\n",
    "        self.tof_dim = len(self.tof_cols)\n",
    "        self.base_cols = ['acc_x', 'acc_y', 'acc_z',\n",
    "                          'rot_x', 'rot_y', 'rot_z', 'rot_w',\n",
    "                          'sequence_id', 'subject', \n",
    "                          'sequence_type', 'gesture', 'orientation'] + [c for c in self.raw_columns if c.startswith('thm_')] + [f\"tof_{i}_v{p}\" for i in range(1, 6) for p in range(64)]\n",
    "        self.use_cols = self.base_cols + self.feature_cols\n",
    "        if self.config.get(\"return_flip_imu\", False):\n",
    "            self.use_cols.extend(self.flip_imu_cols)\n",
    "        self.fold_cols = ['subject', 'sequence_type', 'gesture', 'orientation', 'sequence_id']\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            self.dg_cols = ['adult_child', 'age', 'sex', 'handedness', 'shoulder_to_wrist_height', 'elbow_to_wrist_height']\n",
    "        self.global_imu_indices = {k: sorted([self.imu_cols.index(feat) for feat in feats]) for k, feats in self.imu_channel_keys.items()}\n",
    "        self.global_thm_indices = {k: sorted([self.thm_cols.index(key) for key in self.thm_channel_keys[k]]) for k in range(1, 6)}\n",
    "        self.global_tof_indices = {k: sorted([self.tof_cols.index(key) for key in self.tof_channel_keys[k]]) for k in range(1, 6)}\n",
    "            \n",
    "    def generate_tof_feature_names(self):\n",
    "        features = list()\n",
    "        self.tof_channel_keys = defaultdict(list)\n",
    "        if self.config.get(\"tof_raw\", False):\n",
    "            for i in range(1, 6):\n",
    "                features.extend([f\"tof_{i}_v{p}\" for p in range(64)])\n",
    "                self.tof_channel_keys[i].extend([f\"tof_{i}_v{p}\" for p in range(64)])\n",
    "        for i in range(1, 6):\n",
    "            if self.tof_mode != 0:\n",
    "                for stat in self.tof_region_stats:\n",
    "                    features.append(f'tof_{i}_{stat}')\n",
    "                    self.tof_channel_keys[i].append(f'tof_{i}_{stat}')\n",
    "                if self.tof_mode > 1:\n",
    "                    for r in range(self.tof_mode):\n",
    "                        for stat in self.tof_region_stats:\n",
    "                            features.append(f'tof{self.tof_mode}_{i}_region_{r}_{stat}')\n",
    "                            self.tof_channel_keys[i].append(f'tof{self.tof_mode}_{i}_region_{r}_{stat}')\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        for r in range(mode):\n",
    "                            for stat in self.tof_region_stats:\n",
    "                                features.append(f'tof{mode}_{i}_region_{r}_{stat}')\n",
    "                                self.tof_channel_keys[i].append(f'tof{mode}_{i}_region_{r}_{stat}')\n",
    "        return features\n",
    "\n",
    "    def compute_cross_axis_energy(self, df):\n",
    "        axes=['x', 'y', 'z']\n",
    "        features = {}\n",
    "        for axis in axes:\n",
    "            fft_result = fft(df[f'acc_{axis}'].values)\n",
    "            energy = np.sum(np.abs(fft_result)**2)\n",
    "            features[f\"er_{axis}\"] = energy\n",
    "        for i, axis1 in enumerate(axes):\n",
    "            for axis2 in axes[i+1:]:\n",
    "                features[f'er_r_{axis1}{axis2}'] = features[f'er_{axis1}'] / (features[f'er_{axis2}'] + 1e-6)\n",
    "        for i, axis1 in enumerate(axes):\n",
    "            for axis2 in axes[i+1:]:\n",
    "                features[f'er_c_{axis1}{axis2}'] = np.corrcoef(np.abs(fft(df[f'acc_{axis1}'].values)), np.abs(fft(df[f'acc_{axis2}'].values)))[0, 1]\n",
    "        return {k: v for k, v in features.items() if k in self.er_fearues}\n",
    "\n",
    "    def compute_imu_features(self, df):\n",
    "        if self.config.get(\"rot_fillna\", False):\n",
    "            df['rot_w'] = df['rot_w'].fillna(1)\n",
    "            df[['rot_x', 'rot_y', 'rot_z']] = df[['rot_x', 'rot_y', 'rot_z']].fillna(0)\n",
    "        df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n",
    "        df['rot_angle'] = 2 * np.arccos(df['rot_w'].clip(-1, 1))\n",
    "        df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n",
    "        df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n",
    "            \n",
    "        linear_accel_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            acc_data_group = group[['acc_x', 'acc_y', 'acc_z']]\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            linear_accel_group = remove_gravity_from_acc(acc_data_group, rot_data_group)\n",
    "            linear_accel_list.append(pd.DataFrame(linear_accel_group, columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index))\n",
    "        df_linear_accel = pd.concat(linear_accel_list)\n",
    "        df = pd.concat([df, df_linear_accel], axis=1)\n",
    "        df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n",
    "        df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n",
    "    \n",
    "        angular_vel_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            angular_vel_group = calculate_angular_velocity_from_quat(rot_data_group)\n",
    "            angular_vel_list.append(pd.DataFrame(angular_vel_group, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index))\n",
    "        df_angular_vel = pd.concat(angular_vel_list)\n",
    "        df = pd.concat([df, df_angular_vel], axis=1)\n",
    "    \n",
    "        angular_distance_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            angular_dist_group = calculate_angular_distance(rot_data_group)\n",
    "            angular_distance_list.append(pd.DataFrame(angular_dist_group, columns=['angular_distance'], index=group.index))\n",
    "        df_angular_distance = pd.concat(angular_distance_list)\n",
    "        df = pd.concat([df, df_angular_distance], axis=1)\n",
    "        return df\n",
    "\n",
    "    def compute_flip_features(self, df):\n",
    "        flip_df = df[['sequence_id', 'acc_x', 'acc_y', 'acc_z', 'rot_x', 'rot_y', 'rot_z', 'rot_w']].copy()\n",
    "        flip_df[['acc_x', 'acc_y', 'rot_x', 'rot_y']] *= -1\n",
    "        flip_df = self.compute_imu_features(flip_df)\n",
    "        for col in flip_df.columns:\n",
    "            if col != 'sequence_id':\n",
    "                df[f\"{col}_flip\"] = flip_df[col]\n",
    "        return df\n",
    "\n",
    "    def compute_features(self, df):\n",
    "        df = self.compute_imu_features(df)\n",
    "        if self.tof_mode != 0:\n",
    "            new_columns = {}\n",
    "            for i in range(1, 6):\n",
    "                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "                tof_data = df[pixel_cols].replace(-1, np.nan)\n",
    "                new_columns.update({\n",
    "                    f'tof_{i}_mean': tof_data.mean(axis=1),\n",
    "                    f'tof_{i}_std': tof_data.std(axis=1),\n",
    "                    f'tof_{i}_min': tof_data.min(axis=1),\n",
    "                    f'tof_{i}_max': tof_data.max(axis=1)\n",
    "                })\n",
    "                if self.tof_mode > 1:\n",
    "                    region_size = 64 // self.tof_mode\n",
    "                    for r in range(self.tof_mode):\n",
    "                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                        new_columns.update({\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                        })\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        region_size = 64 // mode\n",
    "                        for r in range(mode):\n",
    "                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                            new_columns.update({\n",
    "                                f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                            })\n",
    "            df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
    "            \n",
    "        def _calc_features(group):\n",
    "            return pd.DataFrame(self.compute_cross_axis_energy(group), index=[group.index[0]])\n",
    "        features_df = df.groupby('sequence_id', group_keys=False).apply(_calc_features)\n",
    "        df = df.join(features_df, how='left')\n",
    "        df[features_df.columns] = df.groupby('sequence_id')[features_df.columns].ffill()\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def generate_features(self, df):\n",
    "        self.le = LabelEncoder()\n",
    "        if self.config.get(\"one_neg\", False):\n",
    "            neg_other = \"Write name on leg\"\n",
    "            df['gesture'] = df['gesture'].apply(lambda x: x if x in self.target_gestures else neg_other)\n",
    "        df['gesture_int'] = self.le.fit_transform(df['gesture'])\n",
    "        self.class_num = len(self.le.classes_)\n",
    "        self.target_ints = np.array([self.le.classes_.tolist().index(name) for name in self.target_gestures])\n",
    "        self.non_target_ints = np.array([self.le.classes_.tolist().index(name) for name in self.non_target_gestures])\n",
    "        \n",
    "        if all(c in df.columns for c in self.feature_cols):\n",
    "            print(\"Features have precomputed, skip compute.\")\n",
    "        else:\n",
    "            print(\"Features not precomputed, do compute.\")\n",
    "            df = self.compute_features(df)\n",
    "\n",
    "        if self.config.get(\"return_flip_imu\", False):\n",
    "            if all(c in df.columns for c in self.flip_imu_cols):\n",
    "                print(\"Flip have precomputed, skip compute.\")\n",
    "            else:\n",
    "                print(\"Flip not precomputed, do compute.\")\n",
    "                df = self.compute_flip_features(df)\n",
    "\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            dg_df = pd.read_csv(self.config[\"dg_path\"])\n",
    "            df = pd.merge(df, dg_df, how='left', on='subject')\n",
    "            df['age'] /= 100\n",
    "            df['shoulder_to_wrist_height'] = df['shoulder_to_wrist_cm'] / df['height_cm']\n",
    "            df['elbow_to_wrist_height'] = df['elbow_to_wrist_cm'] / df['height_cm']\n",
    "        \n",
    "        if self.config.get(\"save_precompute\", False):\n",
    "            df.to_csv(self.config.get(\"save_filename\", \"train.csv\"))\n",
    "        return df\n",
    "\n",
    "    def scale(self, data_unscaled):\n",
    "        scaler_function = self.config.get(\"scaler_function\", StandardScaler())\n",
    "        scaler = scaler_function.fit(np.concatenate(data_unscaled, axis=0))\n",
    "        return [scaler.transform(x) for x in data_unscaled], scaler\n",
    "\n",
    "    def pad(self, data_scaled, cols):\n",
    "        pad_data = np.zeros((len(data_scaled), self.pad_len, len(cols)), dtype='float32')\n",
    "        for i, seq in enumerate(data_scaled):\n",
    "            seq_len = min(len(seq), self.pad_len)\n",
    "            pad_data[i, :seq_len] = seq[:seq_len]\n",
    "        return pad_data\n",
    "\n",
    "    def get_nan_value(self, data, ratio):\n",
    "        max_value = data.max().max()\n",
    "        nan_value = -max_value * ratio\n",
    "        print(f\"Max: {max_value}, set nan to {nan_value}\")\n",
    "        return nan_value\n",
    "\n",
    "    def generate_dataset(self, df):\n",
    "        seq_gp = df.groupby('sequence_id') \n",
    "        imu_unscaled, thm_unscaled, tof_unscaled = list(), list(), list()\n",
    "        if self.config.get(\"return_flip_imu\", False): flip_imu_unscaled = list()\n",
    "        classes, lens = list(), list()\n",
    "        self.imu_nan_value = self.get_nan_value(df[self.imu_cols], self.config[\"nan_ratio\"][\"imu\"])\n",
    "        self.thm_nan_value = self.get_nan_value(df[self.thm_cols], self.config[\"nan_ratio\"][\"thm\"])\n",
    "        self.tof_nan_value = self.get_nan_value(df[self.tof_cols], self.config[\"nan_ratio\"][\"tof\"])\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            self.dg = list()\n",
    "\n",
    "        self.fold_feats = defaultdict(list)\n",
    "        for seq_id, seq_df in seq_gp:\n",
    "            imu_data = seq_df[self.imu_cols]\n",
    "            if self.config[\"fbfill\"][\"imu\"]:\n",
    "                imu_data = imu_data.ffill().bfill()\n",
    "            imu_unscaled.append(imu_data.fillna(self.imu_nan_value).values.astype('float32'))\n",
    "\n",
    "            if self.config.get(\"return_flip_imu\", False):\n",
    "                flip_imu_data = seq_df[self.flip_imu_cols]\n",
    "                if self.config[\"fbfill\"][\"imu\"]:\n",
    "                    flip_imu_data = flip_imu_data.ffill().bfill()\n",
    "                flip_imu_unscaled.append(flip_imu_data.fillna(self.imu_nan_value).values.astype('float32'))\n",
    "\n",
    "            thm_data = seq_df[self.thm_cols]\n",
    "            if self.config[\"fbfill\"][\"thm\"]:\n",
    "                thm_data = thm_data.ffill().bfill()\n",
    "            thm_unscaled.append(thm_data.fillna(self.thm_nan_value).values.astype('float32'))\n",
    "\n",
    "            tof_data = seq_df[self.tof_cols]\n",
    "            if self.config[\"fbfill\"][\"tof\"]:\n",
    "                tof_data = tof_data.ffill().bfill()\n",
    "            tof_unscaled.append(tof_data.fillna(self.tof_nan_value).values.astype('float32'))\n",
    "            \n",
    "            classes.append(seq_df['gesture_int'].iloc[0])\n",
    "            lens.append(len(imu_data))\n",
    "\n",
    "            for col in self.fold_cols:\n",
    "                self.fold_feats[col].append(seq_df[col].iloc[0])\n",
    "\n",
    "            if self.config.get(\"use_dg\", False):\n",
    "                self.dg.append(seq_df[self.dg_cols].iloc[0].values.astype('float32'))\n",
    "            \n",
    "        self.dataset_indices = classes\n",
    "        self.pad_len = int(np.percentile(lens, self.config.get(\"percent\", 95)))\n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            x_unscaled = [np.concatenate([imu, thm, tof], axis=1) for imu, thm, tof in zip(imu_unscaled, thm_unscaled, tof_unscaled)]\n",
    "            x_scaled, self.x_scaler = self.scale(x_unscaled)\n",
    "            x = self.pad(x_scaled, self.imu_cols+self.thm_cols+self.tof_cols)\n",
    "            self.imu = x[..., :self.imu_dim]\n",
    "            self.thm = x[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "            self.tof = x[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "\n",
    "            if self.config.get(\"return_flip_imu\", False):\n",
    "                flip_x_unscaled = [np.concatenate([flip_imu, thm, tof], axis=1) for flip_imu, thm, tof in zip(flip_imu_unscaled, thm_unscaled, tof_unscaled)]\n",
    "                flip_x_scaled = [self.x_scaler.transform(x) for x in flip_x_unscaled]\n",
    "                flip_x = self.pad(flip_x_scaled, self.imu_cols+self.thm_cols+self.tof_cols)\n",
    "                self.flip_imu = flip_x[..., :self.imu_dim]\n",
    "        else:\n",
    "            imu_scaled, self.imu_scaler = self.scale(imu_unscaled)\n",
    "            thm_scaled, self.thm_scaler = self.scale(thm_unscaled)\n",
    "            tof_scaled, self.tof_scaler = self.scale(tof_unscaled)\n",
    "            self.imu = self.pad(imu_scaled, self.imu_cols)\n",
    "            self.thm = self.pad(thm_scaled, self.thm_cols)\n",
    "            self.tof = self.pad(tof_scaled, self.tof_cols)\n",
    "\n",
    "            if self.config.get(\"return_flip_imu\", False):\n",
    "                flip_imu_scaled = [self.imu_scaler.transform(x) for x in flip_imu_unscaled]\n",
    "                self.flip_imu = self.pad(flip_imu_scaled, self.imu_cols)\n",
    "        self.precompute_scaled_nan_values()\n",
    "        self.class_ = F.one_hot(torch.from_numpy(np.array(classes)).long(), num_classes=len(self.le.classes_)).float().numpy()\n",
    "        self.binary_class_ = np.isin(np.array(classes), self.target_ints).astype(np.float32)\n",
    "        self.class_weight = torch.FloatTensor(compute_class_weight('balanced', classes=np.arange(len(self.le.classes_)), y=classes))\n",
    "\n",
    "    def precompute_scaled_nan_values(self):\n",
    "        dummy_df = pd.DataFrame(\n",
    "            np.array([[self.imu_nan_value]*len(self.imu_cols) + \n",
    "                     [self.thm_nan_value]*len(self.thm_cols) +\n",
    "                     [self.tof_nan_value]*len(self.tof_cols)]),\n",
    "            columns=self.imu_cols + self.thm_cols + self.tof_cols\n",
    "        )\n",
    "        \n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            scaled = self.x_scaler.transform(dummy_df)\n",
    "            self.imu_scaled_nan = scaled[0, :self.imu_dim].mean()\n",
    "            self.thm_scaled_nan = scaled[0, self.imu_dim:self.imu_dim+self.thm_dim].mean()\n",
    "            self.tof_scaled_nan = scaled[0, self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim].mean()\n",
    "        else:\n",
    "            self.imu_scaled_nan = self.imu_scaler.transform(dummy_df[self.imu_cols])[0].mean()\n",
    "            self.thm_scaled_nan = self.thm_scaler.transform(dummy_df[self.thm_cols])[0].mean()\n",
    "            self.tof_scaled_nan = self.tof_scaler.transform(dummy_df[self.tof_cols])[0].mean()\n",
    "\n",
    "    def get_scaled_nan_tensors(self, imu, thm, tof):\n",
    "        return torch.full(imu.shape, self.imu_scaled_nan, device=imu.device), \\\n",
    "            torch.full(thm.shape, self.thm_scaled_nan, device=thm.device), \\\n",
    "            torch.full(tof.shape, self.tof_scaled_nan, device=tof.device)\n",
    "\n",
    "    def inference_process(self, sequence, demographics=None, reverse=False):\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            assert demographics is not None, \"Demographics needed\"\n",
    "            df_dg = demographics.to_pandas().copy()\n",
    "            df_dg['age'] /= 100\n",
    "            df_dg['shoulder_to_wrist_height'] = df_dg['shoulder_to_wrist_cm'] / df_dg['height_cm']\n",
    "            df_dg['elbow_to_wrist_height'] = df_dg['elbow_to_wrist_cm'] / df_dg['height_cm']\n",
    "        df_seq = sequence.to_pandas().copy()\n",
    "        if reverse:\n",
    "            df_seq[['acc_x', 'acc_y', 'rot_x', 'rot_y']] *= -1\n",
    "        if self.config.get(\"rot_fillna\", False):\n",
    "            df_seq['rot_w'] = df_seq['rot_w'].fillna(1)\n",
    "            df_seq[['rot_x', 'rot_y', 'rot_z']] = df_seq[['rot_x', 'rot_y', 'rot_z']].fillna(0)\n",
    "        if not all(c in df_seq.columns for c in self.imu_features):\n",
    "            df_seq['acc_mag'] = np.sqrt(df_seq['acc_x']**2 + df_seq['acc_y']**2 + df_seq['acc_z']**2)\n",
    "            df_seq['rot_angle'] = 2 * np.arccos(df_seq['rot_w'].clip(-1, 1))\n",
    "            df_seq['acc_mag_jerk'] = df_seq['acc_mag'].diff().fillna(0)\n",
    "            df_seq['rot_angle_vel'] = df_seq['rot_angle'].diff().fillna(0)\n",
    "            if all(col in df_seq.columns for col in ['acc_x', 'acc_y', 'acc_z', 'rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                linear_accel = remove_gravity_from_acc(\n",
    "                    df_seq[['acc_x', 'acc_y', 'acc_z']], \n",
    "                    df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "                )\n",
    "                df_seq[['linear_acc_x', 'linear_acc_y', 'linear_acc_z']] = linear_accel\n",
    "            else:\n",
    "                df_seq['linear_acc_x'] = df_seq.get('acc_x', 0)\n",
    "                df_seq['linear_acc_y'] = df_seq.get('acc_y', 0)\n",
    "                df_seq['linear_acc_z'] = df_seq.get('acc_z', 0)\n",
    "            df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)\n",
    "            df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n",
    "            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                angular_vel = calculate_angular_velocity_from_quat(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
    "                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = angular_vel\n",
    "            else:\n",
    "                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = 0\n",
    "            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                df_seq['angular_distance'] = calculate_angular_distance(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
    "            else:\n",
    "                df_seq['angular_distance'] = 0\n",
    "\n",
    "        if self.tof_mode != 0:\n",
    "            new_columns = {} \n",
    "            for i in range(1, 6):\n",
    "                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "                tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "                new_columns.update({\n",
    "                    f'tof_{i}_mean': tof_data.mean(axis=1),\n",
    "                    f'tof_{i}_std': tof_data.std(axis=1),\n",
    "                    f'tof_{i}_min': tof_data.min(axis=1),\n",
    "                    f'tof_{i}_max': tof_data.max(axis=1)\n",
    "                })\n",
    "                if self.tof_mode > 1:\n",
    "                    region_size = 64 // self.tof_mode\n",
    "                    for r in range(self.tof_mode):\n",
    "                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                        new_columns.update({\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                        })\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        region_size = 64 // mode\n",
    "                        for r in range(mode):\n",
    "                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                            new_columns.update({\n",
    "                                f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                            })\n",
    "            df_seq = pd.concat([df_seq, pd.DataFrame(new_columns)], axis=1)\n",
    "        \n",
    "        imu_unscaled = df_seq[self.imu_cols]\n",
    "        if self.config[\"fbfill\"][\"imu\"]:\n",
    "            imu_unscaled = imu_unscaled.ffill().bfill()\n",
    "        imu_unscaled = imu_unscaled.fillna(self.imu_nan_value).values.astype('float32')\n",
    "\n",
    "        thm_unscaled = df_seq[self.thm_cols]\n",
    "        if self.config[\"fbfill\"][\"thm\"]:\n",
    "            thm_unscaled = thm_unscaled.ffill().bfill()\n",
    "        thm_unscaled = thm_unscaled.fillna(self.thm_nan_value).values.astype('float32')\n",
    "\n",
    "        tof_unscaled = df_seq[self.tof_cols]\n",
    "        if self.config[\"fbfill\"][\"tof\"]:\n",
    "            tof_unscaled = tof_unscaled.ffill().bfill()\n",
    "        tof_unscaled = tof_unscaled.fillna(self.tof_nan_value).values.astype('float32')\n",
    "        \n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            x_unscaled = np.concatenate([imu_unscaled, thm_unscaled, tof_unscaled], axis=1)\n",
    "            x_scaled = self.x_scaler.transform(x_unscaled)\n",
    "            imu_scaled = x_scaled[..., :self.imu_dim]\n",
    "            thm_scaled = x_scaled[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "            tof_scaled = x_scaled[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "        else:\n",
    "            imu_scaled = self.imu_scaler.transform(imu_unscaled)\n",
    "            thm_scaled = self.thm_scaler.transform(thm_unscaled)\n",
    "            tof_scaled = self.tof_scaler.transform(tof_unscaled)\n",
    "\n",
    "        combined = np.concatenate([imu_scaled, thm_scaled, tof_scaled], axis=1)\n",
    "        padded = np.zeros((self.pad_len, combined.shape[1]), dtype='float32')\n",
    "        seq_len = min(combined.shape[0], self.pad_len)\n",
    "        padded[:seq_len] = combined[:seq_len]\n",
    "        imu = padded[..., :self.imu_dim]\n",
    "        thm = padded[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "        tof = padded[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "\n",
    "        ret = [torch.from_numpy(imu).float().unsqueeze(0), torch.from_numpy(thm).float().unsqueeze(0), torch.from_numpy(tof).float().unsqueeze(0)]\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            dg = df_dg[self.dg_cols].values.astype('float32')\n",
    "            ret.append(torch.from_numpy(dg).float())\n",
    "        return ret\n",
    "\n",
    "    def split5(self, imu, thm, tof):\n",
    "        imus = [imu[:, :, self.global_imu_indices[k]] for k in self.global_imu_indices]\n",
    "        thms = [thm[:, :, self.global_thm_indices[k]] for k in range(1, 6)]\n",
    "        tofs = [tof[:, :, self.global_tof_indices[k]] for k in range(1, 6)]\n",
    "        return imus, thms, tofs\n",
    "\n",
    "    def slide(self, imu, thm, tof, ratio=1.0):\n",
    "        def slide_tensor(tensor, nan_value, ratio):\n",
    "            b, l, d = tensor.shape\n",
    "            length = int(l * ratio)\n",
    "            if length > l:\n",
    "                pad = torch.full((b, length-l, d), nan_value, device=tensor.device)\n",
    "                tensor = torch.cat([tensor, pad], dim=1)\n",
    "            elif length < l:\n",
    "                tensor = tensor[:, :length, :] \n",
    "            return tensor\n",
    "        return slide_tensor(imu, self.imu_scaled_nan, ratio), slide_tensor(thm, self.thm_scaled_nan, ratio), slide_tensor(tof, self.tof_scaled_nan, ratio)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ret = [self.imu[idx], self.thm[idx], self.tof[idx], self.class_[idx], self.binary_class_[idx]]\n",
    "        if self.config.get(\"return_extra\", False):\n",
    "            fold_feat_info = [self.fold_feats[col][idx] for col in self.fold_cols]\n",
    "            ret.append((idx, fold_feat_info))\n",
    "        if self.config.get(\"use_dg\", False):\n",
    "            ret.append(self.dg[idx])\n",
    "        if self.config.get(\"return_flip_imu\", False):\n",
    "            ret.append(self.flip_imu[idx])\n",
    "        return ret\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4781306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T17:04:19.110108Z",
     "iopub.status.busy": "2025-09-02T17:04:19.109870Z",
     "iopub.status.idle": "2025-09-02T17:05:51.570733Z",
     "shell.execute_reply": "2025-09-02T17:05:51.569935Z"
    },
    "papermill": {
     "duration": 92.481082,
     "end_time": "2025-09-02T17:05:51.582113",
     "exception": false,
     "start_time": "2025-09-02T17:04:19.101031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m     valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m valid_loader\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m dataset4 = \u001b[43minit_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36minit_dataset\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minit_dataset\u001b[39m():\n\u001b[32m      2\u001b[39m     dataset_config = {\n\u001b[32m      3\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpercent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m99\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mscaler_config\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mStandardScaler\u001b[49m(),\n\u001b[32m      5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnan_ratio\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m      6\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mimu\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m      7\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mthm\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m      8\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtof\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m      9\u001b[39m         },\n\u001b[32m     10\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfbfill\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     11\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mimu\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     12\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mthm\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     13\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtof\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     14\u001b[39m         },\n\u001b[32m     15\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mone_scale\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     16\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtof_raw\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     17\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtof_mode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m16\u001b[39m,\n\u001b[32m     18\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msave_precompute\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     19\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfold_y\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mgesture\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     20\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfold_groups\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msubject\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m     }\n\u001b[32m     23\u001b[39m     dataset = CMIFoldDataset4(universe_csv_path, dataset_config, full_dataset_function=CMIFeDataset4, n_folds=n_folds, random_seed=seed)\n\u001b[32m     24\u001b[39m     dataset.print_fold_stats()\n",
      "\u001b[31mNameError\u001b[39m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "def init_dataset():\n",
    "    dataset_config = {\n",
    "        \"percent\": 99,\n",
    "        \"scaler_config\": StandardScaler(),\n",
    "        \"nan_ratio\": {\n",
    "            \"imu\": 0,\n",
    "            \"thm\": 0,\n",
    "            \"tof\": 0,\n",
    "        },\n",
    "        \"fbfill\": {\n",
    "            \"imu\": True,\n",
    "            \"thm\": True,\n",
    "            \"tof\": True,\n",
    "        },\n",
    "        \"one_scale\": False,\n",
    "        \"tof_raw\": True,\n",
    "        \"tof_mode\": 16,\n",
    "        \"save_precompute\": False,\n",
    "        \"fold_y\": \"gesture\",\n",
    "        \"fold_groups\": \"subject\",\n",
    "    }\n",
    "\n",
    "    dataset = CMIFoldDataset4(universe_csv_path, dataset_config, full_dataset_function=CMIFeDataset4, n_folds=n_folds, random_seed=seed)\n",
    "    dataset.print_fold_stats()\n",
    "    return dataset\n",
    "\n",
    "def get_fold_dataset(dataset, fold):\n",
    "    _, valid_dataset = dataset.get_fold_datasets(fold)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    return valid_loader\n",
    "\n",
    "dataset4 = init_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1bdbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T17:05:51.602150Z",
     "iopub.status.busy": "2025-09-02T17:05:51.601917Z",
     "iopub.status.idle": "2025-09-02T17:05:51.608399Z",
     "shell.execute_reply": "2025-09-02T17:05:51.607690Z"
    },
    "papermill": {
     "duration": 0.016944,
     "end_time": "2025-09-02T17:05:51.609577",
     "exception": false,
     "start_time": "2025-09-02T17:05:51.592633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model 4 Dataset Processing and Predict Function\n",
    "class Model4Dataset:\n",
    "    def __init__(self):\n",
    "        # IMU channel mappings for split-sensor architecture\n",
    "        self.imu_channel_keys = {\n",
    "            \"acc\": [0, 1, 2],  # linear_acc_x, linear_acc_y, linear_acc_z\n",
    "            \"rot\": [3, 4, 5, 6],  # rot_w, rot_x, rot_y, rot_z\n",
    "            \"other\": list(range(7, 17))  # other IMU features (10 channels total)\n",
    "        }\n",
    "        \n",
    "        # Initialize split indices for sensors\n",
    "        self.global_imu_indices = {\n",
    "            \"acc\": [0, 1, 2],\n",
    "            \"rot\": [3, 4, 5, 6], \n",
    "            \"other\": list(range(7, 17))  # Adjusted to match 10 channels\n",
    "        }\n",
    "        self.global_thm_indices = {i: [i-1] for i in range(1, 6)}  # 0-4 indices for thm1-thm5\n",
    "        self.global_tof_indices = {i: list(range((i-1)*132, i*132)) for i in range(1, 6)}  # TOF regions (132 per sensor)\n",
    "        \n",
    "    def split5(self, imu, thm, tof):\n",
    "        # Split IMU into acc, rot, other components\n",
    "        imus = [imu[:, :, self.global_imu_indices[k]] for k in [\"acc\", \"rot\", \"other\"]]\n",
    "        # Split THM and TOF into 5 sensors each\n",
    "        thms = [thm[:, :, self.global_thm_indices[k]] for k in range(1, 6)]\n",
    "        tofs = [tof[:, :, self.global_tof_indices[k]] for k in range(1, 6)]\n",
    "        return imus, thms, tofs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c241cee6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T17:05:51.627514Z",
     "iopub.status.busy": "2025-09-02T17:05:51.627251Z",
     "iopub.status.idle": "2025-09-02T17:05:51.633358Z",
     "shell.execute_reply": "2025-09-02T17:05:51.632871Z"
    },
    "papermill": {
     "duration": 0.016362,
     "end_time": "2025-09-02T17:05:51.634331",
     "exception": false,
     "start_time": "2025-09-02T17:05:51.617969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CMIModel4(nn.Module):\n",
    "    def __init__(self, target_classes_num, non_target_classes_num, **kwargs):\n",
    "        super().__init__()\n",
    "        self.backbone = CMIBackbone4(dataset.imu_dim, dataset.thm_dim, dataset.tof_dim, **kwargs)\n",
    "        self.target_classifier = nn.Sequential(\n",
    "            nn.Linear((kwargs['lstm_hidden_size']+kwargs['gru_hidden_size'])*2+kwargs['dense_channels'], kwargs[\"cls_channels1\"]),\n",
    "            nn.BatchNorm1d(kwargs[\"cls_channels1\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(kwargs[\"cls_dropout1\"]),\n",
    "            nn.Linear(kwargs[\"cls_channels1\"], kwargs[\"cls_channels2\"]),\n",
    "            nn.BatchNorm1d(kwargs[\"cls_channels2\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(kwargs[\"cls_dropout2\"]),\n",
    "            nn.Linear(kwargs[\"cls_channels2\"], target_classes_num)\n",
    "        )\n",
    "        self.non_target_classifier = nn.Sequential(\n",
    "            nn.Linear((kwargs['lstm_hidden_size']+kwargs['gru_hidden_size'])*2+kwargs['dense_channels'], kwargs[\"cls_channels1\"]),\n",
    "            nn.BatchNorm1d(kwargs[\"cls_channels1\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(kwargs[\"cls_dropout1\"]),\n",
    "            nn.Linear(kwargs[\"cls_channels1\"], kwargs[\"cls_channels2\"]),\n",
    "            nn.BatchNorm1d(kwargs[\"cls_channels2\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(kwargs[\"cls_dropout2\"]),\n",
    "            nn.Linear(kwargs[\"cls_channels2\"], non_target_classes_num)\n",
    "        )\n",
    "    \n",
    "    def forward(self, imu, thm, tof):\n",
    "        feat = self.backbone(imu, thm, tof)\n",
    "        targets_y = self.target_classifier(feat)\n",
    "        non_targets_y = self.non_target_classifier(feat)\n",
    "        return torch.cat([targets_y, non_targets_y], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15912c28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T17:05:51.651755Z",
     "iopub.status.busy": "2025-09-02T17:05:51.651542Z",
     "iopub.status.idle": "2025-09-02T17:05:54.357447Z",
     "shell.execute_reply": "2025-09-02T17:05:54.356873Z"
    },
    "papermill": {
     "duration": 2.716103,
     "end_time": "2025-09-02T17:05:54.358811",
     "exception": false,
     "start_time": "2025-09-02T17:05:51.642708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model4_function = CMIModel4\n",
    "model4_args = {\"imu1_channels\": 128, \"imu2_channels\": 256, \"imu1_dropout\": 0.3, \"imu2_dropout\": 0.25,\n",
    "              \"imu1_layers\": 0, \"imu2_layers\": 0, \n",
    "              \"thm1_channels\": 32, \"thm2_channels\": 64, \"thm1_dropout\": 0.25, \"thm2_dropout\": 0.2,\n",
    "              \"thm1_layers\": 0, \"thm2_layers\": 0, \n",
    "              \"tof1_channels\": 256, \"tof2_channels\": 512, \"tof1_dropout\": 0.4, \"tof2_dropout\": 0.3,\n",
    "              \"tof1_layers\": 0, \"tof2_layers\": 0, \n",
    "              \"lstm_hidden_size\": 128, \"gru_hidden_size\": 128, \"gaussian_noise_rate\": 0.1, \"dense_channels\": 32,\n",
    "              \"cls_channels1\": 256, \"cls_dropout1\": 0.2, \"cls_channels2\": 128, \"cls_dropout2\": 0.2,\n",
    "              \"target_classes_num\": 8, \"non_target_classes_num\": 10,}\n",
    "model4_dir = Path(\"/kaggle/input/cmi-models-public/pytorch/base04/1\")\n",
    "\n",
    "model4_dicts = [\n",
    "    {\n",
    "        \"model_function\": model4_function,\n",
    "        \"model_args\": model4_args,\n",
    "        \"model_path\": model4_dir / f\"fold{fold}/best_ema.pt\",\n",
    "    } for fold in range(n_folds)\n",
    "]\n",
    "\n",
    "def replace4(k):\n",
    "    k = k.replace(\"_orig_mod.\", \"\")\n",
    "    return k\n",
    "\n",
    "models4 = list()\n",
    "for model_dict in model4_dicts:\n",
    "    model_function = model_dict[\"model_function\"]\n",
    "    model_args = model_dict[\"model_args\"]\n",
    "    model_path = model_dict[\"model_path\"]\n",
    "    model = model_function(**model_args).to(CUDA0)\n",
    "    state_dict = {replace4(k): v for k,v in torch.load(model_path).items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.eval()\n",
    "    models4.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7da387dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T17:05:54.446080Z",
     "iopub.status.busy": "2025-09-02T17:05:54.445867Z",
     "iopub.status.idle": "2025-09-02T17:05:54.450578Z",
     "shell.execute_reply": "2025-09-02T17:05:54.450027Z"
    },
    "papermill": {
     "duration": 0.015047,
     "end_time": "2025-09-02T17:05:54.451537",
     "exception": false,
     "start_time": "2025-09-02T17:05:54.436490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict4(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    imu, thm, tof = dataset4.full_dataset.inference_process(sequence)\n",
    "    with torch.no_grad():\n",
    "        imu, thm, tof = to_cuda(imu, thm, tof)\n",
    "        \n",
    "        # Split sensors using Model 4's architecture\n",
    "        imus, thms, tofs = dataset4.full_dataset.split5(imu, thm, tof)\n",
    "        \n",
    "        # Use the correct models (models4)\n",
    "        outputs = []\n",
    "        for model in models4:  \n",
    "            logits = model(imus, thms, tofs)\n",
    "            outputs.append(logits)\n",
    "        \n",
    "        avg_logits = torch.mean(torch.stack(outputs), dim=0)\n",
    "        probabilities = F.softmax(avg_logits, dim=1).cpu().numpy()\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd47bcb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T17:05:54.898126Z",
     "iopub.status.busy": "2025-09-02T17:05:54.897875Z",
     "iopub.status.idle": "2025-09-02T17:05:54.926915Z",
     "shell.execute_reply": "2025-09-02T17:05:54.926375Z"
    },
    "papermill": {
     "duration": 0.03931,
     "end_time": "2025-09-02T17:05:54.927913",
     "exception": false,
     "start_time": "2025-09-02T17:05:54.888603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv\")\n",
    "test_demo = pd.read_csv(\"/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dca5e21d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T17:05:54.945595Z",
     "iopub.status.busy": "2025-09-02T17:05:54.945378Z",
     "iopub.status.idle": "2025-09-02T17:05:56.714016Z",
     "shell.execute_reply": "2025-09-02T17:05:56.713388Z"
    },
    "papermill": {
     "duration": 1.77886,
     "end_time": "2025-09-02T17:05:56.715299",
     "exception": false,
     "start_time": "2025-09-02T17:05:54.936439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "#test on model 4\n",
    "# If test_df is a pandas DataFrame, you should convert it to polars DataFrame before passing to predict4\n",
    "\n",
    "test_df_pl = pl.from_pandas(test_df)\n",
    "predictions = predict4(test_df_pl, test_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f18ebc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T17:05:56.735029Z",
     "iopub.status.busy": "2025-09-02T17:05:56.734635Z",
     "iopub.status.idle": "2025-09-02T17:05:57.576921Z",
     "shell.execute_reply": "2025-09-02T17:05:57.576054Z"
    },
    "papermill": {
     "duration": 0.853074,
     "end_time": "2025-09-02T17:05:57.578188",
     "exception": false,
     "start_time": "2025-09-02T17:05:56.725114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0557   0.03613  0.007126 0.1493   0.1306   0.03546  0.03925  0.007683\n",
      "  0.003838 0.2217   0.2045   0.00772  0.001618 0.0244   0.01519  0.03693\n",
      "  0.00493  0.0179  ]]\n"
     ]
    }
   ],
   "source": [
    "print(predict2(test_df_pl, test_demo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0c3e2c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T17:05:57.596144Z",
     "iopub.status.busy": "2025-09-02T17:05:57.595932Z",
     "iopub.status.idle": "2025-09-02T17:05:57.601870Z",
     "shell.execute_reply": "2025-09-02T17:05:57.601150Z"
    },
    "papermill": {
     "duration": 0.015898,
     "end_time": "2025-09-02T17:05:57.603004",
     "exception": false,
     "start_time": "2025-09-02T17:05:57.587106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03922182, 0.03626273, 0.02329881, 0.20999825, 0.24508175,\n",
       "        0.02132009, 0.03233574, 0.02228571, 0.02344224, 0.0843355 ,\n",
       "        0.10209542, 0.02040125, 0.02224878, 0.02208235, 0.01764466,\n",
       "        0.03170632, 0.02251116, 0.02372731]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d3d01",
   "metadata": {
    "papermill": {
     "duration": 0.008407,
     "end_time": "2025-09-02T17:05:57.619782",
     "exception": false,
     "start_time": "2025-09-02T17:05:57.611375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62ddf3",
   "metadata": {
    "_cell_guid": "0e851084-3ae1-40c2-9728-29167f12d4f8",
    "_uuid": "dd288b0e-f378-4cf6-a8e9-dea21c66a8ca",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-02T17:05:57.637807Z",
     "iopub.status.busy": "2025-09-02T17:05:57.637572Z",
     "iopub.status.idle": "2025-09-02T17:05:57.646157Z",
     "shell.execute_reply": "2025-09-02T17:05:57.645642Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.018763,
     "end_time": "2025-09-02T17:05:57.647184",
     "exception": false,
     "start_time": "2025-09-02T17:05:57.628421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(sequence, demographics):\n",
    "    import numpy as np\n",
    "\n",
    "    p0 = predict1(sequence, demographics)[0]\n",
    "    p1 = predict2(sequence, demographics)[0]\n",
    "    p2 = predict3(sequence, demographics)[0]\n",
    "    p3 = predict4(sequence, demographics)[0]  # Model 4 prediction\n",
    "\n",
    "    eps = 1e-12\n",
    "\n",
    "    def _jsd(a, b, eps=1e-12):\n",
    "        a = np.clip(a, eps, 1.0); b = np.clip(b, eps, 1.0)\n",
    "        m = 0.5 * (a + b)\n",
    "        kl = lambda x, y: float((x * np.log(x / y)).sum())\n",
    "        return 0.5 * kl(a, m) + 0.5 * kl(b, m)\n",
    "\n",
    "    js = (_jsd(p0, p1) + _jsd(p1, p2) + _jsd(p0, p2) + \n",
    "          _jsd(p0, p3) + _jsd(p1, p3) + _jsd(p2, p3)) / 6.0\n",
    "\n",
    "    w = np.array([0.25, 0.25, 0.25, 0.25], dtype=float)  # Adjusted weights for 4 models\n",
    "    log_geom = (\n",
    "        w[0] * np.log(np.clip(p0, eps, 1.0)) +\n",
    "        w[1] * np.log(np.clip(p1, eps, 1.0)) +\n",
    "        w[2] * np.log(np.clip(p2, eps, 1.0)) +\n",
    "        w[3] * np.log(np.clip(p3, eps, 1.0))\n",
    "    )\n",
    "\n",
    "    def _entropy(p):\n",
    "        q = np.clip(p, eps, 1.0)\n",
    "        return -float((q * np.log(q)).sum())\n",
    "\n",
    "    ent = [_entropy(p0), _entropy(p1), _entropy(p2), _entropy(p3)]\n",
    "    best_idx = int(np.argmin(ent))\n",
    "    log_best = np.log(np.clip([p0, p1, p2, p3][best_idx], eps, 1.0))\n",
    "\n",
    "    a, b = 0.10, 0.16                  \n",
    "    t = float(np.clip((js - a) / (b - a), 0.0, 1.0))\n",
    "\n",
    "  \n",
    "    tau = 1.0 - 0.02 * t\n",
    "    log_best_sharp = log_best / tau\n",
    "\n",
    "\n",
    "    log_mix = (1.0 - t) * log_geom + t * log_best_sharp\n",
    "\n",
    "    return dataset.le.classes_[int(np.argmax(log_mix))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c921a0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T17:05:57.710276Z",
     "iopub.status.busy": "2025-09-02T17:05:57.710086Z",
     "iopub.status.idle": "2025-09-02T17:05:57.712867Z",
     "shell.execute_reply": "2025-09-02T17:05:57.712350Z"
    },
    "papermill": {
     "duration": 0.012523,
     "end_time": "2025-09-02T17:05:57.713798",
     "exception": false,
     "start_time": "2025-09-02T17:05:57.701275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(predict(test_df_pl, test_demo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b86825e",
   "metadata": {
    "_cell_guid": "6699bce7-be44-4549-8297-a80b85dd45d0",
    "_uuid": "77e8ff1e-f127-4abc-b00a-6668c999066c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-02T17:05:57.732120Z",
     "iopub.status.busy": "2025-09-02T17:05:57.731666Z",
     "iopub.status.idle": "2025-09-02T17:06:34.650669Z",
     "shell.execute_reply": "2025-09-02T17:06:34.649955Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 36.929611,
     "end_time": "2025-09-02T17:06:34.651987",
     "exception": false,
     "start_time": "2025-09-02T17:05:57.722376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 17:05:58.928614: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "I0000 00:00:1756832760.048248      64 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-09-02 17:06:04.332939: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-09-02 17:06:09.818247: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-09-02 17:06:15.238475: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-09-02 17:06:20.522922: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-09-02 17:06:26.022293: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-09-02 17:06:31.600416: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    }
   ],
   "source": [
    "import kaggle_evaluation.cmi_inference_server\n",
    "inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        data_paths=(\n",
    "            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv',\n",
    "            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0278b56",
   "metadata": {
    "_cell_guid": "5058253c-4357-4421-8d59-f2168eeaf92c",
    "_uuid": "cedb60f1-f94f-4f09-bf11-99ee58e31b33",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-02T17:06:34.671678Z",
     "iopub.status.busy": "2025-09-02T17:06:34.671415Z",
     "iopub.status.idle": "2025-09-02T17:06:34.822978Z",
     "shell.execute_reply": "2025-09-02T17:06:34.822214Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.162301,
     "end_time": "2025-09-02T17:06:34.824223",
     "exception": false,
     "start_time": "2025-09-02T17:06:34.661922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sequence_id              gesture\n",
      "0  SEQ_000011  Eyelash - pull hair\n",
      "1  SEQ_000001  Eyebrow - pull hair\n"
     ]
    }
   ],
   "source": [
    "if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    print(pd.read_parquet(\"submission.parquet\"))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "sourceId": 102335,
     "sourceType": "competition"
    },
    {
     "datasetId": 7645099,
     "sourceId": 12139340,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7748073,
     "sourceId": 12293285,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7771623,
     "sourceId": 12328761,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7827797,
     "sourceId": 12411879,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7932089,
     "sourceId": 12573306,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8182675,
     "sourceId": 12931035,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7862221,
     "sourceId": 12463184,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 240649816,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 246893721,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 251413288,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 398856,
     "modelInstanceId": 379625,
     "sourceId": 470587,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 400086,
     "modelInstanceId": 380358,
     "sourceId": 471764,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 400086,
     "modelInstanceId": 407853,
     "sourceId": 517084,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 349.039849,
   "end_time": "2025-09-02T17:06:38.207490",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-02T17:00:49.167641",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
