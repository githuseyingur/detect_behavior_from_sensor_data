{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-04T13:28:33.153898Z",
     "iopub.status.busy": "2025-07-04T13:28:33.153658Z",
     "iopub.status.idle": "2025-07-04T13:28:51.892980Z",
     "shell.execute_reply": "2025-07-04T13:28:51.892035Z",
     "shell.execute_reply.started": "2025-07-04T13:28:33.153878Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 09:02:28.305120: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-09 09:02:28.700433: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754719348.832930    1479 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754719348.874463    1479 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-09 09:02:29.242297: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os, json, joblib, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tensorflow.keras.utils import Sequence, to_categorical, pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, LayerNormalization, Activation, add, MaxPooling1D, Dropout,\n",
    "    Bidirectional, LSTM, GlobalAveragePooling1D, Dense, Multiply, Reshape,\n",
    "    Lambda, Concatenate, GRU, GaussianNoise\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n",
      "3.8.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('cpu_compiler', '/usr/lib/llvm-18/bin/clang'), ('cuda_compute_capabilities', ['sm_60', 'sm_70', 'sm_80', 'sm_89', 'compute_90']), ('cuda_version', '12.5.1'), ('cudnn_version', '9'), ('is_cuda_build', True), ('is_rocm_build', False), ('is_tensorrt_build', False)])\n"
     ]
    }
   ],
   "source": [
    "print(tf.sysconfig.get_build_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU sayısı: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU sayısı:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T13:28:51.895049Z",
     "iopub.status.busy": "2025-07-04T13:28:51.894464Z",
     "iopub.status.idle": "2025-07-04T13:28:51.900335Z",
     "shell.execute_reply": "2025-07-04T13:28:51.899463Z",
     "shell.execute_reply.started": "2025-07-04T13:28:51.895023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "state_num = 1\n",
    "import random\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "seed_everything(seed=state_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T13:28:51.901740Z",
     "iopub.status.busy": "2025-07-04T13:28:51.901434Z",
     "iopub.status.idle": "2025-07-04T13:28:51.938388Z",
     "shell.execute_reply": "2025-07-04T13:28:51.937661Z",
     "shell.execute_reply.started": "2025-07-04T13:28:51.901713Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ imports ready · tensorflow 2.18.0\n"
     ]
    }
   ],
   "source": [
    "# (Competition metric will only be imported when TRAINing)\n",
    "TRAIN = True                \n",
    "RAW_DIR = Path(\"\")\n",
    "PRETRAINED_DIR = Path(\"new_model_10_fold\")\n",
    "EXPORT_DIR = Path(\"imu_only_gru_5folds\")\n",
    "BATCH_SIZE = 64\n",
    "PAD_PERCENTILE = 95 \n",
    "LR_INIT = 5e-4\n",
    "WD = 3e-3\n",
    "MIXUP_ALPHA = 0.4 \n",
    "EPOCHS = 160\n",
    "PATIENCE = 40\n",
    "N_SPLITS = 5\n",
    "MASKING_PROB = 0.25 \n",
    "GATE_LOSS_WEIGHT = 0.20 # 0.20 \n",
    "\n",
    "print(\"▶ imports ready · tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T13:28:51.939603Z",
     "iopub.status.busy": "2025-07-04T13:28:51.939338Z",
     "iopub.status.idle": "2025-07-04T13:28:51.959963Z",
     "shell.execute_reply": "2025-07-04T13:28:51.959030Z",
     "shell.execute_reply.started": "2025-07-04T13:28:51.939575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "    acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "    for i in range(len(acc_values)):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :]\n",
    "            continue\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200):\n",
    "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    angular_vel = np.zeros((len(quat_values), 3))\n",
    "    for i in range(len(quat_values) - 1):\n",
    "        q_t, q_t_plus_dt = quat_values[i], quat_values[i+1]\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isnan(q_t_plus_dt)): continue\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError: pass\n",
    "    return angular_vel\n",
    "\n",
    "def calculate_angular_distance(rot_data):\n",
    "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    angular_dist = np.zeros(len(quat_values))\n",
    "    for i in range(len(quat_values) - 1):\n",
    "        q1, q2 = quat_values[i], quat_values[i+1]\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isnan(q2)): continue\n",
    "        try:\n",
    "            r1, r2 = R.from_quat(q1), R.from_quat(q2)\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            angular_dist[i] = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "        except ValueError: pass\n",
    "    return angular_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T07:56:04.785466Z",
     "iopub.status.busy": "2025-07-04T07:56:04.784809Z",
     "iopub.status.idle": "2025-07-04T07:56:04.803532Z",
     "shell.execute_reply": "2025-07-04T07:56:04.802817Z",
     "shell.execute_reply.started": "2025-07-04T07:56:04.785437Z"
    }
   },
   "outputs": [],
   "source": [
    "class MixupGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size, class_weight=None, alpha=0.2):\n",
    "        self.X, self.y = X, y\n",
    "        self.batch = batch_size\n",
    "        self.class_weight = class_weight\n",
    "        self.alpha = alpha\n",
    "        self.indices = np.arange(len(X))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i*self.batch:(i+1)*self.batch]\n",
    "        Xb, yb = self.X[idx].copy(), self.y[idx].copy()\n",
    "        \n",
    "        sample_weights = np.ones(len(Xb), dtype='float32')\n",
    "        if self.class_weight:\n",
    "            y_integers = yb.argmax(axis=1)\n",
    "            sample_weights = np.array([self.class_weight[i] for i in y_integers])\n",
    "        \n",
    "        if self.alpha > 0:\n",
    "            lam = np.random.beta(self.alpha, self.alpha)\n",
    "            perm = np.random.permutation(len(Xb))\n",
    "            X_mix = lam * Xb + (1 - lam) * Xb[perm]\n",
    "            y_mix = lam * yb + (1 - lam) * yb[perm]\n",
    "            sample_weights_mix = lam * sample_weights + (1 - lam) * sample_weights[perm]\n",
    "            return X_mix, y_mix, sample_weights_mix\n",
    "\n",
    "        return Xb, yb, sample_weights\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "# --- Yardımcı Fonksiyonlar ---\n",
    "def time_sum(x):\n",
    "    return K.sum(x, axis=1)\n",
    "\n",
    "def squeeze_last_axis(x):\n",
    "    return tf.squeeze(x, axis=-1)\n",
    "\n",
    "def expand_last_axis(x):\n",
    "    return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "# --- SE Blok ---\n",
    "def se_block(input_tensor, ratio=8):\n",
    "    filters = input_tensor.shape[-1]\n",
    "    se = GlobalAveragePooling1D()(input_tensor)\n",
    "    se = Dense(filters // ratio, activation='relu')(se)\n",
    "    se = Dense(filters, activation='sigmoid')(se)\n",
    "    se = Reshape((1, filters))(se)\n",
    "    return Multiply()([input_tensor, se])\n",
    "\n",
    "# --- Residual + SE Blok ---\n",
    "def residual_se_block(x, filters, kernel_size, drop_rate=0.3, weight_decay=1e-4):\n",
    "    shortcut = x\n",
    "\n",
    "    x = Conv1D(filters, kernel_size, padding='same', use_bias=False, kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv1D(filters, kernel_size, padding='same', use_bias=False, kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = se_block(x)\n",
    "\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding='same', use_bias=False, kernel_regularizer=l2(weight_decay))(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(drop_rate)(x)\n",
    "    return x\n",
    "\n",
    "# --- Keras Uyumlu Lightweight Multi-Head Attention Katmanı ---\n",
    "class MultiHeadAttentionLayer(Layer):\n",
    "    def __init__(self, heads=4, proj_dim=64):\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        self.heads = heads\n",
    "        self.proj_dim = proj_dim\n",
    "        self.query_layers = [Dense(proj_dim) for _ in range(heads)]\n",
    "        self.key_layers = [Dense(proj_dim) for _ in range(heads)]\n",
    "        self.value_layers = [Dense(proj_dim) for _ in range(heads)]\n",
    "\n",
    "    def call(self, x):\n",
    "        heads_output = []\n",
    "        for i in range(self.heads):\n",
    "            q = self.query_layers[i](x)\n",
    "            k = self.key_layers[i](x)\n",
    "            v = self.value_layers[i](x)\n",
    "\n",
    "            score = tf.matmul(q, k, transpose_b=True)\n",
    "            score = score / tf.math.sqrt(tf.cast(self.proj_dim, tf.float32))\n",
    "            weights = tf.nn.softmax(score, axis=-1)\n",
    "            head_output = tf.matmul(weights, v)\n",
    "            heads_output.append(head_output)\n",
    "\n",
    "        return tf.concat(heads_output, axis=-1)\n",
    "\n",
    "# --- Model Tanımı ---\n",
    "def build_model(pad_len, imu_dim, n_classes, wd=3e-3):\n",
    "    inp = Input(shape=(pad_len, imu_dim), name='imu_input')\n",
    "\n",
    "    # --- CNN Encoder ---\n",
    "    x = residual_se_block(inp, filters=64, kernel_size=3, drop_rate=0.20, weight_decay=wd)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = residual_se_block(x, filters=128, kernel_size=5, drop_rate=0.25, weight_decay=wd)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = residual_se_block(x, filters=256, kernel_size=7, drop_rate=0.30, weight_decay=wd)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # --- GRU + LSTM Paralel Yol ---\n",
    "    gru_branch = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd)))(x)\n",
    "    gru_branch = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd)))(gru_branch)\n",
    "\n",
    "    lstm_branch = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(wd)))(x)\n",
    "    lstm_branch = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(wd)))(lstm_branch)\n",
    "\n",
    "    x = Concatenate()([gru_branch, lstm_branch])\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # --- Multihead Attention Katmanı ---\n",
    "    x = MultiHeadAttentionLayer(heads=4, proj_dim=64)(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # --- Fully Connected ---\n",
    "    for units, drop in [(256, 0.5), (128, 0.4)]:\n",
    "        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(drop)(x)\n",
    "\n",
    "    output = Dense(n_classes, activation='softmax', kernel_regularizer=l2(wd), name='main_output')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT 5 - CV: 7740 - LB 0.797\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, backend as K\n",
    "\n",
    "# -------------------------\n",
    "# Custom Layers (serializable)\n",
    "# -------------------------\n",
    "class SEBlock1D(layers.Layer):\n",
    "    def __init__(self, reduction=8, wd=2e-4, name=None, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.reduction = reduction\n",
    "        self.wd = wd\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        ch = int(input_shape[-1])\n",
    "        hidden = max(4, ch // self.reduction)\n",
    "        self.gap = layers.GlobalAveragePooling1D()\n",
    "        self.fc1 = layers.Dense(hidden, activation=\"relu\",\n",
    "                                kernel_regularizer=regularizers.l2(self.wd))\n",
    "        self.fc2 = layers.Dense(ch, activation=\"sigmoid\",\n",
    "                                kernel_regularizer=regularizers.l2(self.wd))\n",
    "        self.reshape = layers.Reshape((1, ch))\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        w = self.gap(x)\n",
    "        w = self.fc1(w)\n",
    "        w = self.fc2(w)\n",
    "        w = self.reshape(w)\n",
    "        return x * w\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"reduction\": self.reduction, \"wd\": self.wd, **super().get_config()}\n",
    "\n",
    "\n",
    "class MultiScaleResBlock(layers.Layer):\n",
    "    def __init__(self, filters, wd=2e-4, dilation_rates=(1,2,4), dropout=0.12, name=None, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.filters = filters\n",
    "        self.wd = wd\n",
    "        self.dilation_rates = tuple(dilation_rates)\n",
    "        self.spatial_dropout_rate = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        in_ch = int(input_shape[-1])\n",
    "        branches = []\n",
    "        for i, d in enumerate(self.dilation_rates):\n",
    "            conv = layers.SeparableConv1D(self.filters // len(self.dilation_rates),\n",
    "                                          kernel_size=3,\n",
    "                                          padding=\"same\",\n",
    "                                          dilation_rate=d,\n",
    "                                          depthwise_regularizer=regularizers.l2(self.wd),\n",
    "                                          pointwise_regularizer=regularizers.l2(self.wd))\n",
    "            branches.append(conv)\n",
    "        self.branches = branches\n",
    "        # after concat we project to exact filters to ensure Add works\n",
    "        self.project_after_concat = layers.Conv1D(self.filters, 1, padding=\"same\",\n",
    "                                                  kernel_regularizer=regularizers.l2(self.wd))\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        self.act = layers.Activation(tf.nn.gelu)\n",
    "        self.se = SEBlock1D(reduction=8, wd=self.wd)\n",
    "        # projection for residual if input channels != filters\n",
    "        if in_ch != self.filters:\n",
    "            self.proj = layers.Conv1D(self.filters, 1, padding=\"same\",\n",
    "                                      kernel_regularizer=regularizers.l2(self.wd))\n",
    "            self.proj_bn = layers.BatchNormalization()\n",
    "        else:\n",
    "            self.proj = None\n",
    "        self.sdrop = layers.SpatialDropout1D(self.spatial_dropout_rate)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        outs = []\n",
    "        for conv in self.branches:\n",
    "            outs.append(conv(x))\n",
    "        out = layers.Concatenate()(outs)\n",
    "        out = self.project_after_concat(out)\n",
    "        out = self.bn(out)\n",
    "        out = self.act(out)\n",
    "        out = self.se(out)\n",
    "        if self.proj is not None:\n",
    "            proj = self.proj(x)\n",
    "            proj = self.proj_bn(proj)\n",
    "        else:\n",
    "            proj = x\n",
    "        out = layers.Add()([proj, out])\n",
    "        out = self.act(out)\n",
    "        out = self.sdrop(out)\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"filters\": self.filters, \"wd\": self.wd,\n",
    "                \"dilation_rates\": self.dilation_rates,\n",
    "                \"dropout\": self.spatial_dropout_rate, **super().get_config()}\n",
    "\n",
    "\n",
    "class AttentionPooling(layers.Layer):\n",
    "    def __init__(self, name=None, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        # uses a Dense(1) + softmax over time, then weighted sum\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.dense = layers.Dense(1, use_bias=False)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # x: (B, T, C)\n",
    "        s = self.dense(x)                 # (B, T, 1)\n",
    "        a = tf.nn.softmax(s, axis=1)      # (B, T, 1)\n",
    "        weighted = x * a                  # broadcast (B,T,C)\n",
    "        pooled = tf.reduce_sum(weighted, axis=1)  # (B, C)\n",
    "        return pooled\n",
    "\n",
    "    def get_config(self):\n",
    "        return {**super().get_config()}\n",
    "\n",
    "\n",
    "class StdPooling(layers.Layer):\n",
    "    def __init__(self, eps=1e-6, name=None, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.eps = eps\n",
    "\n",
    "    def call(self, x):\n",
    "        # x: (B, T, C)\n",
    "        mean = tf.reduce_mean(x, axis=1, keepdims=True)\n",
    "        var = tf.reduce_mean(tf.square(x - mean), axis=1)\n",
    "        std = tf.sqrt(tf.maximum(var, self.eps))\n",
    "        return std  # shape (B, C)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"eps\": self.eps, **super().get_config()}\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Model Builder\n",
    "# -------------------------\n",
    "def build_imu_only_model(pad_len, num_features, num_classes, wd=2e-4):\n",
    "    \"\"\"\n",
    "    Returns compiled Keras model.\n",
    "    pad_len: int\n",
    "    num_features: int (e.g. 7)\n",
    "    num_classes: int (18)\n",
    "    wd: weight decay (L2)\n",
    "    \"\"\"\n",
    "    inp = layers.Input(shape=(pad_len, num_features), name=\"imu_input\")\n",
    "    x = layers.LayerNormalization(name=\"input_ln\")(inp)\n",
    "\n",
    "    # stem\n",
    "    x = layers.Conv1D(64, 3, padding=\"same\",\n",
    "                      kernel_regularizer=regularizers.l2(wd),\n",
    "                      name=\"stem_conv\")(x)\n",
    "    x = layers.BatchNormalization(name=\"stem_bn\")(x)\n",
    "    x = layers.Activation(tf.nn.gelu, name=\"stem_act\")(x)\n",
    "\n",
    "    # stacked blocks\n",
    "    x = MultiScaleResBlock(filters=128, wd=wd, dilation_rates=(1,2,4), name=\"msrb_1\")(x)\n",
    "    x = MultiScaleResBlock(filters=192, wd=wd, dilation_rates=(1,2,4), name=\"msrb_2\")(x)\n",
    "    x = MultiScaleResBlock(filters=256, wd=wd, dilation_rates=(1,2,4), name=\"msrb_3\")(x)\n",
    "\n",
    "    # mid conv\n",
    "    x = layers.Conv1D(256, 3, strides=1, padding=\"same\",\n",
    "                      kernel_regularizer=regularizers.l2(wd), name=\"mid_conv\")(x)\n",
    "    x = layers.BatchNormalization(name=\"mid_bn\")(x)\n",
    "    x = layers.Activation(tf.nn.gelu, name=\"mid_act\")(x)\n",
    "\n",
    "    # hybrid RNN encoder\n",
    "    x = layers.Bidirectional(layers.GRU(180, return_sequences=True, dropout=0.12), name=\"bigru1\")(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(160, return_sequences=True, dropout=0.12), name=\"bilstm1\")(x)\n",
    "\n",
    "    x = layers.Conv1D(256, 1, padding=\"same\", kernel_regularizer=regularizers.l2(wd), name=\"proj_conv\")(x)\n",
    "    x = layers.LayerNormalization(name=\"proj_ln\")(x)\n",
    "\n",
    "    # pooling\n",
    "    att = AttentionPooling(name=\"att_pool\")(x)                # (B, C)\n",
    "    avg = layers.GlobalAveragePooling1D(name=\"avg_pool\")(x)   # (B, C)\n",
    "    mx  = layers.GlobalMaxPooling1D(name=\"max_pool\")(x)       # (B, C)\n",
    "    std = StdPooling(name=\"std_pool\")(x)                     # (B, C)\n",
    "\n",
    "    feat = layers.Concatenate(name=\"final_concat\")([att, avg, mx, std])  # (B, 4*C)\n",
    "\n",
    "    # head\n",
    "    h = layers.Dense(384, activation=tf.nn.gelu,\n",
    "                     kernel_regularizer=regularizers.l2(wd), name=\"head_fc1\")(feat)\n",
    "    h = layers.BatchNormalization(name=\"head_bn1\")(h)\n",
    "    h = layers.Dropout(0.45, name=\"head_drop1\")(h)\n",
    "    h = layers.Dense(192, activation=tf.nn.gelu,\n",
    "                     kernel_regularizer=regularizers.l2(wd), name=\"head_fc2\")(h)\n",
    "    h = layers.BatchNormalization(name=\"head_bn2\")(h)\n",
    "    h = layers.Dropout(0.35, name=\"head_drop2\")(h)\n",
    "\n",
    "    out = layers.Dense(num_classes, activation=\"softmax\", name=\"main_output\",\n",
    "                       kernel_regularizer=regularizers.l2(wd))(h)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out, name=\"IMU_Only_Strong_Serial\")\n",
    "\n",
    "    # optimizer\n",
    "    try:\n",
    "        import tensorflow_addons as tfa\n",
    "        optimizer = tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-5)\n",
    "    except Exception:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, backend as K\n",
    "\n",
    "# -------------------------\n",
    "# Custom Layers (serializable)\n",
    "# -------------------------\n",
    "class SEBlock1D(layers.Layer):\n",
    "    def __init__(self, reduction=8, wd=2e-4, name=None, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.reduction = reduction\n",
    "        self.wd = wd\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        ch = int(input_shape[-1])\n",
    "        hidden = max(4, ch // self.reduction)\n",
    "        self.gap = layers.GlobalAveragePooling1D()\n",
    "        self.fc1 = layers.Dense(hidden, activation=\"relu\",\n",
    "                                kernel_regularizer=regularizers.l2(self.wd))\n",
    "        self.fc2 = layers.Dense(ch, activation=\"sigmoid\",\n",
    "                                kernel_regularizer=regularizers.l2(self.wd))\n",
    "        self.reshape = layers.Reshape((1, ch))\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        w = self.gap(x)\n",
    "        w = self.fc1(w)\n",
    "        w = self.fc2(w)\n",
    "        w = self.reshape(w)\n",
    "        return x * w\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"reduction\": self.reduction, \"wd\": self.wd, **super().get_config()}\n",
    "\n",
    "\n",
    "class MultiScaleResBlock(layers.Layer):\n",
    "    def __init__(self, filters, wd=2e-4, dilation_rates=(1,2,4), dropout=0.12, name=None, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.filters = filters\n",
    "        self.wd = wd\n",
    "        self.dilation_rates = tuple(dilation_rates)\n",
    "        self.spatial_dropout_rate = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        in_ch = int(input_shape[-1])\n",
    "        branches = []\n",
    "        for i, d in enumerate(self.dilation_rates):\n",
    "            conv = layers.SeparableConv1D(self.filters // len(self.dilation_rates),\n",
    "                                          kernel_size=3,\n",
    "                                          padding=\"same\",\n",
    "                                          dilation_rate=d,\n",
    "                                          depthwise_regularizer=regularizers.l2(self.wd),\n",
    "                                          pointwise_regularizer=regularizers.l2(self.wd))\n",
    "            branches.append(conv)\n",
    "        self.branches = branches\n",
    "        # after concat we project to exact filters to ensure Add works\n",
    "        self.project_after_concat = layers.Conv1D(self.filters, 1, padding=\"same\",\n",
    "                                                  kernel_regularizer=regularizers.l2(self.wd))\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        self.act = layers.Activation(tf.nn.gelu)\n",
    "        self.se = SEBlock1D(reduction=8, wd=self.wd)\n",
    "        # projection for residual if input channels != filters\n",
    "        if in_ch != self.filters:\n",
    "            self.proj = layers.Conv1D(self.filters, 1, padding=\"same\",\n",
    "                                      kernel_regularizer=regularizers.l2(self.wd))\n",
    "            self.proj_bn = layers.BatchNormalization()\n",
    "        else:\n",
    "            self.proj = None\n",
    "        self.sdrop = layers.SpatialDropout1D(self.spatial_dropout_rate)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        outs = []\n",
    "        for conv in self.branches:\n",
    "            outs.append(conv(x))\n",
    "        out = layers.Concatenate()(outs)\n",
    "        out = self.project_after_concat(out)\n",
    "        out = self.bn(out)\n",
    "        out = self.act(out)\n",
    "        out = self.se(out)\n",
    "        if self.proj is not None:\n",
    "            proj = self.proj(x)\n",
    "            proj = self.proj_bn(proj)\n",
    "        else:\n",
    "            proj = x\n",
    "        out = layers.Add()([proj, out])\n",
    "        out = self.act(out)\n",
    "        out = self.sdrop(out)\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"filters\": self.filters, \"wd\": self.wd,\n",
    "                \"dilation_rates\": self.dilation_rates,\n",
    "                \"dropout\": self.spatial_dropout_rate, **super().get_config()}\n",
    "\n",
    "\n",
    "class AttentionPooling(layers.Layer):\n",
    "    def __init__(self, name=None, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        # uses a Dense(1) + softmax over time, then weighted sum\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.dense = layers.Dense(1, use_bias=False)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # x: (B, T, C)\n",
    "        s = self.dense(x)                 # (B, T, 1)\n",
    "        a = tf.nn.softmax(s, axis=1)      # (B, T, 1)\n",
    "        weighted = x * a                  # broadcast (B,T,C)\n",
    "        pooled = tf.reduce_sum(weighted, axis=1)  # (B, C)\n",
    "        return pooled\n",
    "\n",
    "    def get_config(self):\n",
    "        return {**super().get_config()}\n",
    "\n",
    "\n",
    "class StdPooling(layers.Layer):\n",
    "    def __init__(self, eps=1e-6, name=None, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.eps = eps\n",
    "\n",
    "    def call(self, x):\n",
    "        # x: (B, T, C)\n",
    "        mean = tf.reduce_mean(x, axis=1, keepdims=True)\n",
    "        var = tf.reduce_mean(tf.square(x - mean), axis=1)\n",
    "        std = tf.sqrt(tf.maximum(var, self.eps))\n",
    "        return std  # shape (B, C)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"eps\": self.eps, **super().get_config()}\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Model Builder\n",
    "# -------------------------\n",
    "def build_imu_only_model(pad_len, num_features, num_classes, wd=2e-4):\n",
    "    \"\"\"\n",
    "    Returns compiled Keras model.\n",
    "    pad_len: int\n",
    "    num_features: int (e.g. 7)\n",
    "    num_classes: int (18)\n",
    "    wd: weight decay (L2)\n",
    "    \"\"\"\n",
    "    inp = layers.Input(shape=(pad_len, num_features), name=\"imu_input\")\n",
    "    x = layers.LayerNormalization(name=\"input_ln\")(inp)\n",
    "\n",
    "    # stem\n",
    "    x = layers.Conv1D(64, 3, padding=\"same\",\n",
    "                      kernel_regularizer=regularizers.l2(wd),\n",
    "                      name=\"stem_conv\")(x)\n",
    "    x = layers.BatchNormalization(name=\"stem_bn\")(x)\n",
    "    x = layers.Activation(tf.nn.gelu, name=\"stem_act\")(x)\n",
    "\n",
    "    # stacked blocks\n",
    "    x = MultiScaleResBlock(filters=128, wd=wd, dilation_rates=(1,2,4), name=\"msrb_1\")(x)\n",
    "    x = MultiScaleResBlock(filters=192, wd=wd, dilation_rates=(1,2,4), name=\"msrb_2\")(x)\n",
    "    x = MultiScaleResBlock(filters=256, wd=wd, dilation_rates=(1,2,4), name=\"msrb_3\")(x)\n",
    "\n",
    "    # mid conv\n",
    "    x = layers.Conv1D(256, 3, strides=1, padding=\"same\",\n",
    "                      kernel_regularizer=regularizers.l2(wd), name=\"mid_conv\")(x)\n",
    "    x = layers.BatchNormalization(name=\"mid_bn\")(x)\n",
    "    x = layers.Activation(tf.nn.gelu, name=\"mid_act\")(x)\n",
    "\n",
    "    # hybrid RNN encoder\n",
    "    x = layers.Bidirectional(layers.GRU(180, return_sequences=True, dropout=0.12), name=\"bigru1\")(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(160, return_sequences=True, dropout=0.12), name=\"bilstm1\")(x)\n",
    "\n",
    "    x = layers.Conv1D(256, 1, padding=\"same\", kernel_regularizer=regularizers.l2(wd), name=\"proj_conv\")(x)\n",
    "    x = layers.LayerNormalization(name=\"proj_ln\")(x)\n",
    "\n",
    "    # pooling\n",
    "    att = AttentionPooling(name=\"att_pool\")(x)                # (B, C)\n",
    "    avg = layers.GlobalAveragePooling1D(name=\"avg_pool\")(x)   # (B, C)\n",
    "    mx  = layers.GlobalMaxPooling1D(name=\"max_pool\")(x)       # (B, C)\n",
    "    std = StdPooling(name=\"std_pool\")(x)                     # (B, C)\n",
    "\n",
    "    feat = layers.Concatenate(name=\"final_concat\")([att, avg, mx, std])  # (B, 4*C)\n",
    "\n",
    "    # head\n",
    "    h = layers.Dense(384, activation=tf.nn.gelu,\n",
    "                     kernel_regularizer=regularizers.l2(wd), name=\"head_fc1\")(feat)\n",
    "    h = layers.BatchNormalization(name=\"head_bn1\")(h)\n",
    "    h = layers.Dropout(0.45, name=\"head_drop1\")(h)\n",
    "    h = layers.Dense(192, activation=tf.nn.gelu,\n",
    "                     kernel_regularizer=regularizers.l2(wd), name=\"head_fc2\")(h)\n",
    "    h = layers.BatchNormalization(name=\"head_bn2\")(h)\n",
    "    h = layers.Dropout(0.35, name=\"head_drop2\")(h)\n",
    "\n",
    "    out = layers.Dense(num_classes, activation=\"softmax\", name=\"main_output\",\n",
    "                       kernel_regularizer=regularizers.l2(wd))(h)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out, name=\"IMU_Only_Strong_Serial\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT5 - 1\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, backend as K\n",
    "\n",
    "def se_block(x, reduction=8, name=None):\n",
    "    channels = int(x.shape[-1])\n",
    "    se = layers.GlobalAveragePooling1D(name=(None if name is None else name+\"_gap\"))(x)\n",
    "    se = layers.Dense(max(4, channels//reduction), activation=\"relu\", name=(None if name is None else name+\"_fc1\"))(se)\n",
    "    se = layers.Dense(channels, activation=\"sigmoid\", name=(None if name is None else name+\"_fc2\"))(se)\n",
    "    se = layers.Reshape((1, channels))(se)\n",
    "    return layers.Multiply(name=(None if name is None else name+\"_scale\"))([x, se])\n",
    "\n",
    "def residual_inception_block(x, filters, wd, name=None):\n",
    "    # multi-kernel branch (3,5,7) using SeparableConv1D (efficient)\n",
    "    branch1 = layers.SeparableConv1D(filters//3, 3, padding=\"same\",\n",
    "                                     depthwise_regularizer=regularizers.l2(wd),\n",
    "                                     pointwise_regularizer=regularizers.l2(wd),\n",
    "                                     name=(None if name is None else name+\"_b1\"))(x)\n",
    "    branch2 = layers.SeparableConv1D(filters//3, 5, padding=\"same\",\n",
    "                                     depthwise_regularizer=regularizers.l2(wd),\n",
    "                                     pointwise_regularizer=regularizers.l2(wd),\n",
    "                                     name=(None if name is None else name+\"_b2\"))(x)\n",
    "    branch3 = layers.SeparableConv1D(filters - 2*(filters//3), 7, padding=\"same\",\n",
    "                                     depthwise_regularizer=regularizers.l2(wd),\n",
    "                                     pointwise_regularizer=regularizers.l2(wd),\n",
    "                                     name=(None if name is None else name+\"_b3\"))(x)\n",
    "\n",
    "    out = layers.Concatenate(name=(None if name is None else name+\"_concat\"))([branch1, branch2, branch3])\n",
    "    out = layers.BatchNormalization(name=(None if name is None else name+\"_bn\"))(out)\n",
    "    out = layers.Activation(\"gelu\", name=(None if name is None else name+\"_act\"))(out)\n",
    "\n",
    "    # squeeze-excite\n",
    "    out = se_block(out, reduction=8, name=(None if name is None else name+\"_se\"))\n",
    "\n",
    "    # residual projection if required\n",
    "    if int(x.shape[-1]) != filters:\n",
    "        proj = layers.Conv1D(filters, 1, padding=\"same\",\n",
    "                             kernel_regularizer=regularizers.l2(wd),\n",
    "                             name=(None if name is None else name+\"_proj\"))(x)\n",
    "        proj = layers.BatchNormalization()(proj)\n",
    "    else:\n",
    "        proj = x\n",
    "\n",
    "    out = layers.Add(name=(None if name is None else name+\"_resadd\"))([proj, out])\n",
    "    out = layers.Activation(\"gelu\", name=(None if name is None else name+\"_resact\"))(out)\n",
    "    out = layers.SpatialDropout1D(0.1, name=(None if name is None else name+\"_drop\"))(out)\n",
    "    return out\n",
    "\n",
    "def transformer_encoder_block(x, head=4, ff_dim=None, dropout=0.1, wd=2e-4, name=None):\n",
    "    d_model = int(x.shape[-1])\n",
    "    if ff_dim is None:\n",
    "        ff_dim = d_model * 2\n",
    "    attn = layers.MultiHeadAttention(num_heads=head, key_dim=d_model//head, name=(None if name is None else name+\"_mha\"))(x, x)\n",
    "    attn = layers.Dropout(dropout)(attn)\n",
    "    x = layers.Add()([x, attn])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "    ff = layers.Dense(ff_dim, activation=\"gelu\",\n",
    "                      kernel_regularizer=regularizers.l2(wd))(x)\n",
    "    ff = layers.Dense(d_model, kernel_regularizer=regularizers.l2(wd))(ff)\n",
    "    ff = layers.Dropout(dropout)(ff)\n",
    "    x = layers.Add()([x, ff])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    return x\n",
    "\n",
    "class AttentionPooling(layers.Layer):\n",
    "    \"\"\"Attention pooling layer (learned temporal pooling).\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionPooling, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.dense = layers.Dense(1, use_bias=False, name=self.name + \"_attn_dense\")\n",
    "        self.softmax = layers.Softmax(axis=1, name=self.name + \"_attn_softmax\")\n",
    "        self.multiply = layers.Multiply()\n",
    "        super(AttentionPooling, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs: (B, T, C)\n",
    "        attn_scores = self.dense(inputs)  # (B, T, 1)\n",
    "        attn_scores = self.softmax(attn_scores)\n",
    "        pooled = self.multiply([inputs, attn_scores])\n",
    "        pooled = K.sum(pooled, axis=1)  # (B, C)\n",
    "        return pooled\n",
    "    \n",
    "    # Bu metod, modelin doğru bir şekilde kaydedilip yüklenebilmesi için gereklidir.\n",
    "    def get_config(self):\n",
    "        config = super(AttentionPooling, self).get_config()\n",
    "        return config\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def build_imu_only_model(pad_len, num_features, num_classes, wd=2e-4):\n",
    "    \"\"\"\n",
    "    Returns a compiled Keras model ready for training.\n",
    "    pad_len: int, time dimension after pad/truncate\n",
    "    num_features: int, IMU channels (e.g. 7)\n",
    "    num_classes: int, 18\n",
    "    wd: float, L2 weight decay applied via kernel_regularizer\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(pad_len, num_features), name=\"imu_input\")\n",
    "    x = layers.LayerNormalization(name=\"input_ln\")(inputs)\n",
    "\n",
    "    # initial conv stem\n",
    "    x = layers.Conv1D(64, 3, padding=\"same\", kernel_regularizer=regularizers.l2(wd), name=\"stem_conv\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"gelu\")(x)\n",
    "\n",
    "    # stacked multi-scale residual blocks (keep temporal resolution)\n",
    "    x = residual_inception_block(x, 128, wd, name=\"res_inc_1\")\n",
    "    x = residual_inception_block(x, 192, wd, name=\"res_inc_2\")\n",
    "    x = residual_inception_block(x, 256, wd, name=\"res_inc_3\")\n",
    "\n",
    "    # lightweight sequence encoder (BiGRU) to capture mid-range temporal patterns\n",
    "    x = layers.Bidirectional(layers.GRU(160, return_sequences=True, dropout=0.15), name=\"bigru\")(x)  # out dim 320\n",
    "\n",
    "    # project down to manageable dim before transformer\n",
    "    x = layers.Conv1D(256, 1, padding=\"same\", kernel_regularizer=regularizers.l2(wd), name=\"proj\")(x)\n",
    "    x = layers.LayerNormalization(name=\"proj_ln\")(x)\n",
    "\n",
    "    # transformer encoder stack (2 blocks)\n",
    "    x = transformer_encoder_block(x, head=4, ff_dim=512, dropout=0.12, wd=wd, name=\"trans_enc1\")\n",
    "    x = transformer_encoder_block(x, head=4, ff_dim=512, dropout=0.12, wd=wd, name=\"trans_enc2\")\n",
    "\n",
    "    # attention pooling (learned temporal pooling)\n",
    "    pooled = AttentionPooling(name=\"attn_pool\")(x) # (B, C)\n",
    "\n",
    "    # classifier head\n",
    "    h = layers.Dense(256, activation=\"gelu\", kernel_regularizer=regularizers.l2(wd), name=\"head_fc1\")(pooled)\n",
    "    h = layers.BatchNormalization(name=\"head_bn\")(h)\n",
    "    h = layers.Dropout(0.45, name=\"head_drop\")(h)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"main_output\",\n",
    "                           kernel_regularizer=regularizers.l2(wd))(h)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Compile with recommended settings for competition (label smoothing, stable optimizer)\n",
    "    # opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    # model.compile(optimizer=opt,\n",
    "    #               loss={'main_output': tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)},\n",
    "    #               metrics={'main_output': 'accuracy'})\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU 2 | 0.7740 CV - 0.80 LB   (wd: 3e-3 kullanılmıştır)\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# --- Yardımcı Fonksiyonlar ---\n",
    "def time_sum(x):\n",
    "    return K.sum(x, axis=1)\n",
    "\n",
    "def squeeze_last_axis(x):\n",
    "    return tf.squeeze(x, axis=-1)\n",
    "\n",
    "def expand_last_axis(x):\n",
    "    return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "# --- SE Blok ---\n",
    "def se_block(input_tensor, ratio=8):\n",
    "    channel_axis = -1\n",
    "    filters = input_tensor.shape[channel_axis]\n",
    "    se = GlobalAveragePooling1D()(input_tensor)\n",
    "    se = Dense(filters // ratio, activation='relu')(se)\n",
    "    se = Dense(filters, activation='sigmoid')(se)\n",
    "    se = Reshape((1, filters))(se)\n",
    "    x = Multiply()([input_tensor, se])\n",
    "    return x\n",
    "\n",
    "# --- Residual CNN Blok ---\n",
    "def residual_cnn_block(x, filters, kernel_size, drop_rate=0.3, weight_decay=1e-4):\n",
    "    shortcut = x\n",
    "\n",
    "    x = Conv1D(filters, kernel_size, padding='same', use_bias=False, kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv1D(filters, kernel_size, padding='same', use_bias=False, kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding='same', use_bias=False, kernel_regularizer=l2(weight_decay))(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(drop_rate)(x)\n",
    "    return x\n",
    "\n",
    "# --- Attention Katmanı ---\n",
    "def attention_block(inputs):\n",
    "    score = Dense(1, activation='tanh')(inputs)\n",
    "    score = Lambda(squeeze_last_axis)(score)\n",
    "    weights = Activation('softmax')(score)\n",
    "    weights = Lambda(expand_last_axis)(weights)\n",
    "    weighted = Multiply()([inputs, weights])\n",
    "    return Lambda(time_sum)(weighted)\n",
    "\n",
    "# --- Feature Aggregation Blok ---\n",
    "def feature_aggregation_block(x):\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    return Concatenate()([avg_pool, max_pool])\n",
    "\n",
    "# --- Model Tanımı ---\n",
    "def build_model(pad_len, imu_dim, n_classes, wd=3e-3):\n",
    "    inp = Input(shape=(pad_len, imu_dim), name='imu_input')\n",
    "\n",
    "    # --- CNN Encoder ---\n",
    "    x = residual_cnn_block(inp, filters=64, kernel_size=3, drop_rate=0.20, weight_decay=wd)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = residual_cnn_block(x, filters=128, kernel_size=5, drop_rate=0.25, weight_decay=wd)\n",
    "    x = se_block(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = residual_cnn_block(x, filters=256, kernel_size=7, drop_rate=0.30, weight_decay=wd)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # --- GRU + LSTM Paralel ---\n",
    "    gru_branch = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd)))(x)\n",
    "    lstm_branch = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(wd)))(x)\n",
    "    x = Concatenate()([gru_branch, lstm_branch])\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # --- Attention ---\n",
    "    x = attention_block(x)\n",
    "\n",
    "    # --- Feature Aggregation (dense öncesi alternatif zenginleşirme) ---\n",
    "    # x = feature_aggregation_block(x)  # opsiyonel\n",
    "\n",
    "    # --- Fully Connected ---\n",
    "    for units, drop in [(256, 0.5), (128, 0.4)]:\n",
    "        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(drop)(x)\n",
    "\n",
    "    output = Dense(n_classes, activation='softmax', kernel_regularizer=l2(wd), name='main_output')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T13:28:52.013491Z",
     "iopub.status.busy": "2025-07-04T13:28:52.013231Z",
     "iopub.status.idle": "2025-07-04T13:28:52.034252Z",
     "shell.execute_reply": "2025-07-04T13:28:52.033394Z",
     "shell.execute_reply.started": "2025-07-04T13:28:52.013470Z"
    },
    "trusted": true
   },
   "source": [
    "# GRU 3 | 0.7755 CV - 0.80 LB    (ls1 - wd: 2e-4 kullanılmıştır.)\n",
    "###  YARIŞMA ANALİZİ TARANARAK\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def time_sum(x): return K.sum(x, axis=1)\n",
    "def squeeze_last_axis(x): return tf.squeeze(x, axis=-1)\n",
    "def expand_last_axis(x): return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "def se_block(input_tensor, ratio=8):\n",
    "    ch = input_tensor.shape[-1]\n",
    "    se = GlobalAveragePooling1D()(input_tensor)\n",
    "    se = Dense(ch // ratio, activation='relu')(se)\n",
    "    se = Dense(ch, activation='sigmoid')(se)\n",
    "    se = Reshape((1, ch))(se)\n",
    "    return Multiply()([input_tensor, se])\n",
    "\n",
    "def residual_cnn_block(x, filters, kernel_size, drop_rate=0.25, weight_decay=2e-4, dilation=1):\n",
    "    shortcut = x\n",
    "    x = Conv1D(filters, kernel_size, padding='same', dilation_rate=dilation,\n",
    "               kernel_regularizer=l2(weight_decay), use_bias=False)(x)\n",
    "    x = BatchNormalization()(x); x = Activation('relu')(x)\n",
    "    x = Conv1D(filters, kernel_size, padding='same', dilation_rate=dilation,\n",
    "               kernel_regularizer=l2(weight_decay), use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding='same', use_bias=False, kernel_regularizer=l2(weight_decay))(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    x = Add()([x, shortcut]); x = Activation('relu')(x); x = Dropout(drop_rate)(x)\n",
    "    return x\n",
    "\n",
    "def attention_block(inputs):\n",
    "    score = Dense(1, activation='tanh')(inputs)\n",
    "    score = Lambda(squeeze_last_axis)(score)\n",
    "    weights = Activation('softmax')(score)\n",
    "    weights = Lambda(expand_last_axis)(weights)\n",
    "    return Lambda(time_sum)(Multiply()([inputs, weights]))\n",
    "\n",
    "def build_model(pad_len, imu_dim, n_classes, wd=2e-4):\n",
    "    inp = Input(shape=(pad_len, imu_dim), name='imu_input')\n",
    "    x = residual_cnn_block(inp, 64, 3, drop_rate=0.2, weight_decay=wd, dilation=1)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = residual_cnn_block(x, 128, 5, drop_rate=0.25, weight_decay=wd, dilation=2)\n",
    "    x = se_block(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = residual_cnn_block(x, 256, 7, drop_rate=0.3, weight_decay=wd, dilation=2)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    gru_branch = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd)))(x)\n",
    "    lstm_branch = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(wd)))(x)\n",
    "    x = Concatenate()([gru_branch, lstm_branch])\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = attention_block(x)\n",
    "\n",
    "    for units, drop in [(256, 0.4), (128, 0.3)]:\n",
    "        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(drop)(x)\n",
    "\n",
    "    output = Dense(n_classes, activation='softmax', name='main_output', kernel_regularizer=l2(wd))(x)\n",
    "    return Model(inputs=inp, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEÇMİŞ YARIŞMA ÇÖZÜMLERİNE GÖRE OLUŞTURULDU - DENENECEK \n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def time_sum(x): return K.sum(x, axis=1)\n",
    "def squeeze_last_axis(x): return tf.squeeze(x, axis=-1)\n",
    "def expand_last_axis(x): return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "def se_block(inp, ratio=8):\n",
    "    ch = inp.shape[-1]\n",
    "    se = GlobalAveragePooling1D()(inp)\n",
    "    se = Dense(ch//ratio, activation='relu')(se)\n",
    "    se = Dense(ch, activation='sigmoid')(se)\n",
    "    se = Reshape((1,ch))(se)\n",
    "    return Multiply()([inp, se])\n",
    "    \n",
    "class SegmentAttention(Layer):\n",
    "    def __init__(self, n_segments=4, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_segments = n_segments\n",
    "        # Her segmentin temsilcisi için bir ağırlık oluşturur\n",
    "        self.dense_weights = Dense(1)\n",
    "        self.softmax_activation = Activation('softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        seg_len = tf.shape(x)[1] // self.n_segments\n",
    "        \n",
    "        # Her segmentin ortalamasını alarak bir temsilci vektör oluştur\n",
    "        segs = [tf.reduce_mean(x[:, i*seg_len:(i+1)*seg_len, :], axis=1, keepdims=True)\n",
    "                for i in range(self.n_segments)]\n",
    "        stacked = Concatenate(axis=1)(segs) # Boyut: (None, 4, 512)\n",
    "        \n",
    "        # Ağırlıkları her bir segmentin ortalaması üzerinden hesapla\n",
    "        weights = self.dense_weights(stacked) # Boyut: (None, 4, 1)\n",
    "        weights = self.softmax_activation(weights) # softmax(weights)\n",
    "        \n",
    "        # Ağırlıklı segmentleri topla\n",
    "        weighted_segments = Multiply()([stacked, weights])\n",
    "        \n",
    "        return tf.reduce_sum(weighted_segments, axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'n_segments': self.n_segments})\n",
    "        return config\n",
    "def residual_cnn_block(x, filters, kernel_size, drop_rate=0.2, weight_decay=2e-4, dilation=1):\n",
    "    sc = x\n",
    "    x = Conv1D(filters, kernel_size, dilation_rate=dilation, padding='same', use_bias=False,\n",
    "               kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = BatchNormalization()(x); x = Activation('relu')(x)\n",
    "    x = Conv1D(filters, kernel_size, dilation_rate=dilation, padding='same', use_bias=False,\n",
    "               kernel_regularizer=l2(weight_decay))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if sc.shape[-1] != filters:\n",
    "        sc = Conv1D(filters,1,padding='same',use_bias=False,kernel_regularizer=l2(weight_decay))(sc)\n",
    "        sc = BatchNormalization()(sc)\n",
    "    x = Add()([x, sc]); x = Activation('relu')(x); x = Dropout(drop_rate)(x)\n",
    "    return x\n",
    "\n",
    "def attention_block(inputs):\n",
    "    score = Dense(1, activation='tanh')(inputs)\n",
    "    score = Lambda(squeeze_last_axis)(score)\n",
    "    weights = Activation('softmax')(score)\n",
    "    weights = Lambda(expand_last_axis)(weights)\n",
    "    return Lambda(time_sum)(Multiply()([inputs,weights]))\n",
    "\n",
    "\n",
    "def build_model(pad_len, imu_dim, n_classes):\n",
    "    inp = Input(shape=(pad_len, imu_dim), name='imu_input')\n",
    "    x = residual_cnn_block(inp,64,3,drop_rate=0.2,dilation=1)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = residual_cnn_block(x,128,5,drop_rate=0.25,dilation=2)\n",
    "    x = se_block(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = residual_cnn_block(x,256,7,drop_rate=0.3,dilation=2)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    # Parallel 双 RNN\n",
    "    gru = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(2e-4)))(x)\n",
    "    lstm = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(2e-4)))(x)\n",
    "    x = Concatenate()([gru, lstm])\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    # Segment attention + Temporal attention\n",
    "    seg_att = SegmentAttention(n_segments=4)(x)\n",
    "    temp_att = attention_block(x)\n",
    "    x = Concatenate()([seg_att, temp_att])\n",
    "\n",
    "    for units, drop in [(256,0.4),(128,0.3)]:\n",
    "        x = Dense(units, use_bias=False, kernel_regularizer=l2(2e-4))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(drop)(x)\n",
    "\n",
    "    output = Dense(n_classes, activation='softmax', name='main_output',\n",
    "                   kernel_regularizer=l2(2e-4))(x)\n",
    "    return Model(inputs=inp, outputs=output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# -- Yardımcı Fonksiyonlar --\n",
    "def time_sum(x): return K.sum(x, axis=1)\n",
    "def squeeze_last_axis(x): return tf.squeeze(x, axis=-1)\n",
    "def expand_last_axis(x): return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "# -- SE Blok --\n",
    "def se_block(x, ratio=8):\n",
    "    ch = x.shape[-1]\n",
    "    se = GlobalAveragePooling1D()(x)\n",
    "    se = Dense(ch // ratio, activation='relu')(se)\n",
    "    se = Dense(ch, activation='sigmoid')(se)\n",
    "    se = Reshape((1, ch))(se)\n",
    "    return Multiply()([x, se])\n",
    "\n",
    "# -- Residual CNN Blok --\n",
    "def residual_cnn_block(x, filters, kernel_size, dilation=1, drop_rate=0.3, wd=1e-4):\n",
    "    shortcut = x\n",
    "    x = Conv1D(filters, kernel_size, padding='same', dilation_rate=dilation, use_bias=False,\n",
    "               kernel_regularizer=l2(wd))(x)\n",
    "    x = BatchNormalization()(x); x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv1D(filters, kernel_size, padding='same', dilation_rate=dilation, use_bias=False,\n",
    "               kernel_regularizer=l2(wd))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding='same', use_bias=False, kernel_regularizer=l2(wd))(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(drop_rate)(x)\n",
    "    return x\n",
    "\n",
    "# -- Attention --\n",
    "def attention_block(x):\n",
    "    score = Dense(1, activation='tanh')(x)\n",
    "    score = Lambda(squeeze_last_axis)(score)\n",
    "    weights = Activation('softmax')(score)\n",
    "    weights = Lambda(expand_last_axis)(weights)\n",
    "    return Lambda(time_sum)(Multiply()([x, weights]))\n",
    "\n",
    "# -- Model --\n",
    "def build_model(pad_len, imu_dim, n_classes, wd=1e-4):\n",
    "    inp = Input(shape=(pad_len, imu_dim), name='imu_input')\n",
    "\n",
    "    # -- Encoder: Residual CNN + SE --\n",
    "    x = residual_cnn_block(inp, 64, 3, dilation=1, drop_rate=0.2, wd=wd)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = residual_cnn_block(x, 128, 5, dilation=2, drop_rate=0.25, wd=wd)\n",
    "    x = se_block(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = residual_cnn_block(x, 256, 7, dilation=2, drop_rate=0.3, wd=wd)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # -- GRU + LSTM Paralel --\n",
    "    gru_out = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd)))(x)\n",
    "    lstm_out = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(wd)))(x)\n",
    "    x = Concatenate()([gru_out, lstm_out])\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # -- Attention --\n",
    "    x = attention_block(x)\n",
    "\n",
    "    # -- Fully Connected --\n",
    "    for units, drop in [(256, 0.4), (128, 0.3)]:\n",
    "        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(drop)(x)\n",
    "\n",
    "    out = Dense(n_classes, activation='softmax', kernel_regularizer=l2(wd), name='main_output')(x)\n",
    "    return Model(inputs=inp, outputs=out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T13:28:52.036653Z",
     "iopub.status.busy": "2025-07-04T13:28:52.036359Z",
     "iopub.status.idle": "2025-07-04T13:28:52.058098Z",
     "shell.execute_reply": "2025-07-04T13:28:52.057140Z",
     "shell.execute_reply.started": "2025-07-04T13:28:52.036628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import sobel\n",
    "\n",
    "# ToF için spatial gradyan (sobel) temelli özellikler\n",
    "def calculate_spatial_tof_features(seq_df, sensor_id):\n",
    "    # 1D 64-pikseli 8x8'e reshape edip sobel gradyanı alacağız\n",
    "    pixel_cols = [f\"tof_{sensor_id}_v{p}\" for p in range(64)]\n",
    "    tof_data = seq_df[pixel_cols].replace(-1, np.nan).ffill().bfill().fillna(0).values\n",
    "    \n",
    "    # Frame sayısı x 64 → (N x 8 x 8)\n",
    "    N = len(seq_df)\n",
    "    reshaped = tof_data.reshape(N, 8, 8)\n",
    "    \n",
    "    # Spatial gradyanları hesapla (sobel x ve y)\n",
    "    sobel_x = sobel(reshaped, axis=1)\n",
    "    sobel_y = sobel(reshaped, axis=2)\n",
    "    grad_mag = np.sqrt(sobel_x ** 2 + sobel_y ** 2)\n",
    "\n",
    "    # Özet istatistikleri hesapla\n",
    "    grad_mean = grad_mag.mean(axis=(1, 2))\n",
    "    grad_std  = grad_mag.std(axis=(1, 2))\n",
    "    grad_max  = grad_mag.max(axis=(1, 2))\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        f'tof_{sensor_id}_grad_mean': grad_mean,\n",
    "        f'tof_{sensor_id}_grad_std': grad_std,\n",
    "        f'tof_{sensor_id}_grad_max': grad_max\n",
    "    }, index=seq_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T13:28:52.059207Z",
     "iopub.status.busy": "2025-07-04T13:28:52.058943Z",
     "iopub.status.idle": "2025-07-04T13:28:52.234783Z",
     "shell.execute_reply": "2025-07-04T13:28:52.233780Z",
     "shell.execute_reply.started": "2025-07-04T13:28:52.059187Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "def count_peaks(series):\n",
    "    peaks, _ = find_peaks(series, height=np.mean(series))\n",
    "    return len(peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T14:37:28.597398Z",
     "iopub.status.busy": "2025-07-04T14:37:28.597100Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ TRAIN MODE – loading dataset ...\n",
      "acc_y ortalaması negatif olan subject'ler: ['SUBJ_019262', 'SUBJ_045235']\n",
      "  Removing gravity and calculating linear acceleration features...\n",
      "  Calculating angular velocity and distance from quaternions...\n",
      "  IMU (phys-based + enhanced) 31\n",
      "  Building sequences...\n",
      "  Fitting StandardScaler...\n",
      "  Scaling and padding sequences...\n",
      "  Starting training with Stratified Group K-Fold CV...\n",
      "\n",
      "===== FOLD 1/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754719432.249455    1479 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21770 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754719442.342705    2374 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 134ms/step - accuracy: 0.0886 - loss: 4.3596 - val_accuracy: 0.0769 - val_loss: 4.3522 - learning_rate: 5.0000e-04\n",
      "Epoch 2/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.1587 - loss: 3.6735 - val_accuracy: 0.1065 - val_loss: 3.9532 - learning_rate: 5.0000e-04\n",
      "Epoch 3/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.2264 - loss: 3.3686 - val_accuracy: 0.1378 - val_loss: 3.6887 - learning_rate: 5.0000e-04\n",
      "Epoch 4/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.2568 - loss: 3.2158 - val_accuracy: 0.2634 - val_loss: 3.1678 - learning_rate: 5.0000e-04\n",
      "Epoch 5/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.2966 - loss: 3.1529 - val_accuracy: 0.3735 - val_loss: 2.8606 - learning_rate: 5.0000e-04\n",
      "Epoch 6/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.3356 - loss: 2.9732 - val_accuracy: 0.4142 - val_loss: 2.7433 - learning_rate: 5.0000e-04\n",
      "Epoch 7/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.3604 - loss: 2.8651 - val_accuracy: 0.3932 - val_loss: 2.7835 - learning_rate: 5.0000e-04\n",
      "Epoch 8/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.3757 - loss: 2.8412 - val_accuracy: 0.4437 - val_loss: 2.6756 - learning_rate: 5.0000e-04\n",
      "Epoch 9/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.3925 - loss: 2.7839 - val_accuracy: 0.4246 - val_loss: 2.6799 - learning_rate: 5.0000e-04\n",
      "Epoch 10/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.4272 - loss: 2.6416 - val_accuracy: 0.4831 - val_loss: 2.5105 - learning_rate: 5.0000e-04\n",
      "Epoch 11/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.4333 - loss: 2.6540 - val_accuracy: 0.4622 - val_loss: 2.6235 - learning_rate: 5.0000e-04\n",
      "Epoch 12/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.4180 - loss: 2.6597 - val_accuracy: 0.4640 - val_loss: 2.5069 - learning_rate: 5.0000e-04\n",
      "Epoch 13/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.4642 - loss: 2.6143 - val_accuracy: 0.4357 - val_loss: 2.6413 - learning_rate: 5.0000e-04\n",
      "Epoch 14/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.4720 - loss: 2.4959 - val_accuracy: 0.4991 - val_loss: 2.4468 - learning_rate: 5.0000e-04\n",
      "Epoch 15/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.4895 - loss: 2.4748 - val_accuracy: 0.5151 - val_loss: 2.3944 - learning_rate: 5.0000e-04\n",
      "Epoch 16/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.4915 - loss: 2.4220 - val_accuracy: 0.4837 - val_loss: 2.4438 - learning_rate: 5.0000e-04\n",
      "Epoch 17/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.4838 - loss: 2.4706 - val_accuracy: 0.5218 - val_loss: 2.3241 - learning_rate: 5.0000e-04\n",
      "Epoch 18/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5093 - loss: 2.3530 - val_accuracy: 0.5052 - val_loss: 2.3134 - learning_rate: 5.0000e-04\n",
      "Epoch 19/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.5134 - loss: 2.3679 - val_accuracy: 0.5378 - val_loss: 2.2822 - learning_rate: 5.0000e-04\n",
      "Epoch 20/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5137 - loss: 2.3731 - val_accuracy: 0.5163 - val_loss: 2.3178 - learning_rate: 5.0000e-04\n",
      "Epoch 21/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.5500 - loss: 2.2113 - val_accuracy: 0.5372 - val_loss: 2.2935 - learning_rate: 5.0000e-04\n",
      "Epoch 22/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.5470 - loss: 2.2818 - val_accuracy: 0.5569 - val_loss: 2.1749 - learning_rate: 5.0000e-04\n",
      "Epoch 23/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5406 - loss: 2.2726 - val_accuracy: 0.5514 - val_loss: 2.2420 - learning_rate: 5.0000e-04\n",
      "Epoch 24/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.5099 - loss: 2.4021 - val_accuracy: 0.5446 - val_loss: 2.1842 - learning_rate: 5.0000e-04\n",
      "Epoch 25/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.5461 - loss: 2.2278 - val_accuracy: 0.5323 - val_loss: 2.2797 - learning_rate: 5.0000e-04\n",
      "Epoch 26/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5680 - loss: 2.1580 - val_accuracy: 0.5175 - val_loss: 2.2652 - learning_rate: 5.0000e-04\n",
      "Epoch 27/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.5495 - loss: 2.1631 - val_accuracy: 0.5575 - val_loss: 2.1030 - learning_rate: 5.0000e-04\n",
      "Epoch 28/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.5745 - loss: 2.1364 - val_accuracy: 0.5594 - val_loss: 2.1382 - learning_rate: 5.0000e-04\n",
      "Epoch 29/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.5753 - loss: 2.1148 - val_accuracy: 0.5668 - val_loss: 2.1172 - learning_rate: 5.0000e-04\n",
      "Epoch 30/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.5875 - loss: 2.0902 - val_accuracy: 0.5643 - val_loss: 2.0999 - learning_rate: 5.0000e-04\n",
      "Epoch 31/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5930 - loss: 2.0437 - val_accuracy: 0.5243 - val_loss: 2.2245 - learning_rate: 5.0000e-04\n",
      "Epoch 32/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.5724 - loss: 2.1593 - val_accuracy: 0.5742 - val_loss: 2.1122 - learning_rate: 5.0000e-04\n",
      "Epoch 33/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.5863 - loss: 2.0572 - val_accuracy: 0.5625 - val_loss: 2.1380 - learning_rate: 5.0000e-04\n",
      "Epoch 34/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 127ms/step - accuracy: 0.5913 - loss: 2.0143 - val_accuracy: 0.5840 - val_loss: 2.0447 - learning_rate: 5.0000e-04\n",
      "Epoch 35/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.6308 - loss: 1.9388 - val_accuracy: 0.5631 - val_loss: 2.1414 - learning_rate: 5.0000e-04\n",
      "Epoch 36/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 127ms/step - accuracy: 0.6056 - loss: 2.0216 - val_accuracy: 0.6062 - val_loss: 1.9646 - learning_rate: 5.0000e-04\n",
      "Epoch 37/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.6343 - loss: 1.9264 - val_accuracy: 0.5803 - val_loss: 2.0081 - learning_rate: 5.0000e-04\n",
      "Epoch 38/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.6378 - loss: 1.8882 - val_accuracy: 0.5477 - val_loss: 2.1192 - learning_rate: 5.0000e-04\n",
      "Epoch 39/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6163 - loss: 1.9293 - val_accuracy: 0.5988 - val_loss: 1.9282 - learning_rate: 5.0000e-04\n",
      "Epoch 40/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.6249 - loss: 1.9302 - val_accuracy: 0.5957 - val_loss: 1.9732 - learning_rate: 5.0000e-04\n",
      "Epoch 41/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6647 - loss: 1.8379 - val_accuracy: 0.6043 - val_loss: 1.9341 - learning_rate: 5.0000e-04\n",
      "Epoch 42/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.6392 - loss: 1.9285 - val_accuracy: 0.5895 - val_loss: 1.9396 - learning_rate: 5.0000e-04\n",
      "Epoch 43/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.6652 - loss: 1.8360 - val_accuracy: 0.5637 - val_loss: 2.0594 - learning_rate: 5.0000e-04\n",
      "Epoch 44/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6378 - loss: 1.8913 - val_accuracy: 0.5834 - val_loss: 2.0207 - learning_rate: 5.0000e-04\n",
      "Epoch 45/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.6499 - loss: 1.8691 - val_accuracy: 0.5754 - val_loss: 2.0526 - learning_rate: 5.0000e-04\n",
      "Epoch 46/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.6477 - loss: 1.8656 - val_accuracy: 0.6160 - val_loss: 1.8797 - learning_rate: 5.0000e-04\n",
      "Epoch 47/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.6837 - loss: 1.7663 - val_accuracy: 0.5858 - val_loss: 1.9979 - learning_rate: 5.0000e-04\n",
      "Epoch 48/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.6472 - loss: 1.9002 - val_accuracy: 0.5822 - val_loss: 2.0151 - learning_rate: 5.0000e-04\n",
      "Epoch 49/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.6644 - loss: 1.8608 - val_accuracy: 0.6166 - val_loss: 1.8885 - learning_rate: 5.0000e-04\n",
      "Epoch 50/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.6743 - loss: 1.8201 - val_accuracy: 0.5440 - val_loss: 2.1314 - learning_rate: 5.0000e-04\n",
      "Epoch 51/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.6898 - loss: 1.7592 - val_accuracy: 0.5717 - val_loss: 2.0667 - learning_rate: 5.0000e-04\n",
      "Epoch 52/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6747 - loss: 1.7698 - val_accuracy: 0.5883 - val_loss: 2.0186 - learning_rate: 5.0000e-04\n",
      "Epoch 53/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.7125 - loss: 1.7039 - val_accuracy: 0.5920 - val_loss: 1.9552 - learning_rate: 5.0000e-04\n",
      "Epoch 54/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6946 - loss: 1.6856 - val_accuracy: 0.5631 - val_loss: 2.0108 - learning_rate: 5.0000e-04\n",
      "Epoch 55/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.6792 - loss: 1.8223 - val_accuracy: 0.5908 - val_loss: 1.9659 - learning_rate: 5.0000e-04\n",
      "Epoch 56/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.6764 - loss: 1.7705 - val_accuracy: 0.6135 - val_loss: 1.9203 - learning_rate: 5.0000e-04\n",
      "Epoch 57/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6732 - loss: 1.7918 - val_accuracy: 0.5877 - val_loss: 1.9848 - learning_rate: 5.0000e-04\n",
      "Epoch 58/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.7191 - loss: 1.7124 - val_accuracy: 0.5705 - val_loss: 1.9722 - learning_rate: 5.0000e-04\n",
      "Epoch 59/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7215 - loss: 1.6838 - val_accuracy: 0.6092 - val_loss: 1.8739 - learning_rate: 5.0000e-04\n",
      "Epoch 60/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7216 - loss: 1.6979 - val_accuracy: 0.6135 - val_loss: 1.8830 - learning_rate: 5.0000e-04\n",
      "Epoch 61/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.7654 - loss: 1.5652 - val_accuracy: 0.6166 - val_loss: 1.8799 - learning_rate: 5.0000e-04\n",
      "Epoch 62/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6968 - loss: 1.7340 - val_accuracy: 0.5815 - val_loss: 1.9630 - learning_rate: 5.0000e-04\n",
      "Epoch 63/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.7077 - loss: 1.6752 - val_accuracy: 0.6135 - val_loss: 1.8521 - learning_rate: 5.0000e-04\n",
      "Epoch 64/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.7457 - loss: 1.5775 - val_accuracy: 0.5852 - val_loss: 1.9806 - learning_rate: 5.0000e-04\n",
      "Epoch 65/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.7392 - loss: 1.5770\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7391 - loss: 1.5772 - val_accuracy: 0.6018 - val_loss: 1.9533 - learning_rate: 5.0000e-04\n",
      "Epoch 66/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.7502 - loss: 1.6302 - val_accuracy: 0.6345 - val_loss: 1.8302 - learning_rate: 2.5000e-04\n",
      "Epoch 67/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7753 - loss: 1.5630 - val_accuracy: 0.6123 - val_loss: 1.8965 - learning_rate: 2.5000e-04\n",
      "Epoch 68/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.7612 - loss: 1.5766 - val_accuracy: 0.6209 - val_loss: 1.8507 - learning_rate: 2.5000e-04\n",
      "Epoch 69/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 131ms/step - accuracy: 0.7980 - loss: 1.4430 - val_accuracy: 0.6018 - val_loss: 1.9105 - learning_rate: 2.5000e-04\n",
      "Epoch 70/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7756 - loss: 1.5713 - val_accuracy: 0.6092 - val_loss: 1.8929 - learning_rate: 2.5000e-04\n",
      "Epoch 71/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.7523 - loss: 1.6477 - val_accuracy: 0.6111 - val_loss: 1.8838 - learning_rate: 2.5000e-04\n",
      "Epoch 72/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7474 - loss: 1.6227 - val_accuracy: 0.6283 - val_loss: 1.8255 - learning_rate: 2.5000e-04\n",
      "Epoch 73/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7559 - loss: 1.5902 - val_accuracy: 0.6283 - val_loss: 1.8332 - learning_rate: 2.5000e-04\n",
      "Epoch 74/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8089 - loss: 1.4833 - val_accuracy: 0.6062 - val_loss: 1.9006 - learning_rate: 2.5000e-04\n",
      "Epoch 75/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8017 - loss: 1.4922 - val_accuracy: 0.6037 - val_loss: 1.8820 - learning_rate: 2.5000e-04\n",
      "Epoch 76/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7888 - loss: 1.5393 - val_accuracy: 0.6080 - val_loss: 1.9229 - learning_rate: 2.5000e-04\n",
      "Epoch 77/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7677 - loss: 1.5839 - val_accuracy: 0.5969 - val_loss: 1.8813 - learning_rate: 2.5000e-04\n",
      "Epoch 78/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8105 - loss: 1.4754 - val_accuracy: 0.6209 - val_loss: 1.8546 - learning_rate: 2.5000e-04\n",
      "Epoch 79/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8430 - loss: 1.3868 - val_accuracy: 0.6062 - val_loss: 1.8858 - learning_rate: 2.5000e-04\n",
      "Epoch 80/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7805 - loss: 1.5385 - val_accuracy: 0.5908 - val_loss: 1.8835 - learning_rate: 2.5000e-04\n",
      "Epoch 81/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7828 - loss: 1.5672 - val_accuracy: 0.6246 - val_loss: 1.8219 - learning_rate: 2.5000e-04\n",
      "Epoch 82/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 127ms/step - accuracy: 0.7938 - loss: 1.4651 - val_accuracy: 0.6382 - val_loss: 1.7995 - learning_rate: 2.5000e-04\n",
      "Epoch 83/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7647 - loss: 1.5623 - val_accuracy: 0.6080 - val_loss: 1.8356 - learning_rate: 2.5000e-04\n",
      "Epoch 84/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.7925 - loss: 1.5101 - val_accuracy: 0.6375 - val_loss: 1.7869 - learning_rate: 2.5000e-04\n",
      "Epoch 85/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8070 - loss: 1.4970 - val_accuracy: 0.6178 - val_loss: 1.8463 - learning_rate: 2.5000e-04\n",
      "Epoch 86/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8299 - loss: 1.3977 - val_accuracy: 0.6148 - val_loss: 1.8724 - learning_rate: 2.5000e-04\n",
      "Epoch 87/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 127ms/step - accuracy: 0.8036 - loss: 1.4756 - val_accuracy: 0.5975 - val_loss: 1.9632 - learning_rate: 2.5000e-04\n",
      "Epoch 88/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8515 - loss: 1.3644 - val_accuracy: 0.6332 - val_loss: 1.8112 - learning_rate: 2.5000e-04\n",
      "Epoch 89/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8201 - loss: 1.4568 - val_accuracy: 0.5914 - val_loss: 1.9154 - learning_rate: 2.5000e-04\n",
      "Epoch 90/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 127ms/step - accuracy: 0.8259 - loss: 1.4468 - val_accuracy: 0.6031 - val_loss: 1.9478 - learning_rate: 2.5000e-04\n",
      "Epoch 91/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8518 - loss: 1.3297 - val_accuracy: 0.6000 - val_loss: 1.9431 - learning_rate: 2.5000e-04\n",
      "Epoch 92/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.8098 - loss: 1.4598 - val_accuracy: 0.6295 - val_loss: 1.7879 - learning_rate: 2.5000e-04\n",
      "Epoch 93/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7751 - loss: 1.5375 - val_accuracy: 0.6191 - val_loss: 1.8494 - learning_rate: 2.5000e-04\n",
      "Epoch 94/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.8265 - loss: 1.4301 - val_accuracy: 0.6080 - val_loss: 1.8487 - learning_rate: 2.5000e-04\n",
      "Epoch 95/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8382 - loss: 1.3833 - val_accuracy: 0.6437 - val_loss: 1.7607 - learning_rate: 2.5000e-04\n",
      "Epoch 96/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8641 - loss: 1.3293 - val_accuracy: 0.5902 - val_loss: 1.8972 - learning_rate: 2.5000e-04\n",
      "Epoch 97/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8463 - loss: 1.3827 - val_accuracy: 0.6000 - val_loss: 1.9017 - learning_rate: 2.5000e-04\n",
      "Epoch 98/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8015 - loss: 1.5054 - val_accuracy: 0.6332 - val_loss: 1.7748 - learning_rate: 2.5000e-04\n",
      "Epoch 99/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8144 - loss: 1.5045 - val_accuracy: 0.6000 - val_loss: 1.9047 - learning_rate: 2.5000e-04\n",
      "Epoch 100/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.8571 - loss: 1.3823 - val_accuracy: 0.6000 - val_loss: 1.9210 - learning_rate: 2.5000e-04\n",
      "Epoch 101/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8237 - loss: 1.4315 - val_accuracy: 0.6080 - val_loss: 1.8667 - learning_rate: 2.5000e-04\n",
      "Epoch 102/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8412 - loss: 1.3933 - val_accuracy: 0.6215 - val_loss: 1.8323 - learning_rate: 2.5000e-04\n",
      "Epoch 103/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8368 - loss: 1.4281 - val_accuracy: 0.6209 - val_loss: 1.9146 - learning_rate: 2.5000e-04\n",
      "Epoch 104/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8434 - loss: 1.3810 - val_accuracy: 0.5914 - val_loss: 1.8957 - learning_rate: 2.5000e-04\n",
      "Epoch 105/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.7994 - loss: 1.4820 - val_accuracy: 0.6222 - val_loss: 1.7716 - learning_rate: 2.5000e-04\n",
      "Epoch 106/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8392 - loss: 1.3336 - val_accuracy: 0.5982 - val_loss: 1.8838 - learning_rate: 2.5000e-04\n",
      "Epoch 107/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8157 - loss: 1.4231 - val_accuracy: 0.6160 - val_loss: 1.8417 - learning_rate: 2.5000e-04\n",
      "Epoch 108/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8486 - loss: 1.3675 - val_accuracy: 0.5871 - val_loss: 1.9260 - learning_rate: 2.5000e-04\n",
      "Epoch 109/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8072 - loss: 1.4500 - val_accuracy: 0.6062 - val_loss: 1.8794 - learning_rate: 2.5000e-04\n",
      "Epoch 110/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8661 - loss: 1.3365 - val_accuracy: 0.6258 - val_loss: 1.8149 - learning_rate: 2.5000e-04\n",
      "Epoch 111/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8079 - loss: 1.5012\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8082 - loss: 1.5002 - val_accuracy: 0.6092 - val_loss: 1.8928 - learning_rate: 2.5000e-04\n",
      "Epoch 112/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8462 - loss: 1.4004 - val_accuracy: 0.6135 - val_loss: 1.8704 - learning_rate: 1.2500e-04\n",
      "Epoch 113/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8280 - loss: 1.3908 - val_accuracy: 0.6265 - val_loss: 1.7977 - learning_rate: 1.2500e-04\n",
      "Epoch 114/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8828 - loss: 1.2865 - val_accuracy: 0.6252 - val_loss: 1.8639 - learning_rate: 1.2500e-04\n",
      "Epoch 115/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8789 - loss: 1.3291 - val_accuracy: 0.6228 - val_loss: 1.8476 - learning_rate: 1.2500e-04\n",
      "Epoch 116/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8524 - loss: 1.3751 - val_accuracy: 0.6142 - val_loss: 1.8407 - learning_rate: 1.2500e-04\n",
      "Epoch 117/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8797 - loss: 1.2824 - val_accuracy: 0.6006 - val_loss: 1.9463 - learning_rate: 1.2500e-04\n",
      "Epoch 118/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8616 - loss: 1.3614 - val_accuracy: 0.6283 - val_loss: 1.8434 - learning_rate: 1.2500e-04\n",
      "Epoch 119/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8734 - loss: 1.2849 - val_accuracy: 0.6215 - val_loss: 1.9024 - learning_rate: 1.2500e-04\n",
      "Epoch 120/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.9006 - loss: 1.2732 - val_accuracy: 0.6246 - val_loss: 1.8635 - learning_rate: 1.2500e-04\n",
      "Epoch 121/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8550 - loss: 1.3275 - val_accuracy: 0.6086 - val_loss: 1.8646 - learning_rate: 1.2500e-04\n",
      "Epoch 122/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.9019 - loss: 1.2411 - val_accuracy: 0.6252 - val_loss: 1.8659 - learning_rate: 1.2500e-04\n",
      "Epoch 123/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8477 - loss: 1.3706 - val_accuracy: 0.6234 - val_loss: 1.8743 - learning_rate: 1.2500e-04\n",
      "Epoch 124/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8553 - loss: 1.3909 - val_accuracy: 0.6295 - val_loss: 1.8322 - learning_rate: 1.2500e-04\n",
      "Epoch 125/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8933 - loss: 1.2626 - val_accuracy: 0.6000 - val_loss: 1.9223 - learning_rate: 1.2500e-04\n",
      "Epoch 126/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8508 - loss: 1.3434 - val_accuracy: 0.6283 - val_loss: 1.8209 - learning_rate: 1.2500e-04\n",
      "Epoch 127/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.8749 - loss: 1.3104 - val_accuracy: 0.6006 - val_loss: 1.8450 - learning_rate: 1.2500e-04\n",
      "Epoch 128/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8698 - loss: 1.2321\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8699 - loss: 1.2319 - val_accuracy: 0.6209 - val_loss: 1.8378 - learning_rate: 1.2500e-04\n",
      "Epoch 129/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 127ms/step - accuracy: 0.8476 - loss: 1.3465 - val_accuracy: 0.6142 - val_loss: 1.8277 - learning_rate: 6.2500e-05\n",
      "Epoch 130/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8516 - loss: 1.3874 - val_accuracy: 0.6314 - val_loss: 1.7976 - learning_rate: 6.2500e-05\n",
      "Epoch 131/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.8584 - loss: 1.2890 - val_accuracy: 0.6197 - val_loss: 1.8457 - learning_rate: 6.2500e-05\n",
      "Epoch 132/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8729 - loss: 1.3257 - val_accuracy: 0.6098 - val_loss: 1.9185 - learning_rate: 6.2500e-05\n",
      "Epoch 133/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 127ms/step - accuracy: 0.8809 - loss: 1.3074 - val_accuracy: 0.6037 - val_loss: 1.8917 - learning_rate: 6.2500e-05\n",
      "Epoch 134/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.8859 - loss: 1.3109 - val_accuracy: 0.6265 - val_loss: 1.8315 - learning_rate: 6.2500e-05\n",
      "Epoch 135/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.8591 - loss: 1.3201 - val_accuracy: 0.6277 - val_loss: 1.8109 - learning_rate: 6.2500e-05\n",
      "Epoch 135: early stopping\n",
      "Restoring model weights from the end of the best epoch: 95.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2025-08-09 09:31:48.439299: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step\n",
      "\n",
      "===== FOLD 2/5 =====\n",
      "Epoch 1/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 138ms/step - accuracy: 0.0689 - loss: 4.4614 - val_accuracy: 0.0784 - val_loss: 4.1413 - learning_rate: 5.0000e-04\n",
      "Epoch 2/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.1685 - loss: 3.7231 - val_accuracy: 0.0925 - val_loss: 4.0718 - learning_rate: 5.0000e-04\n",
      "Epoch 3/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.2051 - loss: 3.4867 - val_accuracy: 0.1740 - val_loss: 3.4552 - learning_rate: 5.0000e-04\n",
      "Epoch 4/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 127ms/step - accuracy: 0.2637 - loss: 3.2711 - val_accuracy: 0.3009 - val_loss: 3.0267 - learning_rate: 5.0000e-04\n",
      "Epoch 5/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 127ms/step - accuracy: 0.2901 - loss: 3.1366 - val_accuracy: 0.3805 - val_loss: 2.8371 - learning_rate: 5.0000e-04\n",
      "Epoch 6/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.3402 - loss: 2.9623 - val_accuracy: 0.4032 - val_loss: 2.7389 - learning_rate: 5.0000e-04\n",
      "Epoch 7/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.3554 - loss: 2.9183 - val_accuracy: 0.4350 - val_loss: 2.6878 - learning_rate: 5.0000e-04\n",
      "Epoch 8/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.3601 - loss: 2.9406 - val_accuracy: 0.4277 - val_loss: 2.5922 - learning_rate: 5.0000e-04\n",
      "Epoch 9/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.3867 - loss: 2.8152 - val_accuracy: 0.4559 - val_loss: 2.6334 - learning_rate: 5.0000e-04\n",
      "Epoch 10/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.4055 - loss: 2.6768 - val_accuracy: 0.4479 - val_loss: 2.6189 - learning_rate: 5.0000e-04\n",
      "Epoch 11/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 127ms/step - accuracy: 0.4200 - loss: 2.7318 - val_accuracy: 0.4614 - val_loss: 2.5867 - learning_rate: 5.0000e-04\n",
      "Epoch 12/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.4381 - loss: 2.6144 - val_accuracy: 0.4755 - val_loss: 2.5925 - learning_rate: 5.0000e-04\n",
      "Epoch 13/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.4585 - loss: 2.5802 - val_accuracy: 0.4859 - val_loss: 2.5232 - learning_rate: 5.0000e-04\n",
      "Epoch 14/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.4488 - loss: 2.6656 - val_accuracy: 0.5092 - val_loss: 2.4670 - learning_rate: 5.0000e-04\n",
      "Epoch 15/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.4642 - loss: 2.5122 - val_accuracy: 0.5043 - val_loss: 2.3657 - learning_rate: 5.0000e-04\n",
      "Epoch 16/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 127ms/step - accuracy: 0.4726 - loss: 2.4260 - val_accuracy: 0.4945 - val_loss: 2.4118 - learning_rate: 5.0000e-04\n",
      "Epoch 17/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.4968 - loss: 2.3923 - val_accuracy: 0.4706 - val_loss: 2.5011 - learning_rate: 5.0000e-04\n",
      "Epoch 18/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.4811 - loss: 2.4689 - val_accuracy: 0.5031 - val_loss: 2.3573 - learning_rate: 5.0000e-04\n",
      "Epoch 19/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.5199 - loss: 2.3811 - val_accuracy: 0.5104 - val_loss: 2.3315 - learning_rate: 5.0000e-04\n",
      "Epoch 20/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.4970 - loss: 2.3276 - val_accuracy: 0.5165 - val_loss: 2.3166 - learning_rate: 5.0000e-04\n",
      "Epoch 21/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.5190 - loss: 2.3351 - val_accuracy: 0.5362 - val_loss: 2.2564 - learning_rate: 5.0000e-04\n",
      "Epoch 22/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.5132 - loss: 2.3917 - val_accuracy: 0.5294 - val_loss: 2.2420 - learning_rate: 5.0000e-04\n",
      "Epoch 23/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.5265 - loss: 2.2972 - val_accuracy: 0.5049 - val_loss: 2.3365 - learning_rate: 5.0000e-04\n",
      "Epoch 24/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.5456 - loss: 2.2287 - val_accuracy: 0.5380 - val_loss: 2.2776 - learning_rate: 5.0000e-04\n",
      "Epoch 25/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.5494 - loss: 2.1871 - val_accuracy: 0.5306 - val_loss: 2.2423 - learning_rate: 5.0000e-04\n",
      "Epoch 26/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.5468 - loss: 2.2232 - val_accuracy: 0.5441 - val_loss: 2.2283 - learning_rate: 5.0000e-04\n",
      "Epoch 27/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.5513 - loss: 2.2142 - val_accuracy: 0.5233 - val_loss: 2.2391 - learning_rate: 5.0000e-04\n",
      "Epoch 28/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.5644 - loss: 2.1697 - val_accuracy: 0.5417 - val_loss: 2.2134 - learning_rate: 5.0000e-04\n",
      "Epoch 29/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.5830 - loss: 2.1359 - val_accuracy: 0.5515 - val_loss: 2.1637 - learning_rate: 5.0000e-04\n",
      "Epoch 30/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.5741 - loss: 2.1311 - val_accuracy: 0.5300 - val_loss: 2.1903 - learning_rate: 5.0000e-04\n",
      "Epoch 31/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5729 - loss: 2.1390 - val_accuracy: 0.5484 - val_loss: 2.1733 - learning_rate: 5.0000e-04\n",
      "Epoch 32/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.5840 - loss: 2.0689 - val_accuracy: 0.5337 - val_loss: 2.1799 - learning_rate: 5.0000e-04\n",
      "Epoch 33/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5622 - loss: 2.1687 - val_accuracy: 0.5429 - val_loss: 2.0983 - learning_rate: 5.0000e-04\n",
      "Epoch 34/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5968 - loss: 2.0219 - val_accuracy: 0.5245 - val_loss: 2.2215 - learning_rate: 5.0000e-04\n",
      "Epoch 35/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5966 - loss: 2.0491 - val_accuracy: 0.5496 - val_loss: 2.1705 - learning_rate: 5.0000e-04\n",
      "Epoch 36/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6312 - loss: 1.9380 - val_accuracy: 0.5839 - val_loss: 2.0178 - learning_rate: 5.0000e-04\n",
      "Epoch 37/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.6160 - loss: 1.9859 - val_accuracy: 0.5484 - val_loss: 2.1595 - learning_rate: 5.0000e-04\n",
      "Epoch 38/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5953 - loss: 2.0530 - val_accuracy: 0.5656 - val_loss: 2.1003 - learning_rate: 5.0000e-04\n",
      "Epoch 39/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5993 - loss: 2.0359 - val_accuracy: 0.5539 - val_loss: 2.1263 - learning_rate: 5.0000e-04\n",
      "Epoch 40/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.6326 - loss: 1.8659 - val_accuracy: 0.5650 - val_loss: 2.0389 - learning_rate: 5.0000e-04\n",
      "Epoch 41/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6210 - loss: 1.9482 - val_accuracy: 0.5772 - val_loss: 1.9963 - learning_rate: 5.0000e-04\n",
      "Epoch 42/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.6080 - loss: 2.0167 - val_accuracy: 0.5833 - val_loss: 1.9863 - learning_rate: 5.0000e-04\n",
      "Epoch 43/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.6480 - loss: 1.8994 - val_accuracy: 0.5717 - val_loss: 2.0760 - learning_rate: 5.0000e-04\n",
      "Epoch 44/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6837 - loss: 1.7828 - val_accuracy: 0.5895 - val_loss: 1.9878 - learning_rate: 5.0000e-04\n",
      "Epoch 45/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.6232 - loss: 1.9353 - val_accuracy: 0.5839 - val_loss: 2.0112 - learning_rate: 5.0000e-04\n",
      "Epoch 46/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6499 - loss: 1.8117 - val_accuracy: 0.5950 - val_loss: 1.9568 - learning_rate: 5.0000e-04\n",
      "Epoch 47/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6799 - loss: 1.8028 - val_accuracy: 0.5588 - val_loss: 2.0601 - learning_rate: 5.0000e-04\n",
      "Epoch 48/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6696 - loss: 1.7873 - val_accuracy: 0.5362 - val_loss: 2.0600 - learning_rate: 5.0000e-04\n",
      "Epoch 49/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6820 - loss: 1.7632 - val_accuracy: 0.5766 - val_loss: 1.9751 - learning_rate: 5.0000e-04\n",
      "Epoch 50/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.6775 - loss: 1.7971 - val_accuracy: 0.5699 - val_loss: 1.9778 - learning_rate: 5.0000e-04\n",
      "Epoch 51/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.6773 - loss: 1.7794 - val_accuracy: 0.6066 - val_loss: 1.8992 - learning_rate: 5.0000e-04\n",
      "Epoch 52/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6901 - loss: 1.7773 - val_accuracy: 0.5944 - val_loss: 1.9516 - learning_rate: 5.0000e-04\n",
      "Epoch 53/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.6680 - loss: 1.7718 - val_accuracy: 0.5901 - val_loss: 1.9687 - learning_rate: 5.0000e-04\n",
      "Epoch 54/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6816 - loss: 1.7594 - val_accuracy: 0.5576 - val_loss: 2.1024 - learning_rate: 5.0000e-04\n",
      "Epoch 55/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6898 - loss: 1.7617 - val_accuracy: 0.5741 - val_loss: 1.9846 - learning_rate: 5.0000e-04\n",
      "Epoch 56/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6954 - loss: 1.7304 - val_accuracy: 0.5564 - val_loss: 2.0457 - learning_rate: 5.0000e-04\n",
      "Epoch 57/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6856 - loss: 1.8245 - val_accuracy: 0.5821 - val_loss: 1.9405 - learning_rate: 5.0000e-04\n",
      "Epoch 58/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7119 - loss: 1.7014 - val_accuracy: 0.6152 - val_loss: 1.8458 - learning_rate: 5.0000e-04\n",
      "Epoch 59/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7298 - loss: 1.6113 - val_accuracy: 0.5987 - val_loss: 1.9066 - learning_rate: 5.0000e-04\n",
      "Epoch 60/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7040 - loss: 1.6698 - val_accuracy: 0.6017 - val_loss: 1.9432 - learning_rate: 5.0000e-04\n",
      "Epoch 61/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7188 - loss: 1.6674 - val_accuracy: 0.5766 - val_loss: 1.9859 - learning_rate: 5.0000e-04\n",
      "Epoch 62/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7036 - loss: 1.6865 - val_accuracy: 0.5852 - val_loss: 1.9724 - learning_rate: 5.0000e-04\n",
      "Epoch 63/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.6870 - loss: 1.6999 - val_accuracy: 0.5980 - val_loss: 1.8795 - learning_rate: 5.0000e-04\n",
      "Epoch 64/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7657 - loss: 1.5475 - val_accuracy: 0.5821 - val_loss: 1.9665 - learning_rate: 5.0000e-04\n",
      "Epoch 65/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7361 - loss: 1.6481 - val_accuracy: 0.6042 - val_loss: 1.8669 - learning_rate: 5.0000e-04\n",
      "Epoch 66/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7193 - loss: 1.6314 - val_accuracy: 0.5717 - val_loss: 1.9141 - learning_rate: 5.0000e-04\n",
      "Epoch 67/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7161 - loss: 1.6426 - val_accuracy: 0.5876 - val_loss: 1.9566 - learning_rate: 5.0000e-04\n",
      "Epoch 68/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7702 - loss: 1.5148 - val_accuracy: 0.5705 - val_loss: 1.9937 - learning_rate: 5.0000e-04\n",
      "Epoch 69/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.7473 - loss: 1.5693 - val_accuracy: 0.5864 - val_loss: 1.9573 - learning_rate: 5.0000e-04\n",
      "Epoch 70/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7867 - loss: 1.5325 - val_accuracy: 0.6023 - val_loss: 1.9440 - learning_rate: 5.0000e-04\n",
      "Epoch 71/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7671 - loss: 1.5426 - val_accuracy: 0.6023 - val_loss: 1.9352 - learning_rate: 5.0000e-04\n",
      "Epoch 72/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7799 - loss: 1.4927 - val_accuracy: 0.5692 - val_loss: 1.9716 - learning_rate: 5.0000e-04\n",
      "Epoch 73/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7589 - loss: 1.5881 - val_accuracy: 0.6005 - val_loss: 1.9556 - learning_rate: 5.0000e-04\n",
      "Epoch 74/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.7507 - loss: 1.5586\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7506 - loss: 1.5591 - val_accuracy: 0.5925 - val_loss: 1.9047 - learning_rate: 5.0000e-04\n",
      "Epoch 75/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7956 - loss: 1.4813 - val_accuracy: 0.5980 - val_loss: 1.8899 - learning_rate: 2.5000e-04\n",
      "Epoch 76/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7845 - loss: 1.5088 - val_accuracy: 0.6256 - val_loss: 1.8263 - learning_rate: 2.5000e-04\n",
      "Epoch 77/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7670 - loss: 1.5872 - val_accuracy: 0.6152 - val_loss: 1.8658 - learning_rate: 2.5000e-04\n",
      "Epoch 78/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8231 - loss: 1.4857 - val_accuracy: 0.6146 - val_loss: 1.8678 - learning_rate: 2.5000e-04\n",
      "Epoch 79/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8402 - loss: 1.3637 - val_accuracy: 0.5876 - val_loss: 2.0138 - learning_rate: 2.5000e-04\n",
      "Epoch 80/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8290 - loss: 1.4166 - val_accuracy: 0.6183 - val_loss: 1.8297 - learning_rate: 2.5000e-04\n",
      "Epoch 81/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7826 - loss: 1.5646 - val_accuracy: 0.6158 - val_loss: 1.8878 - learning_rate: 2.5000e-04\n",
      "Epoch 82/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8304 - loss: 1.4222 - val_accuracy: 0.6072 - val_loss: 1.8431 - learning_rate: 2.5000e-04\n",
      "Epoch 83/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8054 - loss: 1.4803 - val_accuracy: 0.6275 - val_loss: 1.7823 - learning_rate: 2.5000e-04\n",
      "Epoch 84/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8019 - loss: 1.5198 - val_accuracy: 0.6109 - val_loss: 1.8466 - learning_rate: 2.5000e-04\n",
      "Epoch 85/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7998 - loss: 1.5095 - val_accuracy: 0.6054 - val_loss: 1.8713 - learning_rate: 2.5000e-04\n",
      "Epoch 86/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7946 - loss: 1.4798 - val_accuracy: 0.6103 - val_loss: 1.8664 - learning_rate: 2.5000e-04\n",
      "Epoch 87/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8209 - loss: 1.4273 - val_accuracy: 0.6011 - val_loss: 1.8821 - learning_rate: 2.5000e-04\n",
      "Epoch 88/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8020 - loss: 1.5278 - val_accuracy: 0.6011 - val_loss: 1.9082 - learning_rate: 2.5000e-04\n",
      "Epoch 89/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8193 - loss: 1.4285 - val_accuracy: 0.5987 - val_loss: 1.9295 - learning_rate: 2.5000e-04\n",
      "Epoch 90/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8052 - loss: 1.4588 - val_accuracy: 0.5797 - val_loss: 1.9393 - learning_rate: 2.5000e-04\n",
      "Epoch 91/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7924 - loss: 1.5124 - val_accuracy: 0.5938 - val_loss: 1.9163 - learning_rate: 2.5000e-04\n",
      "Epoch 92/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8142 - loss: 1.4393 - val_accuracy: 0.6072 - val_loss: 1.8935 - learning_rate: 2.5000e-04\n",
      "Epoch 93/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8270 - loss: 1.4220 - val_accuracy: 0.6042 - val_loss: 1.8821 - learning_rate: 2.5000e-04\n",
      "Epoch 94/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8673 - loss: 1.3081 - val_accuracy: 0.5956 - val_loss: 1.8630 - learning_rate: 2.5000e-04\n",
      "Epoch 95/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8477 - loss: 1.4373 - val_accuracy: 0.6213 - val_loss: 1.8628 - learning_rate: 2.5000e-04\n",
      "Epoch 96/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8119 - loss: 1.4355 - val_accuracy: 0.5938 - val_loss: 1.9371 - learning_rate: 2.5000e-04\n",
      "Epoch 97/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7876 - loss: 1.5440 - val_accuracy: 0.6268 - val_loss: 1.8070 - learning_rate: 2.5000e-04\n",
      "Epoch 98/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8189 - loss: 1.4790 - val_accuracy: 0.6042 - val_loss: 1.8303 - learning_rate: 2.5000e-04\n",
      "Epoch 99/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8103 - loss: 1.4688\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8103 - loss: 1.4687 - val_accuracy: 0.5717 - val_loss: 1.9472 - learning_rate: 2.5000e-04\n",
      "Epoch 100/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8147 - loss: 1.4071 - val_accuracy: 0.6164 - val_loss: 1.8538 - learning_rate: 1.2500e-04\n",
      "Epoch 101/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8176 - loss: 1.4132 - val_accuracy: 0.6189 - val_loss: 1.7937 - learning_rate: 1.2500e-04\n",
      "Epoch 102/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8310 - loss: 1.3534 - val_accuracy: 0.6060 - val_loss: 1.8590 - learning_rate: 1.2500e-04\n",
      "Epoch 103/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8668 - loss: 1.3384 - val_accuracy: 0.6311 - val_loss: 1.8207 - learning_rate: 1.2500e-04\n",
      "Epoch 104/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8513 - loss: 1.3577 - val_accuracy: 0.6232 - val_loss: 1.8662 - learning_rate: 1.2500e-04\n",
      "Epoch 105/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8500 - loss: 1.3855 - val_accuracy: 0.6042 - val_loss: 1.8893 - learning_rate: 1.2500e-04\n",
      "Epoch 106/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8211 - loss: 1.4822 - val_accuracy: 0.6036 - val_loss: 1.8686 - learning_rate: 1.2500e-04\n",
      "Epoch 107/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8335 - loss: 1.3801 - val_accuracy: 0.6201 - val_loss: 1.8795 - learning_rate: 1.2500e-04\n",
      "Epoch 108/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8268 - loss: 1.4338 - val_accuracy: 0.6029 - val_loss: 1.8309 - learning_rate: 1.2500e-04\n",
      "Epoch 109/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8475 - loss: 1.3939 - val_accuracy: 0.6140 - val_loss: 1.8207 - learning_rate: 1.2500e-04\n",
      "Epoch 110/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8534 - loss: 1.3750 - val_accuracy: 0.6170 - val_loss: 1.8897 - learning_rate: 1.2500e-04\n",
      "Epoch 111/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8310 - loss: 1.4442 - val_accuracy: 0.6213 - val_loss: 1.8343 - learning_rate: 1.2500e-04\n",
      "Epoch 112/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8575 - loss: 1.3279 - val_accuracy: 0.6397 - val_loss: 1.8047 - learning_rate: 1.2500e-04\n",
      "Epoch 113/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8970 - loss: 1.2734 - val_accuracy: 0.6134 - val_loss: 1.9049 - learning_rate: 1.2500e-04\n",
      "Epoch 114/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8573 - loss: 1.3876 - val_accuracy: 0.5925 - val_loss: 1.9414 - learning_rate: 1.2500e-04\n",
      "Epoch 115/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8356 - loss: 1.3569 - val_accuracy: 0.5919 - val_loss: 1.9414 - learning_rate: 1.2500e-04\n",
      "Epoch 116/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8863 - loss: 1.2281 - val_accuracy: 0.6330 - val_loss: 1.7967 - learning_rate: 1.2500e-04\n",
      "Epoch 117/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8166 - loss: 1.4440 - val_accuracy: 0.6152 - val_loss: 1.8383 - learning_rate: 1.2500e-04\n",
      "Epoch 118/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8206 - loss: 1.4384 - val_accuracy: 0.5980 - val_loss: 1.8970 - learning_rate: 1.2500e-04\n",
      "Epoch 119/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 126ms/step - accuracy: 0.8695 - loss: 1.3018 - val_accuracy: 0.6134 - val_loss: 1.9150 - learning_rate: 1.2500e-04\n",
      "Epoch 120/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8495 - loss: 1.3713 - val_accuracy: 0.6097 - val_loss: 1.8396 - learning_rate: 1.2500e-04\n",
      "Epoch 121/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8219 - loss: 1.4716 - val_accuracy: 0.6036 - val_loss: 1.9448 - learning_rate: 1.2500e-04\n",
      "Epoch 122/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8438 - loss: 1.3984 - val_accuracy: 0.6183 - val_loss: 1.7905 - learning_rate: 1.2500e-04\n",
      "Epoch 123/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8733 - loss: 1.3169 - val_accuracy: 0.5919 - val_loss: 1.8857 - learning_rate: 1.2500e-04\n",
      "Epoch 124/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.7887 - loss: 1.5566 - val_accuracy: 0.6121 - val_loss: 1.8640 - learning_rate: 1.2500e-04\n",
      "Epoch 125/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8728 - loss: 1.3314 - val_accuracy: 0.6379 - val_loss: 1.8240 - learning_rate: 1.2500e-04\n",
      "Epoch 126/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8547 - loss: 1.3290 - val_accuracy: 0.6422 - val_loss: 1.7489 - learning_rate: 1.2500e-04\n",
      "Epoch 127/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8294 - loss: 1.4367 - val_accuracy: 0.6281 - val_loss: 1.8036 - learning_rate: 1.2500e-04\n",
      "Epoch 128/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8548 - loss: 1.3762 - val_accuracy: 0.6072 - val_loss: 1.8542 - learning_rate: 1.2500e-04\n",
      "Epoch 129/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8659 - loss: 1.3222 - val_accuracy: 0.6109 - val_loss: 1.8869 - learning_rate: 1.2500e-04\n",
      "Epoch 130/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8384 - loss: 1.4233 - val_accuracy: 0.6140 - val_loss: 1.8630 - learning_rate: 1.2500e-04\n",
      "Epoch 131/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8786 - loss: 1.3105 - val_accuracy: 0.6011 - val_loss: 1.8548 - learning_rate: 1.2500e-04\n",
      "Epoch 132/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8396 - loss: 1.3905 - val_accuracy: 0.6293 - val_loss: 1.8490 - learning_rate: 1.2500e-04\n",
      "Epoch 133/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8637 - loss: 1.3957 - val_accuracy: 0.6324 - val_loss: 1.7831 - learning_rate: 1.2500e-04\n",
      "Epoch 134/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8566 - loss: 1.3490 - val_accuracy: 0.5938 - val_loss: 1.8652 - learning_rate: 1.2500e-04\n",
      "Epoch 135/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8801 - loss: 1.2611 - val_accuracy: 0.6219 - val_loss: 1.8100 - learning_rate: 1.2500e-04\n",
      "Epoch 136/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8864 - loss: 1.2964 - val_accuracy: 0.6373 - val_loss: 1.7998 - learning_rate: 1.2500e-04\n",
      "Epoch 137/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8934 - loss: 1.2584 - val_accuracy: 0.6146 - val_loss: 1.8303 - learning_rate: 1.2500e-04\n",
      "Epoch 138/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8716 - loss: 1.2944 - val_accuracy: 0.6330 - val_loss: 1.8165 - learning_rate: 1.2500e-04\n",
      "Epoch 139/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.9144 - loss: 1.2128 - val_accuracy: 0.6164 - val_loss: 1.8580 - learning_rate: 1.2500e-04\n",
      "Epoch 140/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8215 - loss: 1.3994 - val_accuracy: 0.6029 - val_loss: 1.9264 - learning_rate: 1.2500e-04\n",
      "Epoch 141/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8251 - loss: 1.3656 - val_accuracy: 0.6256 - val_loss: 1.8232 - learning_rate: 1.2500e-04\n",
      "Epoch 142/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8772 - loss: 1.2816 - val_accuracy: 0.6501 - val_loss: 1.7263 - learning_rate: 1.2500e-04\n",
      "Epoch 143/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8437 - loss: 1.3634 - val_accuracy: 0.6140 - val_loss: 1.9069 - learning_rate: 1.2500e-04\n",
      "Epoch 144/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8587 - loss: 1.3413 - val_accuracy: 0.6140 - val_loss: 1.8801 - learning_rate: 1.2500e-04\n",
      "Epoch 145/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8525 - loss: 1.3417 - val_accuracy: 0.6127 - val_loss: 1.8585 - learning_rate: 1.2500e-04\n",
      "Epoch 146/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8868 - loss: 1.2585 - val_accuracy: 0.6140 - val_loss: 1.8947 - learning_rate: 1.2500e-04\n",
      "Epoch 147/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8788 - loss: 1.2911 - val_accuracy: 0.6127 - val_loss: 1.8405 - learning_rate: 1.2500e-04\n",
      "Epoch 148/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8291 - loss: 1.4418 - val_accuracy: 0.6140 - val_loss: 1.8308 - learning_rate: 1.2500e-04\n",
      "Epoch 149/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8850 - loss: 1.2592 - val_accuracy: 0.5974 - val_loss: 1.9022 - learning_rate: 1.2500e-04\n",
      "Epoch 150/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8528 - loss: 1.3397 - val_accuracy: 0.6078 - val_loss: 1.8592 - learning_rate: 1.2500e-04\n",
      "Epoch 151/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8760 - loss: 1.3149 - val_accuracy: 0.6305 - val_loss: 1.8317 - learning_rate: 1.2500e-04\n",
      "Epoch 152/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8628 - loss: 1.3225 - val_accuracy: 0.6311 - val_loss: 1.7846 - learning_rate: 1.2500e-04\n",
      "Epoch 153/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 127ms/step - accuracy: 0.8823 - loss: 1.2846 - val_accuracy: 0.6238 - val_loss: 1.8460 - learning_rate: 1.2500e-04\n",
      "Epoch 154/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.8567 - loss: 1.3092 - val_accuracy: 0.6054 - val_loss: 1.8532 - learning_rate: 1.2500e-04\n",
      "Epoch 155/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.9162 - loss: 1.1803 - val_accuracy: 0.6201 - val_loss: 1.7875 - learning_rate: 1.2500e-04\n",
      "Epoch 156/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 127ms/step - accuracy: 0.8417 - loss: 1.3829 - val_accuracy: 0.5950 - val_loss: 1.9108 - learning_rate: 1.2500e-04\n",
      "Epoch 157/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.8451 - loss: 1.3765 - val_accuracy: 0.5754 - val_loss: 1.9919 - learning_rate: 1.2500e-04\n",
      "Epoch 158/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9118 - loss: 1.1909\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 127ms/step - accuracy: 0.9115 - loss: 1.1918 - val_accuracy: 0.6317 - val_loss: 1.7684 - learning_rate: 1.2500e-04\n",
      "Epoch 159/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 127ms/step - accuracy: 0.8757 - loss: 1.2745 - val_accuracy: 0.6072 - val_loss: 1.8397 - learning_rate: 6.2500e-05\n",
      "Epoch 160/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 127ms/step - accuracy: 0.8800 - loss: 1.3203 - val_accuracy: 0.6238 - val_loss: 1.8163 - learning_rate: 6.2500e-05\n",
      "Restoring model weights from the end of the best epoch: 142.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\n",
      "===== FOLD 3/5 =====\n",
      "Epoch 1/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 132ms/step - accuracy: 0.0929 - loss: 4.3076 - val_accuracy: 0.0808 - val_loss: 4.1968 - learning_rate: 5.0000e-04\n",
      "Epoch 2/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.1691 - loss: 3.6800 - val_accuracy: 0.0915 - val_loss: 4.4070 - learning_rate: 5.0000e-04\n",
      "Epoch 3/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.2121 - loss: 3.4034 - val_accuracy: 0.1654 - val_loss: 3.6724 - learning_rate: 5.0000e-04\n",
      "Epoch 4/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.2773 - loss: 3.1930 - val_accuracy: 0.3390 - val_loss: 2.9556 - learning_rate: 5.0000e-04\n",
      "Epoch 5/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.3160 - loss: 3.0333 - val_accuracy: 0.3958 - val_loss: 2.8633 - learning_rate: 5.0000e-04\n",
      "Epoch 6/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.3484 - loss: 2.9092 - val_accuracy: 0.4293 - val_loss: 2.7135 - learning_rate: 5.0000e-04\n",
      "Epoch 7/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.3857 - loss: 2.7626 - val_accuracy: 0.4324 - val_loss: 2.6743 - learning_rate: 5.0000e-04\n",
      "Epoch 8/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.3648 - loss: 2.8652 - val_accuracy: 0.4697 - val_loss: 2.5510 - learning_rate: 5.0000e-04\n",
      "Epoch 9/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.3849 - loss: 2.7672 - val_accuracy: 0.4716 - val_loss: 2.5457 - learning_rate: 5.0000e-04\n",
      "Epoch 10/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.4153 - loss: 2.7097 - val_accuracy: 0.4975 - val_loss: 2.4320 - learning_rate: 5.0000e-04\n",
      "Epoch 11/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.4177 - loss: 2.6606 - val_accuracy: 0.4615 - val_loss: 2.5618 - learning_rate: 5.0000e-04\n",
      "Epoch 12/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.4523 - loss: 2.5678 - val_accuracy: 0.4665 - val_loss: 2.5743 - learning_rate: 5.0000e-04\n",
      "Epoch 13/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.4473 - loss: 2.5601 - val_accuracy: 0.5032 - val_loss: 2.4198 - learning_rate: 5.0000e-04\n",
      "Epoch 14/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.4695 - loss: 2.5139 - val_accuracy: 0.4735 - val_loss: 2.5122 - learning_rate: 5.0000e-04\n",
      "Epoch 15/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.4911 - loss: 2.4142 - val_accuracy: 0.5051 - val_loss: 2.4583 - learning_rate: 5.0000e-04\n",
      "Epoch 16/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.4586 - loss: 2.5543 - val_accuracy: 0.5309 - val_loss: 2.3189 - learning_rate: 5.0000e-04\n",
      "Epoch 17/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.4944 - loss: 2.4325 - val_accuracy: 0.5360 - val_loss: 2.3360 - learning_rate: 5.0000e-04\n",
      "Epoch 18/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5158 - loss: 2.3882 - val_accuracy: 0.5101 - val_loss: 2.3657 - learning_rate: 5.0000e-04\n",
      "Epoch 19/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5312 - loss: 2.2935 - val_accuracy: 0.5448 - val_loss: 2.3414 - learning_rate: 5.0000e-04\n",
      "Epoch 20/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.5251 - loss: 2.3050 - val_accuracy: 0.5360 - val_loss: 2.3127 - learning_rate: 5.0000e-04\n",
      "Epoch 21/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5360 - loss: 2.2637 - val_accuracy: 0.5328 - val_loss: 2.3430 - learning_rate: 5.0000e-04\n",
      "Epoch 22/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5365 - loss: 2.3632 - val_accuracy: 0.5429 - val_loss: 2.2645 - learning_rate: 5.0000e-04\n",
      "Epoch 23/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5553 - loss: 2.2688 - val_accuracy: 0.5051 - val_loss: 2.3197 - learning_rate: 5.0000e-04\n",
      "Epoch 24/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.5223 - loss: 2.3209 - val_accuracy: 0.5518 - val_loss: 2.2063 - learning_rate: 5.0000e-04\n",
      "Epoch 25/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5168 - loss: 2.3255 - val_accuracy: 0.5492 - val_loss: 2.2446 - learning_rate: 5.0000e-04\n",
      "Epoch 26/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5589 - loss: 2.1929 - val_accuracy: 0.5625 - val_loss: 2.1774 - learning_rate: 5.0000e-04\n",
      "Epoch 27/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5595 - loss: 2.1375 - val_accuracy: 0.5619 - val_loss: 2.2238 - learning_rate: 5.0000e-04\n",
      "Epoch 28/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5620 - loss: 2.1512 - val_accuracy: 0.5972 - val_loss: 2.0972 - learning_rate: 5.0000e-04\n",
      "Epoch 29/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5902 - loss: 2.0241 - val_accuracy: 0.5808 - val_loss: 2.0998 - learning_rate: 5.0000e-04\n",
      "Epoch 30/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5963 - loss: 2.0421 - val_accuracy: 0.5966 - val_loss: 2.0149 - learning_rate: 5.0000e-04\n",
      "Epoch 31/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.6005 - loss: 2.0318 - val_accuracy: 0.5732 - val_loss: 2.1434 - learning_rate: 5.0000e-04\n",
      "Epoch 32/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5825 - loss: 2.0974 - val_accuracy: 0.5821 - val_loss: 2.0768 - learning_rate: 5.0000e-04\n",
      "Epoch 33/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5876 - loss: 2.0577 - val_accuracy: 0.6212 - val_loss: 1.9760 - learning_rate: 5.0000e-04\n",
      "Epoch 34/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6120 - loss: 2.0222 - val_accuracy: 0.5600 - val_loss: 2.1136 - learning_rate: 5.0000e-04\n",
      "Epoch 35/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5970 - loss: 2.0187 - val_accuracy: 0.5631 - val_loss: 2.0647 - learning_rate: 5.0000e-04\n",
      "Epoch 36/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6074 - loss: 1.9664 - val_accuracy: 0.5581 - val_loss: 2.1134 - learning_rate: 5.0000e-04\n",
      "Epoch 37/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.6320 - loss: 1.9414 - val_accuracy: 0.6010 - val_loss: 2.0307 - learning_rate: 5.0000e-04\n",
      "Epoch 38/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6104 - loss: 1.9521 - val_accuracy: 0.5960 - val_loss: 2.0192 - learning_rate: 5.0000e-04\n",
      "Epoch 39/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6281 - loss: 1.9090 - val_accuracy: 0.5524 - val_loss: 2.1062 - learning_rate: 5.0000e-04\n",
      "Epoch 40/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6528 - loss: 1.7954 - val_accuracy: 0.5777 - val_loss: 2.0544 - learning_rate: 5.0000e-04\n",
      "Epoch 41/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6450 - loss: 1.8827 - val_accuracy: 0.6035 - val_loss: 1.9958 - learning_rate: 5.0000e-04\n",
      "Epoch 42/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6390 - loss: 1.8911 - val_accuracy: 0.6042 - val_loss: 1.9277 - learning_rate: 5.0000e-04\n",
      "Epoch 43/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6319 - loss: 1.9276 - val_accuracy: 0.6073 - val_loss: 1.9580 - learning_rate: 5.0000e-04\n",
      "Epoch 44/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6819 - loss: 1.7760 - val_accuracy: 0.5511 - val_loss: 2.1600 - learning_rate: 5.0000e-04\n",
      "Epoch 45/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6187 - loss: 1.9221 - val_accuracy: 0.5997 - val_loss: 1.9930 - learning_rate: 5.0000e-04\n",
      "Epoch 46/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6393 - loss: 1.8309 - val_accuracy: 0.6155 - val_loss: 1.9113 - learning_rate: 5.0000e-04\n",
      "Epoch 47/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.6268 - loss: 1.9877 - val_accuracy: 0.5461 - val_loss: 2.1686 - learning_rate: 5.0000e-04\n",
      "Epoch 48/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6708 - loss: 1.7811 - val_accuracy: 0.5758 - val_loss: 2.0681 - learning_rate: 5.0000e-04\n",
      "Epoch 49/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.6789 - loss: 1.7235\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.6788 - loss: 1.7239 - val_accuracy: 0.6105 - val_loss: 1.9416 - learning_rate: 5.0000e-04\n",
      "Epoch 50/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6822 - loss: 1.7828 - val_accuracy: 0.6143 - val_loss: 1.9938 - learning_rate: 2.5000e-04\n",
      "Epoch 51/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6692 - loss: 1.7666 - val_accuracy: 0.6566 - val_loss: 1.7977 - learning_rate: 2.5000e-04\n",
      "Epoch 52/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7276 - loss: 1.6910 - val_accuracy: 0.6477 - val_loss: 1.7808 - learning_rate: 2.5000e-04\n",
      "Epoch 53/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6942 - loss: 1.7740 - val_accuracy: 0.5985 - val_loss: 1.9699 - learning_rate: 2.5000e-04\n",
      "Epoch 54/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6792 - loss: 1.8123 - val_accuracy: 0.6206 - val_loss: 1.9274 - learning_rate: 2.5000e-04\n",
      "Epoch 55/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7229 - loss: 1.6253 - val_accuracy: 0.6282 - val_loss: 1.8718 - learning_rate: 2.5000e-04\n",
      "Epoch 56/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7182 - loss: 1.6933 - val_accuracy: 0.6503 - val_loss: 1.8024 - learning_rate: 2.5000e-04\n",
      "Epoch 57/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6952 - loss: 1.7481 - val_accuracy: 0.6225 - val_loss: 1.9043 - learning_rate: 2.5000e-04\n",
      "Epoch 58/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6850 - loss: 1.7187 - val_accuracy: 0.6086 - val_loss: 1.8811 - learning_rate: 2.5000e-04\n",
      "Epoch 59/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7301 - loss: 1.6625 - val_accuracy: 0.6351 - val_loss: 1.8944 - learning_rate: 2.5000e-04\n",
      "Epoch 60/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7474 - loss: 1.6012 - val_accuracy: 0.6080 - val_loss: 1.8958 - learning_rate: 2.5000e-04\n",
      "Epoch 61/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7592 - loss: 1.5930 - val_accuracy: 0.6143 - val_loss: 1.9200 - learning_rate: 2.5000e-04\n",
      "Epoch 62/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7642 - loss: 1.6039 - val_accuracy: 0.6313 - val_loss: 1.8687 - learning_rate: 2.5000e-04\n",
      "Epoch 63/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7105 - loss: 1.7734 - val_accuracy: 0.5997 - val_loss: 1.9327 - learning_rate: 2.5000e-04\n",
      "Epoch 64/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7772 - loss: 1.5040 - val_accuracy: 0.5833 - val_loss: 1.9806 - learning_rate: 2.5000e-04\n",
      "Epoch 65/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7422 - loss: 1.6119 - val_accuracy: 0.6054 - val_loss: 1.9380 - learning_rate: 2.5000e-04\n",
      "Epoch 66/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7506 - loss: 1.5716 - val_accuracy: 0.6256 - val_loss: 1.8351 - learning_rate: 2.5000e-04\n",
      "Epoch 67/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.7554 - loss: 1.5703\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7551 - loss: 1.5708 - val_accuracy: 0.6269 - val_loss: 1.8284 - learning_rate: 2.5000e-04\n",
      "Epoch 68/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7525 - loss: 1.6721 - val_accuracy: 0.6332 - val_loss: 1.8754 - learning_rate: 1.2500e-04\n",
      "Epoch 69/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7444 - loss: 1.6654 - val_accuracy: 0.6256 - val_loss: 1.8942 - learning_rate: 1.2500e-04\n",
      "Epoch 70/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7542 - loss: 1.6125 - val_accuracy: 0.6193 - val_loss: 1.8364 - learning_rate: 1.2500e-04\n",
      "Epoch 71/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8108 - loss: 1.4998 - val_accuracy: 0.6174 - val_loss: 1.9055 - learning_rate: 1.2500e-04\n",
      "Epoch 72/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7695 - loss: 1.5688 - val_accuracy: 0.6370 - val_loss: 1.8595 - learning_rate: 1.2500e-04\n",
      "Epoch 73/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7767 - loss: 1.5481 - val_accuracy: 0.6604 - val_loss: 1.7156 - learning_rate: 1.2500e-04\n",
      "Epoch 74/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7657 - loss: 1.6073 - val_accuracy: 0.6319 - val_loss: 1.8193 - learning_rate: 1.2500e-04\n",
      "Epoch 75/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7368 - loss: 1.7162 - val_accuracy: 0.6319 - val_loss: 1.8101 - learning_rate: 1.2500e-04\n",
      "Epoch 76/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7784 - loss: 1.5656 - val_accuracy: 0.6591 - val_loss: 1.8148 - learning_rate: 1.2500e-04\n",
      "Epoch 77/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7932 - loss: 1.4411 - val_accuracy: 0.6187 - val_loss: 1.8799 - learning_rate: 1.2500e-04\n",
      "Epoch 78/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7795 - loss: 1.5339 - val_accuracy: 0.6484 - val_loss: 1.7664 - learning_rate: 1.2500e-04\n",
      "Epoch 79/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7822 - loss: 1.5513 - val_accuracy: 0.6408 - val_loss: 1.8141 - learning_rate: 1.2500e-04\n",
      "Epoch 80/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7600 - loss: 1.5753 - val_accuracy: 0.6496 - val_loss: 1.7335 - learning_rate: 1.2500e-04\n",
      "Epoch 81/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8058 - loss: 1.4631 - val_accuracy: 0.6427 - val_loss: 1.7983 - learning_rate: 1.2500e-04\n",
      "Epoch 82/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8289 - loss: 1.4185 - val_accuracy: 0.6206 - val_loss: 1.8419 - learning_rate: 1.2500e-04\n",
      "Epoch 83/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8074 - loss: 1.4829 - val_accuracy: 0.6212 - val_loss: 1.8841 - learning_rate: 1.2500e-04\n",
      "Epoch 84/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8197 - loss: 1.4423 - val_accuracy: 0.6136 - val_loss: 1.8807 - learning_rate: 1.2500e-04\n",
      "Epoch 85/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7946 - loss: 1.5568 - val_accuracy: 0.6509 - val_loss: 1.7888 - learning_rate: 1.2500e-04\n",
      "Epoch 86/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8046 - loss: 1.5311 - val_accuracy: 0.6206 - val_loss: 1.8461 - learning_rate: 1.2500e-04\n",
      "Epoch 87/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8059 - loss: 1.4792 - val_accuracy: 0.6433 - val_loss: 1.7633 - learning_rate: 1.2500e-04\n",
      "Epoch 88/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8378 - loss: 1.3953 - val_accuracy: 0.6288 - val_loss: 1.9049 - learning_rate: 1.2500e-04\n",
      "Epoch 89/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8320 - loss: 1.4173\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8319 - loss: 1.4175 - val_accuracy: 0.6086 - val_loss: 1.8783 - learning_rate: 1.2500e-04\n",
      "Epoch 90/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8361 - loss: 1.4080 - val_accuracy: 0.6225 - val_loss: 1.8545 - learning_rate: 6.2500e-05\n",
      "Epoch 91/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8405 - loss: 1.4320 - val_accuracy: 0.6029 - val_loss: 1.9463 - learning_rate: 6.2500e-05\n",
      "Epoch 92/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7957 - loss: 1.4999 - val_accuracy: 0.6572 - val_loss: 1.8161 - learning_rate: 6.2500e-05\n",
      "Epoch 93/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7917 - loss: 1.5137 - val_accuracy: 0.6376 - val_loss: 1.8460 - learning_rate: 6.2500e-05\n",
      "Epoch 94/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8024 - loss: 1.4917 - val_accuracy: 0.6458 - val_loss: 1.7479 - learning_rate: 6.2500e-05\n",
      "Epoch 95/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8154 - loss: 1.4816 - val_accuracy: 0.6471 - val_loss: 1.8108 - learning_rate: 6.2500e-05\n",
      "Epoch 96/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8250 - loss: 1.4675 - val_accuracy: 0.6155 - val_loss: 1.8370 - learning_rate: 6.2500e-05\n",
      "Epoch 97/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8085 - loss: 1.5063 - val_accuracy: 0.6275 - val_loss: 1.8296 - learning_rate: 6.2500e-05\n",
      "Epoch 98/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8033 - loss: 1.4963 - val_accuracy: 0.6477 - val_loss: 1.8253 - learning_rate: 6.2500e-05\n",
      "Epoch 99/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8299 - loss: 1.4252 - val_accuracy: 0.6458 - val_loss: 1.7348 - learning_rate: 6.2500e-05\n",
      "Epoch 100/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7978 - loss: 1.4945 - val_accuracy: 0.6446 - val_loss: 1.8514 - learning_rate: 6.2500e-05\n",
      "Epoch 101/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8386 - loss: 1.4689 - val_accuracy: 0.6035 - val_loss: 1.8995 - learning_rate: 6.2500e-05\n",
      "Epoch 102/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8387 - loss: 1.4157 - val_accuracy: 0.6225 - val_loss: 1.8668 - learning_rate: 6.2500e-05\n",
      "Epoch 103/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8249 - loss: 1.4709 - val_accuracy: 0.6597 - val_loss: 1.7744 - learning_rate: 6.2500e-05\n",
      "Epoch 104/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8598 - loss: 1.3738 - val_accuracy: 0.6351 - val_loss: 1.8537 - learning_rate: 6.2500e-05\n",
      "Epoch 105/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8359 - loss: 1.4300 - val_accuracy: 0.6256 - val_loss: 1.8061 - learning_rate: 6.2500e-05\n",
      "Epoch 106/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8421 - loss: 1.4049\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8419 - loss: 1.4054 - val_accuracy: 0.6484 - val_loss: 1.8171 - learning_rate: 6.2500e-05\n",
      "Epoch 107/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7944 - loss: 1.5132 - val_accuracy: 0.6471 - val_loss: 1.8227 - learning_rate: 3.1250e-05\n",
      "Epoch 108/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8401 - loss: 1.4090 - val_accuracy: 0.6433 - val_loss: 1.7765 - learning_rate: 3.1250e-05\n",
      "Epoch 109/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8469 - loss: 1.4252 - val_accuracy: 0.6452 - val_loss: 1.8803 - learning_rate: 3.1250e-05\n",
      "Epoch 110/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8082 - loss: 1.5092 - val_accuracy: 0.6357 - val_loss: 1.8448 - learning_rate: 3.1250e-05\n",
      "Epoch 111/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8435 - loss: 1.4038 - val_accuracy: 0.6313 - val_loss: 1.8453 - learning_rate: 3.1250e-05\n",
      "Epoch 112/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8118 - loss: 1.5077 - val_accuracy: 0.6484 - val_loss: 1.7595 - learning_rate: 3.1250e-05\n",
      "Epoch 113/160\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8147 - loss: 1.4758 - val_accuracy: 0.6553 - val_loss: 1.7441 - learning_rate: 3.1250e-05\n",
      "Epoch 113: early stopping\n",
      "Restoring model weights from the end of the best epoch: 73.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2025-08-09 10:28:08.145953: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step\n",
      "\n",
      "===== FOLD 4/5 =====\n",
      "Epoch 1/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 132ms/step - accuracy: 0.0919 - loss: 4.3231 - val_accuracy: 0.0886 - val_loss: 4.1793 - learning_rate: 5.0000e-04\n",
      "Epoch 2/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.1895 - loss: 3.5847 - val_accuracy: 0.0852 - val_loss: 4.3096 - learning_rate: 5.0000e-04\n",
      "Epoch 3/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.2271 - loss: 3.4001 - val_accuracy: 0.1988 - val_loss: 3.4814 - learning_rate: 5.0000e-04\n",
      "Epoch 4/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.2737 - loss: 3.2717 - val_accuracy: 0.3232 - val_loss: 3.0062 - learning_rate: 5.0000e-04\n",
      "Epoch 5/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.3206 - loss: 2.9744 - val_accuracy: 0.3928 - val_loss: 2.7843 - learning_rate: 5.0000e-04\n",
      "Epoch 6/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.3050 - loss: 3.1126 - val_accuracy: 0.3753 - val_loss: 2.8430 - learning_rate: 5.0000e-04\n",
      "Epoch 7/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.3596 - loss: 2.8722 - val_accuracy: 0.4097 - val_loss: 2.7482 - learning_rate: 5.0000e-04\n",
      "Epoch 8/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.3490 - loss: 2.8791 - val_accuracy: 0.4151 - val_loss: 2.6755 - learning_rate: 5.0000e-04\n",
      "Epoch 9/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.3646 - loss: 2.8525 - val_accuracy: 0.4652 - val_loss: 2.5746 - learning_rate: 5.0000e-04\n",
      "Epoch 10/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.4120 - loss: 2.7087 - val_accuracy: 0.4523 - val_loss: 2.6041 - learning_rate: 5.0000e-04\n",
      "Epoch 11/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.4172 - loss: 2.6696 - val_accuracy: 0.4645 - val_loss: 2.5872 - learning_rate: 5.0000e-04\n",
      "Epoch 12/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.4669 - loss: 2.5903 - val_accuracy: 0.4882 - val_loss: 2.4505 - learning_rate: 5.0000e-04\n",
      "Epoch 13/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.4351 - loss: 2.5884 - val_accuracy: 0.4909 - val_loss: 2.4587 - learning_rate: 5.0000e-04\n",
      "Epoch 14/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.4357 - loss: 2.5864 - val_accuracy: 0.4442 - val_loss: 2.6349 - learning_rate: 5.0000e-04\n",
      "Epoch 15/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.4443 - loss: 2.5913 - val_accuracy: 0.5017 - val_loss: 2.4074 - learning_rate: 5.0000e-04\n",
      "Epoch 16/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.4710 - loss: 2.5437 - val_accuracy: 0.5321 - val_loss: 2.3379 - learning_rate: 5.0000e-04\n",
      "Epoch 17/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.4861 - loss: 2.4065 - val_accuracy: 0.5490 - val_loss: 2.2828 - learning_rate: 5.0000e-04\n",
      "Epoch 18/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.4798 - loss: 2.4098 - val_accuracy: 0.5125 - val_loss: 2.3222 - learning_rate: 5.0000e-04\n",
      "Epoch 19/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5086 - loss: 2.3907 - val_accuracy: 0.4490 - val_loss: 2.4940 - learning_rate: 5.0000e-04\n",
      "Epoch 20/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5212 - loss: 2.3225 - val_accuracy: 0.4936 - val_loss: 2.3625 - learning_rate: 5.0000e-04\n",
      "Epoch 21/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5160 - loss: 2.3675 - val_accuracy: 0.5396 - val_loss: 2.2295 - learning_rate: 5.0000e-04\n",
      "Epoch 22/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5075 - loss: 2.3032 - val_accuracy: 0.5057 - val_loss: 2.3258 - learning_rate: 5.0000e-04\n",
      "Epoch 23/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.5062 - loss: 2.3711 - val_accuracy: 0.5260 - val_loss: 2.2807 - learning_rate: 5.0000e-04\n",
      "Epoch 24/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5368 - loss: 2.2521 - val_accuracy: 0.5341 - val_loss: 2.2376 - learning_rate: 5.0000e-04\n",
      "Epoch 25/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.5496 - loss: 2.2280 - val_accuracy: 0.5416 - val_loss: 2.1845 - learning_rate: 5.0000e-04\n",
      "Epoch 26/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.5700 - loss: 2.1764 - val_accuracy: 0.4902 - val_loss: 2.3168 - learning_rate: 5.0000e-04\n",
      "Epoch 27/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5366 - loss: 2.2240 - val_accuracy: 0.5287 - val_loss: 2.2210 - learning_rate: 5.0000e-04\n",
      "Epoch 28/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5622 - loss: 2.1941 - val_accuracy: 0.5429 - val_loss: 2.1769 - learning_rate: 5.0000e-04\n",
      "Epoch 29/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.5729 - loss: 2.1449 - val_accuracy: 0.5355 - val_loss: 2.1730 - learning_rate: 5.0000e-04\n",
      "Epoch 30/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5502 - loss: 2.2249 - val_accuracy: 0.5707 - val_loss: 2.0820 - learning_rate: 5.0000e-04\n",
      "Epoch 31/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5614 - loss: 2.1854 - val_accuracy: 0.5585 - val_loss: 2.1608 - learning_rate: 5.0000e-04\n",
      "Epoch 32/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5761 - loss: 2.1383 - val_accuracy: 0.5713 - val_loss: 2.0975 - learning_rate: 5.0000e-04\n",
      "Epoch 33/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.5778 - loss: 2.1206 - val_accuracy: 0.5673 - val_loss: 2.0778 - learning_rate: 5.0000e-04\n",
      "Epoch 34/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.5878 - loss: 2.0538 - val_accuracy: 0.5558 - val_loss: 2.1483 - learning_rate: 5.0000e-04\n",
      "Epoch 35/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.6099 - loss: 1.9880 - val_accuracy: 0.5348 - val_loss: 2.1189 - learning_rate: 5.0000e-04\n",
      "Epoch 36/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.5744 - loss: 2.0419 - val_accuracy: 0.5571 - val_loss: 2.1099 - learning_rate: 5.0000e-04\n",
      "Epoch 37/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5975 - loss: 2.0025 - val_accuracy: 0.5794 - val_loss: 2.0308 - learning_rate: 5.0000e-04\n",
      "Epoch 38/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 121ms/step - accuracy: 0.6144 - loss: 2.0162 - val_accuracy: 0.5490 - val_loss: 2.1341 - learning_rate: 5.0000e-04\n",
      "Epoch 39/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6157 - loss: 1.9420 - val_accuracy: 0.5227 - val_loss: 2.1555 - learning_rate: 5.0000e-04\n",
      "Epoch 40/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5926 - loss: 2.0854 - val_accuracy: 0.5794 - val_loss: 2.0140 - learning_rate: 5.0000e-04\n",
      "Epoch 41/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.6083 - loss: 2.0189 - val_accuracy: 0.5477 - val_loss: 2.0703 - learning_rate: 5.0000e-04\n",
      "Epoch 42/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.6538 - loss: 1.8384 - val_accuracy: 0.5950 - val_loss: 1.9602 - learning_rate: 5.0000e-04\n",
      "Epoch 43/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.6101 - loss: 1.9116 - val_accuracy: 0.5761 - val_loss: 2.0128 - learning_rate: 5.0000e-04\n",
      "Epoch 44/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6220 - loss: 1.9236 - val_accuracy: 0.5551 - val_loss: 2.0239 - learning_rate: 5.0000e-04\n",
      "Epoch 45/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 122ms/step - accuracy: 0.6267 - loss: 1.9398 - val_accuracy: 0.5578 - val_loss: 1.9912 - learning_rate: 5.0000e-04\n",
      "Epoch 46/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.6349 - loss: 1.8946 - val_accuracy: 0.5680 - val_loss: 2.0202 - learning_rate: 5.0000e-04\n",
      "Epoch 47/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6360 - loss: 1.9583 - val_accuracy: 0.5788 - val_loss: 1.9681 - learning_rate: 5.0000e-04\n",
      "Epoch 48/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6058 - loss: 1.9224 - val_accuracy: 0.5936 - val_loss: 1.8846 - learning_rate: 5.0000e-04\n",
      "Epoch 49/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.6751 - loss: 1.7673 - val_accuracy: 0.5429 - val_loss: 2.0867 - learning_rate: 5.0000e-04\n",
      "Epoch 50/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.6739 - loss: 1.7989 - val_accuracy: 0.5578 - val_loss: 1.9847 - learning_rate: 5.0000e-04\n",
      "Epoch 51/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.6642 - loss: 1.8151 - val_accuracy: 0.5463 - val_loss: 2.0341 - learning_rate: 5.0000e-04\n",
      "Epoch 52/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.6397 - loss: 1.8655 - val_accuracy: 0.5727 - val_loss: 1.9966 - learning_rate: 5.0000e-04\n",
      "Epoch 53/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6862 - loss: 1.7551 - val_accuracy: 0.5632 - val_loss: 1.9906 - learning_rate: 5.0000e-04\n",
      "Epoch 54/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6689 - loss: 1.7882 - val_accuracy: 0.5747 - val_loss: 1.9676 - learning_rate: 5.0000e-04\n",
      "Epoch 55/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.6737 - loss: 1.7977 - val_accuracy: 0.5727 - val_loss: 1.9502 - learning_rate: 5.0000e-04\n",
      "Epoch 56/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6922 - loss: 1.7354 - val_accuracy: 0.5761 - val_loss: 1.9165 - learning_rate: 5.0000e-04\n",
      "Epoch 57/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6626 - loss: 1.8020 - val_accuracy: 0.5713 - val_loss: 1.9821 - learning_rate: 5.0000e-04\n",
      "Epoch 58/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.6546 - loss: 1.8242\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.6545 - loss: 1.8246 - val_accuracy: 0.5639 - val_loss: 1.9738 - learning_rate: 5.0000e-04\n",
      "Epoch 59/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7132 - loss: 1.6872 - val_accuracy: 0.5585 - val_loss: 1.9589 - learning_rate: 2.5000e-04\n",
      "Epoch 60/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6846 - loss: 1.7630 - val_accuracy: 0.5842 - val_loss: 1.8668 - learning_rate: 2.5000e-04\n",
      "Epoch 61/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7233 - loss: 1.7000 - val_accuracy: 0.5991 - val_loss: 1.9065 - learning_rate: 2.5000e-04\n",
      "Epoch 62/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7014 - loss: 1.7475 - val_accuracy: 0.5903 - val_loss: 1.8920 - learning_rate: 2.5000e-04\n",
      "Epoch 63/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7112 - loss: 1.7015 - val_accuracy: 0.5713 - val_loss: 1.9058 - learning_rate: 2.5000e-04\n",
      "Epoch 64/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7253 - loss: 1.6815 - val_accuracy: 0.6105 - val_loss: 1.9152 - learning_rate: 2.5000e-04\n",
      "Epoch 65/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.7466 - loss: 1.6215 - val_accuracy: 0.5815 - val_loss: 1.9402 - learning_rate: 2.5000e-04\n",
      "Epoch 66/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7096 - loss: 1.7201 - val_accuracy: 0.5963 - val_loss: 1.9200 - learning_rate: 2.5000e-04\n",
      "Epoch 67/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7335 - loss: 1.6617 - val_accuracy: 0.5680 - val_loss: 1.9925 - learning_rate: 2.5000e-04\n",
      "Epoch 68/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7184 - loss: 1.6651 - val_accuracy: 0.5740 - val_loss: 1.9041 - learning_rate: 2.5000e-04\n",
      "Epoch 69/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7511 - loss: 1.5984 - val_accuracy: 0.5963 - val_loss: 1.9381 - learning_rate: 2.5000e-04\n",
      "Epoch 70/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7658 - loss: 1.5636 - val_accuracy: 0.5903 - val_loss: 1.9401 - learning_rate: 2.5000e-04\n",
      "Epoch 71/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7636 - loss: 1.5736 - val_accuracy: 0.5801 - val_loss: 1.9185 - learning_rate: 2.5000e-04\n",
      "Epoch 72/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7702 - loss: 1.5565 - val_accuracy: 0.5882 - val_loss: 1.9200 - learning_rate: 2.5000e-04\n",
      "Epoch 73/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7350 - loss: 1.6071 - val_accuracy: 0.5335 - val_loss: 2.0756 - learning_rate: 2.5000e-04\n",
      "Epoch 74/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7348 - loss: 1.6532 - val_accuracy: 0.5977 - val_loss: 1.9146 - learning_rate: 2.5000e-04\n",
      "Epoch 75/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7522 - loss: 1.5983 - val_accuracy: 0.5747 - val_loss: 1.9697 - learning_rate: 2.5000e-04\n",
      "Epoch 76/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7512 - loss: 1.5733 - val_accuracy: 0.6011 - val_loss: 1.8784 - learning_rate: 2.5000e-04\n",
      "Epoch 77/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7356 - loss: 1.6536 - val_accuracy: 0.6146 - val_loss: 1.8074 - learning_rate: 2.5000e-04\n",
      "Epoch 78/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.7865 - loss: 1.5126 - val_accuracy: 0.6187 - val_loss: 1.8371 - learning_rate: 2.5000e-04\n",
      "Epoch 79/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7937 - loss: 1.5034 - val_accuracy: 0.5903 - val_loss: 1.9097 - learning_rate: 2.5000e-04\n",
      "Epoch 80/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7658 - loss: 1.6078 - val_accuracy: 0.5862 - val_loss: 1.8708 - learning_rate: 2.5000e-04\n",
      "Epoch 81/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8060 - loss: 1.4437 - val_accuracy: 0.5747 - val_loss: 1.9108 - learning_rate: 2.5000e-04\n",
      "Epoch 82/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7366 - loss: 1.6594 - val_accuracy: 0.5963 - val_loss: 1.8546 - learning_rate: 2.5000e-04\n",
      "Epoch 83/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7376 - loss: 1.5993 - val_accuracy: 0.5977 - val_loss: 1.8924 - learning_rate: 2.5000e-04\n",
      "Epoch 84/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7459 - loss: 1.6533 - val_accuracy: 0.6072 - val_loss: 1.8462 - learning_rate: 2.5000e-04\n",
      "Epoch 85/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7809 - loss: 1.5228 - val_accuracy: 0.5761 - val_loss: 1.9508 - learning_rate: 2.5000e-04\n",
      "Epoch 86/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7760 - loss: 1.5260 - val_accuracy: 0.5693 - val_loss: 1.9285 - learning_rate: 2.5000e-04\n",
      "Epoch 87/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8290 - loss: 1.4044 - val_accuracy: 0.5997 - val_loss: 1.8960 - learning_rate: 2.5000e-04\n",
      "Epoch 88/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8129 - loss: 1.4285 - val_accuracy: 0.6173 - val_loss: 1.7840 - learning_rate: 2.5000e-04\n",
      "Epoch 89/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7509 - loss: 1.5996 - val_accuracy: 0.5963 - val_loss: 1.8777 - learning_rate: 2.5000e-04\n",
      "Epoch 90/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8158 - loss: 1.4216 - val_accuracy: 0.5788 - val_loss: 1.9587 - learning_rate: 2.5000e-04\n",
      "Epoch 91/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.7967 - loss: 1.4823 - val_accuracy: 0.6024 - val_loss: 1.8983 - learning_rate: 2.5000e-04\n",
      "Epoch 92/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.7812 - loss: 1.5655 - val_accuracy: 0.5680 - val_loss: 2.0041 - learning_rate: 2.5000e-04\n",
      "Epoch 93/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.7999 - loss: 1.5104 - val_accuracy: 0.5903 - val_loss: 1.8563 - learning_rate: 2.5000e-04\n",
      "Epoch 94/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8172 - loss: 1.4618\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8171 - loss: 1.4619 - val_accuracy: 0.5862 - val_loss: 1.9266 - learning_rate: 2.5000e-04\n",
      "Epoch 95/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7720 - loss: 1.5439 - val_accuracy: 0.5788 - val_loss: 1.9571 - learning_rate: 1.2500e-04\n",
      "Epoch 96/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8468 - loss: 1.4005 - val_accuracy: 0.5970 - val_loss: 1.9178 - learning_rate: 1.2500e-04\n",
      "Epoch 97/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8213 - loss: 1.3977 - val_accuracy: 0.5774 - val_loss: 1.9449 - learning_rate: 1.2500e-04\n",
      "Epoch 98/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7861 - loss: 1.5740 - val_accuracy: 0.5923 - val_loss: 1.9304 - learning_rate: 1.2500e-04\n",
      "Epoch 99/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8206 - loss: 1.4801 - val_accuracy: 0.5984 - val_loss: 1.8964 - learning_rate: 1.2500e-04\n",
      "Epoch 100/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8449 - loss: 1.3756 - val_accuracy: 0.5903 - val_loss: 1.8341 - learning_rate: 1.2500e-04\n",
      "Epoch 101/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8364 - loss: 1.3856 - val_accuracy: 0.5936 - val_loss: 1.9166 - learning_rate: 1.2500e-04\n",
      "Epoch 102/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7798 - loss: 1.5668 - val_accuracy: 0.6126 - val_loss: 1.8406 - learning_rate: 1.2500e-04\n",
      "Epoch 103/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8432 - loss: 1.4014 - val_accuracy: 0.5889 - val_loss: 1.9147 - learning_rate: 1.2500e-04\n",
      "Epoch 104/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8558 - loss: 1.3872 - val_accuracy: 0.6085 - val_loss: 1.8712 - learning_rate: 1.2500e-04\n",
      "Epoch 105/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8083 - loss: 1.4832 - val_accuracy: 0.6105 - val_loss: 1.8436 - learning_rate: 1.2500e-04\n",
      "Epoch 106/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8338 - loss: 1.4687 - val_accuracy: 0.5666 - val_loss: 2.0020 - learning_rate: 1.2500e-04\n",
      "Epoch 107/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8201 - loss: 1.4753 - val_accuracy: 0.6011 - val_loss: 1.9251 - learning_rate: 1.2500e-04\n",
      "Epoch 108/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8604 - loss: 1.3792 - val_accuracy: 0.6146 - val_loss: 1.8415 - learning_rate: 1.2500e-04\n",
      "Epoch 109/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8101 - loss: 1.5083 - val_accuracy: 0.6254 - val_loss: 1.8499 - learning_rate: 1.2500e-04\n",
      "Epoch 110/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8276 - loss: 1.4338 - val_accuracy: 0.5957 - val_loss: 1.9446 - learning_rate: 1.2500e-04\n",
      "Epoch 111/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7904 - loss: 1.5435 - val_accuracy: 0.5815 - val_loss: 1.8990 - learning_rate: 1.2500e-04\n",
      "Epoch 112/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8389 - loss: 1.4277 - val_accuracy: 0.6065 - val_loss: 1.8690 - learning_rate: 1.2500e-04\n",
      "Epoch 113/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7988 - loss: 1.4836 - val_accuracy: 0.5551 - val_loss: 2.0142 - learning_rate: 1.2500e-04\n",
      "Epoch 114/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8356 - loss: 1.4535 - val_accuracy: 0.5815 - val_loss: 1.9494 - learning_rate: 1.2500e-04\n",
      "Epoch 115/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.7988 - loss: 1.5160 - val_accuracy: 0.5747 - val_loss: 1.9464 - learning_rate: 1.2500e-04\n",
      "Epoch 116/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8055 - loss: 1.4742 - val_accuracy: 0.5862 - val_loss: 1.9329 - learning_rate: 1.2500e-04\n",
      "Epoch 117/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8170 - loss: 1.4153 - val_accuracy: 0.5963 - val_loss: 1.8753 - learning_rate: 1.2500e-04\n",
      "Epoch 118/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8188 - loss: 1.4230 - val_accuracy: 0.5808 - val_loss: 1.9057 - learning_rate: 1.2500e-04\n",
      "Epoch 119/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8505 - loss: 1.3598 - val_accuracy: 0.5889 - val_loss: 1.8633 - learning_rate: 1.2500e-04\n",
      "Epoch 120/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8056 - loss: 1.4560 - val_accuracy: 0.5936 - val_loss: 1.9308 - learning_rate: 1.2500e-04\n",
      "Epoch 121/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8413 - loss: 1.3795 - val_accuracy: 0.5991 - val_loss: 1.9246 - learning_rate: 1.2500e-04\n",
      "Epoch 122/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8516 - loss: 1.3671 - val_accuracy: 0.5882 - val_loss: 1.9541 - learning_rate: 1.2500e-04\n",
      "Epoch 123/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8675 - loss: 1.3327 - val_accuracy: 0.5957 - val_loss: 1.8975 - learning_rate: 1.2500e-04\n",
      "Epoch 124/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8367 - loss: 1.3974 - val_accuracy: 0.5896 - val_loss: 1.9064 - learning_rate: 1.2500e-04\n",
      "Epoch 125/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8343 - loss: 1.4315\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8341 - loss: 1.4315 - val_accuracy: 0.5923 - val_loss: 1.9300 - learning_rate: 1.2500e-04\n",
      "Epoch 126/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.8279 - loss: 1.4217 - val_accuracy: 0.6160 - val_loss: 1.8585 - learning_rate: 6.2500e-05\n",
      "Epoch 127/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8520 - loss: 1.3802 - val_accuracy: 0.6207 - val_loss: 1.7986 - learning_rate: 6.2500e-05\n",
      "Epoch 128/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8808 - loss: 1.2625 - val_accuracy: 0.6051 - val_loss: 1.8314 - learning_rate: 6.2500e-05\n",
      "Epoch 129/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8145 - loss: 1.5185 - val_accuracy: 0.5659 - val_loss: 1.9656 - learning_rate: 6.2500e-05\n",
      "Epoch 130/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8397 - loss: 1.4575 - val_accuracy: 0.5882 - val_loss: 1.9059 - learning_rate: 6.2500e-05\n",
      "Epoch 131/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8436 - loss: 1.3533 - val_accuracy: 0.5869 - val_loss: 1.9338 - learning_rate: 6.2500e-05\n",
      "Epoch 132/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8304 - loss: 1.3924 - val_accuracy: 0.6133 - val_loss: 1.7926 - learning_rate: 6.2500e-05\n",
      "Epoch 133/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8095 - loss: 1.4792 - val_accuracy: 0.6315 - val_loss: 1.7842 - learning_rate: 6.2500e-05\n",
      "Epoch 134/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8545 - loss: 1.3732 - val_accuracy: 0.6065 - val_loss: 1.8794 - learning_rate: 6.2500e-05\n",
      "Epoch 135/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8389 - loss: 1.4057 - val_accuracy: 0.5970 - val_loss: 1.8872 - learning_rate: 6.2500e-05\n",
      "Epoch 136/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8316 - loss: 1.4114 - val_accuracy: 0.5794 - val_loss: 1.9247 - learning_rate: 6.2500e-05\n",
      "Epoch 137/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8219 - loss: 1.4530 - val_accuracy: 0.5984 - val_loss: 1.8592 - learning_rate: 6.2500e-05\n",
      "Epoch 138/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8452 - loss: 1.4229 - val_accuracy: 0.5828 - val_loss: 1.9204 - learning_rate: 6.2500e-05\n",
      "Epoch 139/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8963 - loss: 1.2797 - val_accuracy: 0.6133 - val_loss: 1.8524 - learning_rate: 6.2500e-05\n",
      "Epoch 140/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8620 - loss: 1.3361 - val_accuracy: 0.6173 - val_loss: 1.8317 - learning_rate: 6.2500e-05\n",
      "Epoch 141/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8682 - loss: 1.3580 - val_accuracy: 0.6011 - val_loss: 1.8412 - learning_rate: 6.2500e-05\n",
      "Epoch 142/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8131 - loss: 1.5039 - val_accuracy: 0.5936 - val_loss: 1.9128 - learning_rate: 6.2500e-05\n",
      "Epoch 143/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8478 - loss: 1.3775 - val_accuracy: 0.6139 - val_loss: 1.8678 - learning_rate: 6.2500e-05\n",
      "Epoch 144/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8409 - loss: 1.4178 - val_accuracy: 0.5822 - val_loss: 2.0221 - learning_rate: 6.2500e-05\n",
      "Epoch 145/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8383 - loss: 1.4260 - val_accuracy: 0.6085 - val_loss: 1.8059 - learning_rate: 6.2500e-05\n",
      "Epoch 146/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8540 - loss: 1.3567 - val_accuracy: 0.6362 - val_loss: 1.7852 - learning_rate: 6.2500e-05\n",
      "Epoch 147/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8389 - loss: 1.4411 - val_accuracy: 0.6166 - val_loss: 1.7999 - learning_rate: 6.2500e-05\n",
      "Epoch 148/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8957 - loss: 1.2621 - val_accuracy: 0.6139 - val_loss: 1.8750 - learning_rate: 6.2500e-05\n",
      "Epoch 149/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 125ms/step - accuracy: 0.8480 - loss: 1.4205 - val_accuracy: 0.6031 - val_loss: 1.8694 - learning_rate: 6.2500e-05\n",
      "Epoch 150/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8189 - loss: 1.4675 - val_accuracy: 0.5646 - val_loss: 2.0239 - learning_rate: 6.2500e-05\n",
      "Epoch 151/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8545 - loss: 1.3473 - val_accuracy: 0.5950 - val_loss: 1.8502 - learning_rate: 6.2500e-05\n",
      "Epoch 152/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8617 - loss: 1.3450 - val_accuracy: 0.5936 - val_loss: 1.8493 - learning_rate: 6.2500e-05\n",
      "Epoch 153/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8801 - loss: 1.2836 - val_accuracy: 0.6031 - val_loss: 1.8469 - learning_rate: 6.2500e-05\n",
      "Epoch 154/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8458 - loss: 1.3853 - val_accuracy: 0.5991 - val_loss: 1.9096 - learning_rate: 6.2500e-05\n",
      "Epoch 155/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8954 - loss: 1.2420 - val_accuracy: 0.5849 - val_loss: 1.9037 - learning_rate: 6.2500e-05\n",
      "Epoch 156/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8571 - loss: 1.3458 - val_accuracy: 0.6099 - val_loss: 1.8667 - learning_rate: 6.2500e-05\n",
      "Epoch 157/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8286 - loss: 1.4026 - val_accuracy: 0.5849 - val_loss: 1.8954 - learning_rate: 6.2500e-05\n",
      "Epoch 158/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8850 - loss: 1.2901 - val_accuracy: 0.6099 - val_loss: 1.8185 - learning_rate: 6.2500e-05\n",
      "Epoch 159/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 123ms/step - accuracy: 0.8539 - loss: 1.3698 - val_accuracy: 0.6018 - val_loss: 1.9314 - learning_rate: 6.2500e-05\n",
      "Epoch 160/160\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 124ms/step - accuracy: 0.8978 - loss: 1.2224 - val_accuracy: 0.5882 - val_loss: 1.9523 - learning_rate: 6.2500e-05\n",
      "Restoring model weights from the end of the best epoch: 146.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2025-08-09 11:01:46.153089: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step\n",
      "\n",
      "===== FOLD 5/5 =====\n",
      "Epoch 1/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 133ms/step - accuracy: 0.0713 - loss: 4.4921 - val_accuracy: 0.0799 - val_loss: 4.3185 - learning_rate: 5.0000e-04\n",
      "Epoch 2/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.1699 - loss: 3.6941 - val_accuracy: 0.1119 - val_loss: 4.0976 - learning_rate: 5.0000e-04\n",
      "Epoch 3/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.2295 - loss: 3.3598 - val_accuracy: 0.1715 - val_loss: 3.6045 - learning_rate: 5.0000e-04\n",
      "Epoch 4/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.2715 - loss: 3.2229 - val_accuracy: 0.2545 - val_loss: 3.1936 - learning_rate: 5.0000e-04\n",
      "Epoch 5/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.3262 - loss: 3.0198 - val_accuracy: 0.3719 - val_loss: 2.8771 - learning_rate: 5.0000e-04\n",
      "Epoch 6/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.3560 - loss: 2.8884 - val_accuracy: 0.4075 - val_loss: 2.6710 - learning_rate: 5.0000e-04\n",
      "Epoch 7/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.3548 - loss: 2.9375 - val_accuracy: 0.4401 - val_loss: 2.6379 - learning_rate: 5.0000e-04\n",
      "Epoch 8/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.3977 - loss: 2.7167 - val_accuracy: 0.4474 - val_loss: 2.6094 - learning_rate: 5.0000e-04\n",
      "Epoch 9/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.4063 - loss: 2.7052 - val_accuracy: 0.4382 - val_loss: 2.6774 - learning_rate: 5.0000e-04\n",
      "Epoch 10/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.4175 - loss: 2.6918 - val_accuracy: 0.4431 - val_loss: 2.6489 - learning_rate: 5.0000e-04\n",
      "Epoch 11/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.4200 - loss: 2.7357 - val_accuracy: 0.4819 - val_loss: 2.4853 - learning_rate: 5.0000e-04\n",
      "Epoch 12/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.4581 - loss: 2.5656 - val_accuracy: 0.4788 - val_loss: 2.5210 - learning_rate: 5.0000e-04\n",
      "Epoch 13/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.4652 - loss: 2.5121 - val_accuracy: 0.4997 - val_loss: 2.5049 - learning_rate: 5.0000e-04\n",
      "Epoch 14/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.4603 - loss: 2.5277 - val_accuracy: 0.5138 - val_loss: 2.4089 - learning_rate: 5.0000e-04\n",
      "Epoch 15/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.4849 - loss: 2.4411 - val_accuracy: 0.5040 - val_loss: 2.3740 - learning_rate: 5.0000e-04\n",
      "Epoch 16/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.4840 - loss: 2.5169 - val_accuracy: 0.5187 - val_loss: 2.3586 - learning_rate: 5.0000e-04\n",
      "Epoch 17/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.4936 - loss: 2.4275 - val_accuracy: 0.5095 - val_loss: 2.3791 - learning_rate: 5.0000e-04\n",
      "Epoch 18/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.4778 - loss: 2.4962 - val_accuracy: 0.4856 - val_loss: 2.4592 - learning_rate: 5.0000e-04\n",
      "Epoch 19/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.5081 - loss: 2.3781 - val_accuracy: 0.5335 - val_loss: 2.3025 - learning_rate: 5.0000e-04\n",
      "Epoch 20/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.5398 - loss: 2.2804 - val_accuracy: 0.5022 - val_loss: 2.3717 - learning_rate: 5.0000e-04\n",
      "Epoch 21/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5249 - loss: 2.3082 - val_accuracy: 0.5138 - val_loss: 2.2766 - learning_rate: 5.0000e-04\n",
      "Epoch 22/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5407 - loss: 2.2457 - val_accuracy: 0.5347 - val_loss: 2.2398 - learning_rate: 5.0000e-04\n",
      "Epoch 23/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5579 - loss: 2.2435 - val_accuracy: 0.5347 - val_loss: 2.2127 - learning_rate: 5.0000e-04\n",
      "Epoch 24/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5520 - loss: 2.2406 - val_accuracy: 0.5304 - val_loss: 2.2125 - learning_rate: 5.0000e-04\n",
      "Epoch 25/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5674 - loss: 2.1390 - val_accuracy: 0.5218 - val_loss: 2.2321 - learning_rate: 5.0000e-04\n",
      "Epoch 26/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5688 - loss: 2.1799 - val_accuracy: 0.5212 - val_loss: 2.2365 - learning_rate: 5.0000e-04\n",
      "Epoch 27/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5684 - loss: 2.1536 - val_accuracy: 0.5353 - val_loss: 2.2070 - learning_rate: 5.0000e-04\n",
      "Epoch 28/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.5858 - loss: 2.1081 - val_accuracy: 0.5403 - val_loss: 2.2090 - learning_rate: 5.0000e-04\n",
      "Epoch 29/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5948 - loss: 2.0867 - val_accuracy: 0.5630 - val_loss: 2.1262 - learning_rate: 5.0000e-04\n",
      "Epoch 30/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6012 - loss: 2.0240 - val_accuracy: 0.5224 - val_loss: 2.2096 - learning_rate: 5.0000e-04\n",
      "Epoch 31/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.5850 - loss: 2.0771 - val_accuracy: 0.5501 - val_loss: 2.1366 - learning_rate: 5.0000e-04\n",
      "Epoch 32/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.5823 - loss: 2.1074 - val_accuracy: 0.5599 - val_loss: 2.1064 - learning_rate: 5.0000e-04\n",
      "Epoch 33/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6100 - loss: 2.0523 - val_accuracy: 0.5599 - val_loss: 2.0667 - learning_rate: 5.0000e-04\n",
      "Epoch 34/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6278 - loss: 1.9945 - val_accuracy: 0.5550 - val_loss: 2.0749 - learning_rate: 5.0000e-04\n",
      "Epoch 35/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6085 - loss: 2.0095 - val_accuracy: 0.5464 - val_loss: 2.1423 - learning_rate: 5.0000e-04\n",
      "Epoch 36/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.6017 - loss: 2.0258 - val_accuracy: 0.5507 - val_loss: 2.1969 - learning_rate: 5.0000e-04\n",
      "Epoch 37/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.6074 - loss: 2.0290 - val_accuracy: 0.5446 - val_loss: 2.0969 - learning_rate: 5.0000e-04\n",
      "Epoch 38/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.6375 - loss: 1.9185 - val_accuracy: 0.5243 - val_loss: 2.1508 - learning_rate: 5.0000e-04\n",
      "Epoch 39/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6280 - loss: 1.9378 - val_accuracy: 0.5679 - val_loss: 2.0184 - learning_rate: 5.0000e-04\n",
      "Epoch 40/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.6491 - loss: 1.8510 - val_accuracy: 0.5796 - val_loss: 1.9931 - learning_rate: 5.0000e-04\n",
      "Epoch 41/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6193 - loss: 1.9581 - val_accuracy: 0.5710 - val_loss: 2.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 42/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6730 - loss: 1.8513 - val_accuracy: 0.5907 - val_loss: 1.9681 - learning_rate: 5.0000e-04\n",
      "Epoch 43/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6360 - loss: 1.8874 - val_accuracy: 0.5458 - val_loss: 2.0747 - learning_rate: 5.0000e-04\n",
      "Epoch 44/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6513 - loss: 1.8567 - val_accuracy: 0.5544 - val_loss: 2.0758 - learning_rate: 5.0000e-04\n",
      "Epoch 45/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.6591 - loss: 1.8397 - val_accuracy: 0.5833 - val_loss: 2.0066 - learning_rate: 5.0000e-04\n",
      "Epoch 46/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6615 - loss: 1.8262 - val_accuracy: 0.5894 - val_loss: 1.9436 - learning_rate: 5.0000e-04\n",
      "Epoch 47/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6903 - loss: 1.7680 - val_accuracy: 0.5691 - val_loss: 2.0007 - learning_rate: 5.0000e-04\n",
      "Epoch 48/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6568 - loss: 1.8315 - val_accuracy: 0.5747 - val_loss: 2.0109 - learning_rate: 5.0000e-04\n",
      "Epoch 49/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6744 - loss: 1.8279 - val_accuracy: 0.5759 - val_loss: 1.9596 - learning_rate: 5.0000e-04\n",
      "Epoch 50/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6533 - loss: 1.7847 - val_accuracy: 0.5857 - val_loss: 1.9707 - learning_rate: 5.0000e-04\n",
      "Epoch 51/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6562 - loss: 1.8864 - val_accuracy: 0.5796 - val_loss: 1.9970 - learning_rate: 5.0000e-04\n",
      "Epoch 52/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.6709 - loss: 1.8228 - val_accuracy: 0.5753 - val_loss: 2.0285 - learning_rate: 5.0000e-04\n",
      "Epoch 53/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6897 - loss: 1.7499 - val_accuracy: 0.5980 - val_loss: 1.9449 - learning_rate: 5.0000e-04\n",
      "Epoch 54/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6895 - loss: 1.7706 - val_accuracy: 0.5913 - val_loss: 1.9022 - learning_rate: 5.0000e-04\n",
      "Epoch 55/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7138 - loss: 1.7299 - val_accuracy: 0.6269 - val_loss: 1.8519 - learning_rate: 5.0000e-04\n",
      "Epoch 56/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7166 - loss: 1.6718 - val_accuracy: 0.5882 - val_loss: 1.9320 - learning_rate: 5.0000e-04\n",
      "Epoch 57/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7045 - loss: 1.6902 - val_accuracy: 0.6042 - val_loss: 1.8825 - learning_rate: 5.0000e-04\n",
      "Epoch 58/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7230 - loss: 1.6761 - val_accuracy: 0.6036 - val_loss: 1.8917 - learning_rate: 5.0000e-04\n",
      "Epoch 59/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7264 - loss: 1.6643 - val_accuracy: 0.5999 - val_loss: 1.9324 - learning_rate: 5.0000e-04\n",
      "Epoch 60/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6917 - loss: 1.7395 - val_accuracy: 0.5784 - val_loss: 2.0019 - learning_rate: 5.0000e-04\n",
      "Epoch 61/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7001 - loss: 1.7605 - val_accuracy: 0.5870 - val_loss: 1.9373 - learning_rate: 5.0000e-04\n",
      "Epoch 62/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7190 - loss: 1.6136 - val_accuracy: 0.5612 - val_loss: 2.0366 - learning_rate: 5.0000e-04\n",
      "Epoch 63/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.6825 - loss: 1.7708 - val_accuracy: 0.5827 - val_loss: 1.9543 - learning_rate: 5.0000e-04\n",
      "Epoch 64/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7261 - loss: 1.6928 - val_accuracy: 0.5882 - val_loss: 1.9226 - learning_rate: 5.0000e-04\n",
      "Epoch 65/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7390 - loss: 1.6397 - val_accuracy: 0.5900 - val_loss: 1.9502 - learning_rate: 5.0000e-04\n",
      "Epoch 66/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7525 - loss: 1.5917 - val_accuracy: 0.5913 - val_loss: 1.9147 - learning_rate: 5.0000e-04\n",
      "Epoch 67/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7119 - loss: 1.6590 - val_accuracy: 0.5575 - val_loss: 2.0366 - learning_rate: 5.0000e-04\n",
      "Epoch 68/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7102 - loss: 1.6973 - val_accuracy: 0.6097 - val_loss: 1.8503 - learning_rate: 5.0000e-04\n",
      "Epoch 69/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7516 - loss: 1.5893 - val_accuracy: 0.6146 - val_loss: 1.8702 - learning_rate: 5.0000e-04\n",
      "Epoch 70/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7718 - loss: 1.5611 - val_accuracy: 0.5870 - val_loss: 1.9196 - learning_rate: 5.0000e-04\n",
      "Epoch 71/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.7322 - loss: 1.6097\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7323 - loss: 1.6096 - val_accuracy: 0.5907 - val_loss: 1.8927 - learning_rate: 5.0000e-04\n",
      "Epoch 72/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7522 - loss: 1.6089 - val_accuracy: 0.5974 - val_loss: 1.9106 - learning_rate: 2.5000e-04\n",
      "Epoch 73/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7948 - loss: 1.5329 - val_accuracy: 0.5864 - val_loss: 1.8905 - learning_rate: 2.5000e-04\n",
      "Epoch 74/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7759 - loss: 1.5972 - val_accuracy: 0.6281 - val_loss: 1.8340 - learning_rate: 2.5000e-04\n",
      "Epoch 75/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.7846 - loss: 1.5650 - val_accuracy: 0.6171 - val_loss: 1.8660 - learning_rate: 2.5000e-04\n",
      "Epoch 76/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8077 - loss: 1.4849 - val_accuracy: 0.6122 - val_loss: 1.8919 - learning_rate: 2.5000e-04\n",
      "Epoch 77/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8462 - loss: 1.3852 - val_accuracy: 0.5980 - val_loss: 1.9420 - learning_rate: 2.5000e-04\n",
      "Epoch 78/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8009 - loss: 1.5188 - val_accuracy: 0.6325 - val_loss: 1.7929 - learning_rate: 2.5000e-04\n",
      "Epoch 79/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8088 - loss: 1.4633 - val_accuracy: 0.6030 - val_loss: 1.9119 - learning_rate: 2.5000e-04\n",
      "Epoch 80/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8016 - loss: 1.4891 - val_accuracy: 0.6214 - val_loss: 1.8438 - learning_rate: 2.5000e-04\n",
      "Epoch 81/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8304 - loss: 1.3987 - val_accuracy: 0.6054 - val_loss: 1.9170 - learning_rate: 2.5000e-04\n",
      "Epoch 82/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8457 - loss: 1.3688 - val_accuracy: 0.6398 - val_loss: 1.7867 - learning_rate: 2.5000e-04\n",
      "Epoch 83/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7941 - loss: 1.4657 - val_accuracy: 0.6214 - val_loss: 1.8478 - learning_rate: 2.5000e-04\n",
      "Epoch 84/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8257 - loss: 1.4079 - val_accuracy: 0.6165 - val_loss: 1.8764 - learning_rate: 2.5000e-04\n",
      "Epoch 85/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8036 - loss: 1.4660 - val_accuracy: 0.6195 - val_loss: 1.8494 - learning_rate: 2.5000e-04\n",
      "Epoch 86/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8304 - loss: 1.3839 - val_accuracy: 0.6208 - val_loss: 1.8425 - learning_rate: 2.5000e-04\n",
      "Epoch 87/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8159 - loss: 1.4566 - val_accuracy: 0.6128 - val_loss: 1.8798 - learning_rate: 2.5000e-04\n",
      "Epoch 88/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8545 - loss: 1.3464 - val_accuracy: 0.6232 - val_loss: 1.8484 - learning_rate: 2.5000e-04\n",
      "Epoch 89/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8114 - loss: 1.4279 - val_accuracy: 0.6073 - val_loss: 1.8536 - learning_rate: 2.5000e-04\n",
      "Epoch 90/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8293 - loss: 1.4417 - val_accuracy: 0.6441 - val_loss: 1.7304 - learning_rate: 2.5000e-04\n",
      "Epoch 91/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8222 - loss: 1.4433 - val_accuracy: 0.6030 - val_loss: 1.9364 - learning_rate: 2.5000e-04\n",
      "Epoch 92/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8595 - loss: 1.3448 - val_accuracy: 0.6146 - val_loss: 1.8580 - learning_rate: 2.5000e-04\n",
      "Epoch 93/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8505 - loss: 1.3730 - val_accuracy: 0.6140 - val_loss: 1.8397 - learning_rate: 2.5000e-04\n",
      "Epoch 94/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8397 - loss: 1.3588 - val_accuracy: 0.6208 - val_loss: 1.8921 - learning_rate: 2.5000e-04\n",
      "Epoch 95/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8460 - loss: 1.4178 - val_accuracy: 0.5839 - val_loss: 1.8944 - learning_rate: 2.5000e-04\n",
      "Epoch 96/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8091 - loss: 1.4652 - val_accuracy: 0.6472 - val_loss: 1.7901 - learning_rate: 2.5000e-04\n",
      "Epoch 97/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8584 - loss: 1.3227 - val_accuracy: 0.6079 - val_loss: 1.9492 - learning_rate: 2.5000e-04\n",
      "Epoch 98/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8858 - loss: 1.2847 - val_accuracy: 0.6159 - val_loss: 1.8375 - learning_rate: 2.5000e-04\n",
      "Epoch 99/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8137 - loss: 1.4639 - val_accuracy: 0.6202 - val_loss: 1.8164 - learning_rate: 2.5000e-04\n",
      "Epoch 100/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.7973 - loss: 1.5001 - val_accuracy: 0.6294 - val_loss: 1.8210 - learning_rate: 2.5000e-04\n",
      "Epoch 101/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8344 - loss: 1.4004 - val_accuracy: 0.6374 - val_loss: 1.8239 - learning_rate: 2.5000e-04\n",
      "Epoch 102/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8079 - loss: 1.5060 - val_accuracy: 0.6091 - val_loss: 1.9013 - learning_rate: 2.5000e-04\n",
      "Epoch 103/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.7831 - loss: 1.4799 - val_accuracy: 0.5821 - val_loss: 2.0019 - learning_rate: 2.5000e-04\n",
      "Epoch 104/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8615 - loss: 1.3081 - val_accuracy: 0.6257 - val_loss: 1.8777 - learning_rate: 2.5000e-04\n",
      "Epoch 105/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8609 - loss: 1.3638 - val_accuracy: 0.6159 - val_loss: 1.8797 - learning_rate: 2.5000e-04\n",
      "Epoch 106/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8808 - loss: 1.2721 - val_accuracy: 0.6331 - val_loss: 1.8385 - learning_rate: 2.5000e-04\n",
      "Epoch 107/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8479 - loss: 1.3947 - val_accuracy: 0.5765 - val_loss: 1.9473 - learning_rate: 2.5000e-04\n",
      "Epoch 108/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8471 - loss: 1.3566 - val_accuracy: 0.6011 - val_loss: 1.9080 - learning_rate: 2.5000e-04\n",
      "Epoch 109/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8252 - loss: 1.4499 - val_accuracy: 0.6386 - val_loss: 1.7939 - learning_rate: 2.5000e-04\n",
      "Epoch 110/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8684 - loss: 1.2902 - val_accuracy: 0.5839 - val_loss: 1.9228 - learning_rate: 2.5000e-04\n",
      "Epoch 111/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8470 - loss: 1.3647 - val_accuracy: 0.6091 - val_loss: 1.9096 - learning_rate: 2.5000e-04\n",
      "Epoch 112/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8591 - loss: 1.2817\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8591 - loss: 1.2824 - val_accuracy: 0.6060 - val_loss: 1.8832 - learning_rate: 2.5000e-04\n",
      "Epoch 113/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8353 - loss: 1.4031 - val_accuracy: 0.5986 - val_loss: 1.9003 - learning_rate: 1.2500e-04\n",
      "Epoch 114/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8553 - loss: 1.3436 - val_accuracy: 0.6503 - val_loss: 1.8034 - learning_rate: 1.2500e-04\n",
      "Epoch 115/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8668 - loss: 1.3391 - val_accuracy: 0.6195 - val_loss: 1.8612 - learning_rate: 1.2500e-04\n",
      "Epoch 116/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8733 - loss: 1.2877 - val_accuracy: 0.6411 - val_loss: 1.7724 - learning_rate: 1.2500e-04\n",
      "Epoch 117/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 122ms/step - accuracy: 0.8481 - loss: 1.4280 - val_accuracy: 0.6208 - val_loss: 1.8656 - learning_rate: 1.2500e-04\n",
      "Epoch 118/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8919 - loss: 1.2433 - val_accuracy: 0.6411 - val_loss: 1.7785 - learning_rate: 1.2500e-04\n",
      "Epoch 119/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8581 - loss: 1.3801 - val_accuracy: 0.6478 - val_loss: 1.7926 - learning_rate: 1.2500e-04\n",
      "Epoch 120/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.8485 - loss: 1.3965 - val_accuracy: 0.6472 - val_loss: 1.7899 - learning_rate: 1.2500e-04\n",
      "Epoch 121/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8649 - loss: 1.3604 - val_accuracy: 0.6527 - val_loss: 1.6994 - learning_rate: 1.2500e-04\n",
      "Epoch 122/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8816 - loss: 1.3052 - val_accuracy: 0.5980 - val_loss: 1.9518 - learning_rate: 1.2500e-04\n",
      "Epoch 123/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.9002 - loss: 1.2384 - val_accuracy: 0.6325 - val_loss: 1.7479 - learning_rate: 1.2500e-04\n",
      "Epoch 124/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8506 - loss: 1.3568 - val_accuracy: 0.6417 - val_loss: 1.7516 - learning_rate: 1.2500e-04\n",
      "Epoch 125/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8514 - loss: 1.3950 - val_accuracy: 0.6202 - val_loss: 1.8542 - learning_rate: 1.2500e-04\n",
      "Epoch 126/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8880 - loss: 1.2284 - val_accuracy: 0.6325 - val_loss: 1.8410 - learning_rate: 1.2500e-04\n",
      "Epoch 127/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8384 - loss: 1.3592 - val_accuracy: 0.6171 - val_loss: 1.8655 - learning_rate: 1.2500e-04\n",
      "Epoch 128/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8369 - loss: 1.3647 - val_accuracy: 0.6349 - val_loss: 1.8012 - learning_rate: 1.2500e-04\n",
      "Epoch 129/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8308 - loss: 1.4542 - val_accuracy: 0.6349 - val_loss: 1.7707 - learning_rate: 1.2500e-04\n",
      "Epoch 130/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8472 - loss: 1.3662 - val_accuracy: 0.6325 - val_loss: 1.8197 - learning_rate: 1.2500e-04\n",
      "Epoch 131/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8461 - loss: 1.3411 - val_accuracy: 0.6159 - val_loss: 1.8497 - learning_rate: 1.2500e-04\n",
      "Epoch 132/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8682 - loss: 1.3262 - val_accuracy: 0.6085 - val_loss: 1.8847 - learning_rate: 1.2500e-04\n",
      "Epoch 133/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8964 - loss: 1.2243 - val_accuracy: 0.6392 - val_loss: 1.7828 - learning_rate: 1.2500e-04\n",
      "Epoch 134/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.9290 - loss: 1.1799 - val_accuracy: 0.6300 - val_loss: 1.8086 - learning_rate: 1.2500e-04\n",
      "Epoch 135/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8522 - loss: 1.3219 - val_accuracy: 0.6318 - val_loss: 1.7985 - learning_rate: 1.2500e-04\n",
      "Epoch 136/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8919 - loss: 1.2785 - val_accuracy: 0.6411 - val_loss: 1.8241 - learning_rate: 1.2500e-04\n",
      "Epoch 137/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8696 - loss: 1.3391\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8694 - loss: 1.3393 - val_accuracy: 0.6460 - val_loss: 1.7532 - learning_rate: 1.2500e-04\n",
      "Epoch 138/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8934 - loss: 1.2630 - val_accuracy: 0.6146 - val_loss: 1.8933 - learning_rate: 6.2500e-05\n",
      "Epoch 139/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8258 - loss: 1.3940 - val_accuracy: 0.6441 - val_loss: 1.7679 - learning_rate: 6.2500e-05\n",
      "Epoch 140/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8701 - loss: 1.3335 - val_accuracy: 0.6404 - val_loss: 1.7999 - learning_rate: 6.2500e-05\n",
      "Epoch 141/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8984 - loss: 1.2758 - val_accuracy: 0.6349 - val_loss: 1.7930 - learning_rate: 6.2500e-05\n",
      "Epoch 142/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8528 - loss: 1.3687 - val_accuracy: 0.6263 - val_loss: 1.8595 - learning_rate: 6.2500e-05\n",
      "Epoch 143/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8721 - loss: 1.3026 - val_accuracy: 0.6134 - val_loss: 1.8318 - learning_rate: 6.2500e-05\n",
      "Epoch 144/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8769 - loss: 1.2573 - val_accuracy: 0.6429 - val_loss: 1.7843 - learning_rate: 6.2500e-05\n",
      "Epoch 145/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.8875 - loss: 1.2648 - val_accuracy: 0.6269 - val_loss: 1.8814 - learning_rate: 6.2500e-05\n",
      "Epoch 146/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8788 - loss: 1.2991 - val_accuracy: 0.6079 - val_loss: 1.8783 - learning_rate: 6.2500e-05\n",
      "Epoch 147/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.9030 - loss: 1.2188 - val_accuracy: 0.6361 - val_loss: 1.8219 - learning_rate: 6.2500e-05\n",
      "Epoch 148/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8656 - loss: 1.3402 - val_accuracy: 0.6263 - val_loss: 1.8537 - learning_rate: 6.2500e-05\n",
      "Epoch 149/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8884 - loss: 1.2316 - val_accuracy: 0.6527 - val_loss: 1.7652 - learning_rate: 6.2500e-05\n",
      "Epoch 150/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8463 - loss: 1.3183 - val_accuracy: 0.5980 - val_loss: 1.9335 - learning_rate: 6.2500e-05\n",
      "Epoch 151/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8939 - loss: 1.2627 - val_accuracy: 0.6435 - val_loss: 1.7718 - learning_rate: 6.2500e-05\n",
      "Epoch 152/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8898 - loss: 1.2716 - val_accuracy: 0.6368 - val_loss: 1.8499 - learning_rate: 6.2500e-05\n",
      "Epoch 153/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8943 - loss: 1.2552 - val_accuracy: 0.6312 - val_loss: 1.7767 - learning_rate: 6.2500e-05\n",
      "Epoch 154/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8793 - loss: 1.2806\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8791 - loss: 1.2810 - val_accuracy: 0.6392 - val_loss: 1.7629 - learning_rate: 6.2500e-05\n",
      "Epoch 155/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.9053 - loss: 1.1889 - val_accuracy: 0.6245 - val_loss: 1.8448 - learning_rate: 3.1250e-05\n",
      "Epoch 156/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8855 - loss: 1.2546 - val_accuracy: 0.6441 - val_loss: 1.7438 - learning_rate: 3.1250e-05\n",
      "Epoch 157/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.8675 - loss: 1.3519 - val_accuracy: 0.6441 - val_loss: 1.7875 - learning_rate: 3.1250e-05\n",
      "Epoch 158/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.8412 - loss: 1.3481 - val_accuracy: 0.6411 - val_loss: 1.7914 - learning_rate: 3.1250e-05\n",
      "Epoch 159/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 124ms/step - accuracy: 0.9190 - loss: 1.1968 - val_accuracy: 0.6552 - val_loss: 1.7407 - learning_rate: 3.1250e-05\n",
      "Epoch 160/160\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - accuracy: 0.9016 - loss: 1.2574 - val_accuracy: 0.6533 - val_loss: 1.7446 - learning_rate: 3.1250e-05\n",
      "Restoring model weights from the end of the best epoch: 159.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2025-08-09 11:34:30.580806: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step\n",
      "\n",
      "✔ Training done.\n",
      "Overall OOF H‑F1 Score = 0.7740\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    print(\"▶ TRAIN MODE – loading dataset ...\")\n",
    "    df = pd.read_csv(RAW_DIR / \"train.csv\")\n",
    "    \n",
    "    #train_dem_df = pd.read_csv(RAW_DIR / \"train_demographics.csv\")\n",
    "    #df = pd.merge(df, train_dem_df[['subject', 'handedness']], on='subject', how='left')\n",
    " \n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    df['gesture_int'] = le.fit_transform(df['gesture'])\n",
    "    np.save(EXPORT_DIR / \"gesture_classes.npy\", le.classes_)\n",
    "\n",
    "    acc_y_neg_subjects = (\n",
    "        df.groupby('subject')['acc_y']\n",
    "        .mean()\n",
    "        .loc[lambda x: x < 0]\n",
    "        .index\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    print(\"acc_y ortalaması negatif olan subject'ler:\", acc_y_neg_subjects)\n",
    "\n",
    "    # Bu subject'leri tamamen drop et\n",
    "    df = df[~df['subject'].isin(acc_y_neg_subjects)].reset_index(drop=True)\n",
    "\n",
    "    # --- [Önemli Değişiklik] Gelişmiş Fiziksel ve İstatistiksel Özellikler ---\n",
    "    print(\"  Removing gravity and calculating linear acceleration features...\")\n",
    "    linear_accel_list = [pd.DataFrame(remove_gravity_from_acc(group[['acc_x', 'acc_y', 'acc_z']], group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]), columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index) for _, group in df.groupby('sequence_id')]\n",
    "    df = pd.concat([df, pd.concat(linear_accel_list)], axis=1)\n",
    "    \n",
    "    # Lineer İvme Özellikleri\n",
    "    df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n",
    "    # df['linear_acc_mag_jerk'] already exists, but consider a smoother derivative or higher order jerks if needed\n",
    "    df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0) # Keep current for now\n",
    "    \n",
    "  \n",
    "    \n",
    "    print(\"  Calculating angular velocity and distance from quaternions...\")\n",
    "    angular_vel_list = [pd.DataFrame(calculate_angular_velocity_from_quat(group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]), columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index) for _, group in df.groupby('sequence_id')]\n",
    "    df = pd.concat([df, pd.concat(angular_vel_list)], axis=1)\n",
    "    angular_dist_list = [pd.DataFrame(calculate_angular_distance(group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]), columns=['angular_distance'], index=group.index) for _, group in df.groupby('sequence_id')]\n",
    "    df = pd.concat([df, pd.concat(angular_dist_list)], axis=1)\n",
    "\n",
    "  \n",
    "    #- HUSEYİN GUR (GMN) 1.FE 80>81\n",
    "    # Hız ve İvme için Anlık İstatistiksel Özellikler (Mevcut sensör okumalarına ek olarak) \n",
    "    for col in ['acc_x', 'acc_y', 'acc_z',  'linear_acc_x', 'linear_acc_y', 'linear_acc_z', 'angular_vel_x', 'angular_vel_y', 'angular_vel_z']:  # 'rot_w', 'rot_x', 'rot_y', 'rot_z' eksik\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_diff'] = df.groupby('sequence_id')[col].diff().fillna(0)\n",
    "            df[f'{col}_abs_diff'] = np.abs(df.groupby('sequence_id')[col].diff()).fillna(0) # Mutlak fark\n",
    "    #- HUSEYİN GUR (GMN) 1.FE 80>81\n",
    "\n",
    "    # --- [Önemli Değişiklik] Fiziksel ve Yeni İstatistiksel FE'yi Yansıtan Özellik Listesi ---\n",
    "    imu_cols_base = ['acc_x', 'acc_y', 'acc_z',] + [c for c in df.columns if c.startswith('rot_')]  #+ ['handedness', 'height_cm']\n",
    "\n",
    "    imu_engineered = [\n",
    "    'linear_acc_mag', 'linear_acc_mag_jerk',\n",
    "    'angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_distance'\n",
    "\n",
    "    ]\n",
    "    #- HUSEYİN GUR (GMN) 1.FE 80>81\n",
    "    # Yeni eklenen differansiyel ve mutlak fark özellikleri\n",
    "    for col in ['acc_x', 'acc_y', 'acc_z', 'linear_acc_x', 'linear_acc_y', 'linear_acc_z', 'angular_vel_x', 'angular_vel_y', 'angular_vel_z']:\n",
    "        if col in df.columns:\n",
    "            imu_engineered.append(f'{col}_diff')\n",
    "            imu_engineered.append(f'{col}_abs_diff')\n",
    "     #- HUSEYİN GUR (GMN) 1.FE 80>81\n",
    "\n",
    "    imu_cols = list(dict.fromkeys(imu_cols_base + imu_engineered))\n",
    "    \n",
    "    # HUSEYIN GUR - THM - CGPT 3.FE ÇOK AZ BAŞARISIZ 81>81 FAKAT CV 83\n",
    "    #df = extract_temporal_thm_features(df)\n",
    "    #df = extract_spatial_thm_features(df)\n",
    "    # HUSEYIN GUR - THM - CGPT 3.FE\n",
    "    \n",
    "    final_feature_cols = imu_cols\n",
    "    imu_dim_final = len(imu_cols)\n",
    "    \n",
    "    print(f\"  IMU (phys-based + enhanced) {imu_dim_final}\")\n",
    "    np.save(EXPORT_DIR / \"feature_cols.npy\", np.array(final_feature_cols))\n",
    "\n",
    "    print(\"  Building sequences...\")\n",
    "    seq_gp = df.groupby('sequence_id') \n",
    "    X_list_unscaled, y_list_int, groups_list, lens = [], [], [], [] \n",
    "    for seq_id, seq_df in seq_gp:\n",
    "        seq_df_copy = seq_df.copy()\n",
    "        # Sadece belirlenen nihai özellik sütunlarını kullan\n",
    "        X_list_unscaled.append(seq_df_copy[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32'))\n",
    "        y_list_int.append(seq_df_copy['gesture_int'].iloc[0])\n",
    "        groups_list.append(seq_df_copy['subject'].iloc[0])\n",
    "        lens.append(len(seq_df_copy))\n",
    "\n",
    "    print(\"  Fitting StandardScaler...\")\n",
    "    all_steps_concatenated = np.concatenate(X_list_unscaled, axis=0)\n",
    "    scaler = StandardScaler().fit(all_steps_concatenated)\n",
    "    joblib.dump(scaler, EXPORT_DIR / \"scaler.pkl\")\n",
    "    \n",
    "    print(\"  Scaling and padding sequences...\")\n",
    "    X_scaled_list = [scaler.transform(x_seq) for x_seq in X_list_unscaled]\n",
    "    pad_len = int(np.percentile(lens, PAD_PERCENTILE)); np.save(EXPORT_DIR / \"sequence_maxlen.npy\", pad_len)\n",
    "    X = pad_sequences(X_scaled_list, maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n",
    "\n",
    "     # --- DEĞİŞİKLİK BAŞLANGICI --- SOL ELLERİ HER FOLD'A EŞİT DAĞIT!\n",
    "    subject_acc_x_mean_global = df.groupby('subject')['acc_x'].mean()\n",
    "    subject_is_acc_x_mean_negative = (subject_acc_x_mean_global < 0).astype(str) # '0' veya '1' string olarak\n",
    "    \n",
    "    y_stratify = np.array([f\"{gesture_label}_{subject_is_acc_x_mean_negative.loc[sub_id]}\" \n",
    "                           for gesture_label, sub_id in zip(y_list_int, groups_list)])\n",
    "    # --- DEĞİŞİKLİK SONU ---\n",
    "\n",
    "    groups, y = np.array(groups_list), to_categorical(y_list_int, num_classes=len(le.classes_))\n",
    "    print(\"  Starting training with Stratified Group K-Fold CV...\")\n",
    "    sgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=state_num)\n",
    "    oof_preds = np.zeros_like(y, dtype='float32')\n",
    "    \n",
    "\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(sgkf.split(X, y_stratify, groups)):\n",
    "        print(f\"\\n===== FOLD {fold+1}/{N_SPLITS} =====\")\n",
    "        X_tr, X_val, y_tr, y_val = X[train_idx], X[val_idx], y[train_idx], y[val_idx]\n",
    "        \n",
    "        # --- [Önemli Değişiklik] Model Derlemesi ve Geri Çağırmalar ---\n",
    "        model = build_imu_only_model(pad_len, imu_dim_final, len(le.classes_))\n",
    "        \n",
    "        # Learning Rate Scheduler ekleme - TEK BAŞINA 80>81\n",
    "        # Bu scheduler, belirli bir metrik iyileşmediğinde öğrenme oranını azaltır.\n",
    "        lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=16,\n",
    "            cooldown=2,\n",
    "            min_lr=3e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        model.compile(optimizer=Adam(LR_INIT),\n",
    "                      loss={'main_output': tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)},\n",
    "                      loss_weights={'main_output': 1.0},\n",
    "                      metrics={'main_output': 'accuracy'})\n",
    "        \n",
    "        class_weight_dict = dict(enumerate(compute_class_weight('balanced', classes=np.arange(len(le.classes_)), y=y_tr.argmax(1))))\n",
    "        \n",
    "        # GatedMixupGenerator'ın imu_dim parametresini güncelledik\n",
    "        train_gen = MixupGenerator(X_tr, y_tr, batch_size=BATCH_SIZE, class_weight=class_weight_dict, alpha=MIXUP_ALPHA)\n",
    "        val_gen = MixupGenerator(X_val, y_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "        # EarlyStopping ve LearningRateScheduler'ı birlikte kullan\n",
    "        cb = [\n",
    "            EarlyStopping(patience=PATIENCE, restore_best_weights=True, verbose=1, monitor='val_accuracy', mode='max'),\n",
    "            lr_scheduler\n",
    "        ]\n",
    "        \n",
    "        model.fit(train_gen, epochs=EPOCHS, validation_data=val_gen, callbacks=cb, verbose=1)\n",
    "        model.save(EXPORT_DIR / f\"gesture_model_fold_{fold}.h5\")\n",
    "        preds_val = model.predict(X_val) # Gate çıktısını ayır\n",
    "        oof_preds[val_idx] = preds_val\n",
    "\n",
    "    print(\"\\n✔ Training done.\")\n",
    "\n",
    "    from metric import CompetitionMetric\n",
    "    true_oof_int = y.argmax(1)\n",
    "    pred_oof_int = oof_preds.argmax(1)\n",
    "        \n",
    "    h_f1_oof = CompetitionMetric().calculate_hierarchical_f1(\n",
    "        pd.DataFrame({'gesture': le.classes_[true_oof_int]}),\n",
    "        pd.DataFrame({'gesture': le.classes_[pred_oof_int]}))\n",
    "    print(f\"Overall OOF H‑F1 Score = {h_f1_oof:.4f}\")\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "sourceId": 102335,
     "sourceType": "competition"
    },
    {
     "datasetId": 7713851,
     "sourceId": 12274546,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7782242,
     "sourceId": 12344631,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 242954653,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kaggle-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
